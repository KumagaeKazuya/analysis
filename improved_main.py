"""
YOLO11 åºƒè§’ã‚«ãƒ¡ãƒ©åˆ†æã‚·ã‚¹ãƒ†ãƒ  - æ”¹è‰¯ç‰ˆï¼ˆçµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚° + æ·±åº¦æ¨å®šçµ±åˆå¯¾å¿œç‰ˆï¼‰

ğŸ”§ ä¸»ãªæ”¹å–„ç‚¹:
1. çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¯¾å¿œ
2. æ·±åº¦æ¨å®šï¼ˆMiDaSï¼‰çµ±åˆæ©Ÿèƒ½
3. æ·±åº¦å¯¾å¿œè©•ä¾¡å™¨ã®è‡ªå‹•é¸æŠ
4. è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®è‡ªå‹•åˆ‡ã‚Šæ›¿ãˆ
5. ã‚¨ãƒ©ãƒ¼åé›†ã¨ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
6. ğŸ”§ ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸è¶³å¯¾å¿œã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½
7. ğŸ”§ ErrorCategory.EVALUATION å¯¾å¿œï¼ˆStage 5ä¿®æ­£ï¼‰
8. ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆã®å¼·åŒ–ï¼ˆStage 6ä¿®æ­£ï¼‰
9. ğŸ”§ ç›´æ¥ã‚¤ãƒ³ãƒãƒ¼ãƒˆå‰Šé™¤ã¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Œå…¨çµ±åˆ
"""
import argparse
import os
import sys
import json
import logging
from datetime import datetime
from pathlib import Path
import argparse
from typing import Dict, Any, Optional, List
import time
from datetime import datetime  # ğŸ”§ è¿½åŠ 
import traceback
import platform
from utils.camera_calibration import undistort_with_json
import numpy as np

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒª
try:
    import cv2
    import numpy as np
    import pandas as pd
    from ultralytics import YOLO
    import torch
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    import yaml
    print("âœ… å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆæˆåŠŸ")
except ImportError as e:
    print(f"âŒ å¿…é ˆãƒ©ã‚¤ãƒ–ãƒ©ãƒªä¸è¶³: {e}")
    print("ğŸ“¦ ä»¥ä¸‹ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„:")
    print("pip install ultralytics opencv-python numpy pandas matplotlib tqdm pyyaml torch")
    sys.exit(1)

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
ERROR_HANDLER_AVAILABLE = False
try:
    from utils.error_handler import (
        BaseYOLOError,
        ConfigurationError,
        VideoProcessingError,
        ResponseBuilder,
        handle_errors,
        ErrorContext,
        ErrorCategory,
        ErrorReporter,
        ErrorSeverity
    )
    ERROR_HANDLER_AVAILABLE = True
    print("âœ… çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("ğŸ”§ åŸºæœ¬ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¾ã™")
    ERROR_HANDLER_AVAILABLE = False

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - yolopose_analyzerï¼ˆXLargeãƒ¢ãƒ‡ãƒ«ç¢ºå®Ÿä½¿ç”¨ç‰ˆï¼‰
YOLOPOSE_ANALYZER_AVAILABLE = False

try:
    from yolopose_analyzer import analyze_frames_with_tracking_enhanced
    YOLOPOSE_ANALYZER_AVAILABLE = True
    print("âœ… yolopose_analyzer ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ yolopose_analyzer ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    YOLOPOSE_ANALYZER_AVAILABLE = False
    
    # ğŸ”§ åŸºæœ¬ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã‚¯ãƒ©ã‚¹ï¼ˆå®Œå…¨ç‰ˆï¼‰
    class BaseYOLOError(Exception):
        def __init__(self, message, details=None):
            super().__init__(message)
            self.message = message
            self.details = details or {}
    
    class ConfigurationError(BaseYOLOError):
        pass
    
    class VideoProcessingError(BaseYOLOError):
        pass
    
    # ğŸ”§ ErrorCategory ã®å®Œå…¨å®Ÿè£…
    class ErrorCategory:
        INITIALIZATION = "initialization"
        VIDEO_PROCESSING = "video_processing"
        EVALUATION = "evaluation"  # ğŸ”§ Stage 5ã‚¨ãƒ©ãƒ¼è§£æ±ºç”¨
        EXPERIMENT = "experiment"
        CONFIGURATION = "configuration"
        MODEL_LOADING = "model_loading"
        MODEL = "model"  # ğŸ”§ è¿½åŠ 
        DEPTH_PROCESSING = "depth_processing"
        PROCESSING = "processing"
        IO_OPERATIONS = "io_operations"
    
    # ğŸ”§ ErrorSeverity ã®å®Œå…¨å®Ÿè£…
    class ErrorSeverity:
        LOW = "low"
        MEDIUM = "medium"
        HIGH = "high"
        CRITICAL = "critical"
    
    # Line 89-250ä»˜è¿‘ã®ResponseBuilderã‚¯ãƒ©ã‚¹ã‚’ä»¥ä¸‹ã§å®Œå…¨ç½®æ›:

    class ResponseBuilder:
        """çµ±ä¸€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ãƒ“ãƒ«ãƒ€ãƒ¼ï¼ˆå®Œå…¨å¾Œæ–¹äº’æ›ç‰ˆï¼‰"""

        @staticmethod
        def success(data=None, message=""):
            """æˆåŠŸãƒ¬ã‚¹ãƒãƒ³ã‚¹"""
            return {"success": True, "data": data, "message": message}

        @staticmethod
        def error(
            error=None,
            include_traceback: bool = True,
            suggestions=None,
            message=None,                         # ğŸ”§ yolopose_analyzerç”¨
            details=None,                         # ğŸ”§ yolopose_analyzerç”¨
            exception=None,                       # ğŸ”§ å¾Œæ–¹äº’æ›æ€§
            **kwargs                              # ğŸ”§ å®Œå…¨äº’æ›ã®ãŸã‚
        ):
            """ã‚¨ãƒ©ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆå®Œå…¨äº’æ›APIï¼‰"""
    
            # å¼•æ•°ã®æ­£è¦åŒ–ï¼ˆè¤‡æ•°ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¯¾å¿œï¼‰
            target_error = error or exception
    
            if message:
                # messageãŒç›´æ¥æŒ‡å®šã•ã‚ŒãŸå ´åˆï¼ˆyolopose_analyzerç”¨ï¼‰
                response = {
                    "success": False,
                    "error": {
                        "error_type": "CustomError",
                        "message": message,
                        "category": "unknown",
                        "severity": "error",
                        "timestamp": datetime.now().isoformat()
                    }
                }
            elif hasattr(target_error, 'to_dict') and callable(getattr(target_error, 'to_dict')):
                # BaseYOLOErrorã®å ´åˆ
                response = {
                    "success": False,
                    "error": target_error.to_dict()
                }
            elif isinstance(target_error, Exception):
                # æ¨™æº–Exceptionã®å ´åˆ
                response = {
                    "success": False,
                    "error": {
                        "error_type": type(target_error).__name__,
                        "message": str(target_error),
                        "category": "unknown",
                        "severity": "error",
                        "timestamp": datetime.now().isoformat()
                    }
                }
        
                if include_traceback:
                    import traceback
                    response["error"]["traceback"] = traceback.format_exc()
            elif isinstance(target_error, str):
                # æ–‡å­—åˆ—ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
                response = {
                    "success": False,
                    "error": {
                        "error_type": "StringError",
                        "message": target_error,
                        "category": "unknown", 
                        "severity": "error",
                        "timestamp": datetime.now().isoformat()
                    }
                }
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                response = {
                    "success": False,
                    "error": {
                        "error_type": "UnknownError",
                        "message": "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                        "category": "unknown",
                        "severity": "error",
                        "timestamp": datetime.now().isoformat()
                    }
                }
    
            # suggestionsè¿½åŠ 
            if suggestions:
                response["suggestions"] = suggestions
        
            # detailsè¿½åŠ ï¼ˆé‡è¦ï¼ï¼‰
            if details:
                response["details"] = details
        
            # ãã®ä»–ã®kwargså¯¾å¿œ
            for key, value in kwargs.items():
                if key not in response:
                    response[key] = value
        
            return response

        @staticmethod
        def validation_error(field=None, message=None, details=None, **kwargs):
            """ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼ï¼ˆå®Œå…¨å¾Œæ–¹äº’æ›ï¼‰"""
            if message:
                error_message = message
            elif field:
                error_message = f"ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼: {field}"
            else:
                error_message = "ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³ã‚¨ãƒ©ãƒ¼"
            
            response = {
                "success": False,
                "error": {
                    "error_type": "ValidationError",
                    "message": error_message,
                    "field": field,
                    "category": "validation",
                    "severity": "error",
                    "timestamp": datetime.now().isoformat()
                }
            }
        
            # detailså¼•æ•°ã®ã‚µãƒãƒ¼ãƒˆï¼ˆé‡è¦ï¼ï¼‰
            if details:
                response["details"] = details
            
            # ãã®ä»–ã®kwargså¯¾å¿œ
            for key, value in kwargs.items():
                if key not in response:
                    response[key] = value
            
            return response

        @staticmethod
        def processing_error(step=None, message=None, details=None, **kwargs):
            """å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆå®Œå…¨äº’æ›ï¼‰"""
            error_message = message or f"å‡¦ç†ã‚¨ãƒ©ãƒ¼: {step}"
            result = {
                "success": False,
                "error": {
                    "error_type": "ProcessingError", 
                    "message": error_message,
                    "step": step,
                    "category": "processing",
                    "severity": "error",
                    "timestamp": datetime.now().isoformat()
               }
            }
        
            if details:
                result["details"] = details
            
            # ãã®ä»–ã®kwargså¯¾å¿œ
            for key, value in kwargs.items():
                if key not in result:
                    result[key] = value
            
            return result

        @staticmethod
        def configuration_error(config_key=None, message=None, details=None, **kwargs):
            """è¨­å®šã‚¨ãƒ©ãƒ¼ï¼ˆå®Œå…¨äº’æ›ï¼‰"""
            error_message = message or f"è¨­å®šã‚¨ãƒ©ãƒ¼: {config_key}"
            result = {
                "success": False,
                "error": {
                    "error_type": "ConfigurationError",
                    "message": error_message,
                    "config_key": config_key,
                    "category": "configuration",
                    "severity": "error",
                    "timestamp": datetime.now().isoformat()
                }
            }
        
            if details:
                result["details"] = details
            
            # ãã®ä»–ã®kwargså¯¾å¿œ
            for key, value in kwargs.items():
                if key not in result:
                    result[key] = value
            
            return result

        @staticmethod
        def model_error(model_path=None, message=None, details=None, **kwargs):
            """ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼ï¼ˆå®Œå…¨äº’æ›ï¼‰"""
            error_message = message or f"ãƒ¢ãƒ‡ãƒ«ã‚¨ãƒ©ãƒ¼: {model_path}"
            result = {
                "success": False,
                "error": {
                    "error_type": "ModelError",
                    "message": error_message,
                    "model_path": model_path,
                    "category": "model",
                    "severity": "error",
                    "timestamp": datetime.now().isoformat()
                }
            }
        
            if details:
                result["details"] = details
            
            # ãã®ä»–ã®kwargså¯¾å¿œ
            for key, value in kwargs.items():
                if key not in result:
                    result[key] = value
            
            return result
    
    class ErrorContext:
        def __init__(self, name, logger=None, raise_on_error=False):
            self.name = name
            self.logger = logger or logging.getLogger(__name__)
            self.raise_on_error = raise_on_error
            
        def __enter__(self):
            self.logger.debug(f"ğŸ” ã‚¨ãƒ©ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆé–‹å§‹: {self.name}")
            return self
            
        def __exit__(self, exc_type, exc_val, exc_tb):
            if exc_type and self.logger:
                self.logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ in {self.name}: {exc_val}")
            elif self.logger:
                self.logger.debug(f"âœ… ã‚¨ãƒ©ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ­£å¸¸çµ‚äº†: {self.name}")
            return not self.raise_on_error
        
        def add_info(self, key, value):
            self.logger.debug(f"ğŸ“ {self.name} - {key}: {value}")

    class ErrorReporter:
        def __init__(self):
            self.errors = []
        
        def add_error(self, error, context=None):
            self.errors.append({"error": str(error), "context": context, "timestamp": datetime.now().isoformat()})

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - è©•ä¾¡å™¨ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
EVALUATOR_AVAILABLE = False
DEPTH_EVALUATOR_AVAILABLE = False
COMPREHENSIVE_EVALUATOR_AVAILABLE = False
ComprehensiveEvaluator = None
DepthEnhancedEvaluator = None

try:
    # evaluatorsãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®æ®µéšçš„ãƒã‚§ãƒƒã‚¯
    import evaluators
    print("âœ… evaluators ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
    
    try:
        from evaluators.comprehensive_evaluator import ComprehensiveEvaluator
        COMPREHENSIVE_EVALUATOR_AVAILABLE = True
        EVALUATOR_AVAILABLE = True
        print("âœ… ComprehensiveEvaluator ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        
        try:
            from evaluators.comprehensive_evaluator import DepthEnhancedEvaluator
            DEPTH_EVALUATOR_AVAILABLE = True
            print("âœ… DepthEnhancedEvaluator ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
        except (ImportError, AttributeError) as e:
            print(f"âš ï¸ DepthEnhancedEvaluator ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“: {e}")
            DEPTH_EVALUATOR_AVAILABLE = False
            
    except ImportError as e:
        print(f"âš ï¸ ComprehensiveEvaluator ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆã«å¤±æ•—: {e}")
        COMPREHENSIVE_EVALUATOR_AVAILABLE = False
        EVALUATOR_AVAILABLE = False
        
except ImportError as e:
    print(f"âš ï¸ evaluators ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    EVALUATOR_AVAILABLE = False
    COMPREHENSIVE_EVALUATOR_AVAILABLE = False
    DEPTH_EVALUATOR_AVAILABLE = False

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - è¨­å®šç®¡ç†ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
CONFIG_AVAILABLE = False
Config = None

try:
    from utils.config import Config
    CONFIG_AVAILABLE = True
    print("âœ… Config ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ Config ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("ğŸ”§ åŸºæœ¬è¨­å®šæ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
    CONFIG_AVAILABLE = False

# ğŸ”§ BasicEvaluatorï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
if not EVALUATOR_AVAILABLE:
    print("ğŸ”§ åŸºæœ¬è©•ä¾¡æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
    
    class BasicEvaluator:
        def __init__(self, config=None):
            self.config = config or {}
            self.results = {}
            self.logger = logging.getLogger(__name__)
        
        def evaluate_comprehensive(self, video_path, detection_results, video_name):
            """åŸºæœ¬çš„ãªè©•ä¾¡ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
            try:
                self.logger.info(f"ğŸ” åŸºæœ¬è©•ä¾¡é–‹å§‹: {video_name}")
                
                # ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã®æ”¹å–„
                if isinstance(detection_results, dict):
                    if detection_results.get("success", False):
                        data = detection_results.get("data", {})
                    else:
                        self.logger.warning("detection_results ãŒå¤±æ•—çŠ¶æ…‹ã§ã™")
                        data = {}
                else:
                    data = {}
                
                csv_path = data.get("csv_path") or data.get("enhanced_csv_path")
                
                basic_metrics = {
                    "video_name": video_name,
                    "video_path": str(video_path),
                    "detection_count": data.get("detection_count", 0),
                    "frame_count": data.get("frame_count", 0),
                    "processing_time": data.get("processing_time", 0),
                    "timestamp": datetime.now().isoformat(),
                    "evaluator_type": "BasicEvaluator",
                    "depth_enabled": data.get("depth_enabled", False),
                    "processing_stats": data.get("processing_stats", {})
                }
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«åˆ†æã®æ”¹å–„
                if csv_path and Path(csv_path).exists():
                    try:
                        df = pd.read_csv(csv_path)
                        self.logger.info(f"ğŸ“Š CSVåˆ†æ: {len(df)}è¡Œã®ãƒ‡ãƒ¼ã‚¿")
                        
                        csv_metrics = {
                            "total_detections": len(df),
                            "detection_success": True,
                            "csv_path": str(csv_path)
                        }
                        
                        # ã‚«ãƒ©ãƒ ãƒ™ãƒ¼ã‚¹ã®è©³ç´°åˆ†æ
                        available_columns = df.columns.tolist()
                        csv_metrics["available_columns"] = available_columns
                        
                        if 'track_id' in df.columns:
                            csv_metrics["unique_track_ids"] = df['track_id'].nunique()
                        
                        if 'confidence' in df.columns:
                            csv_metrics["confidence_mean"] = float(df['confidence'].mean())
                            csv_metrics["confidence_std"] = float(df['confidence'].std())
                            csv_metrics["confidence_min"] = float(df['confidence'].min())
                            csv_metrics["confidence_max"] = float(df['confidence'].max())
                        
                        # æ·±åº¦é–¢é€£ã‚«ãƒ©ãƒ ã®è©³ç´°ç¢ºèª
                        depth_columns = [col for col in df.columns if 'depth' in col.lower()]
                        if depth_columns:
                            csv_metrics["depth_columns"] = depth_columns
                            csv_metrics["depth_analysis_available"] = True
                            # æ·±åº¦ãƒ‡ãƒ¼ã‚¿ã®çµ±è¨ˆ
                            for depth_col in depth_columns:
                                if df[depth_col].dtype in ['float64', 'int64']:
                                    csv_metrics[f"{depth_col}_mean"] = float(df[depth_col].mean())
                        
                        basic_metrics.update(csv_metrics)
                        
                    except Exception as e:
                        self.logger.warning(f"CSVåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                        basic_metrics.update({
                            "detection_success": False,
                            "csv_error": str(e),
                            "csv_path": str(csv_path) if csv_path else None
                        })
                else:
                    self.logger.warning(f"CSV ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
                    basic_metrics["csv_available"] = False
                
                self.logger.info(f"âœ… åŸºæœ¬è©•ä¾¡å®Œäº†: {video_name}")
                return ResponseBuilder.success(data=basic_metrics)
                
            except Exception as e:
                self.logger.error(f"âŒ åŸºæœ¬è©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
                return ResponseBuilder.error(e, suggestions=[
                    "è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãç”Ÿæˆã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                    "å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®å‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ])
        
        def evaluate_with_depth(self, video_path, detection_results, video_name):
            """æ·±åº¦å¯¾å¿œè©•ä¾¡ï¼ˆBasicEvaluatorç‰ˆï¼‰"""
            self.logger.info(f"ğŸ” æ·±åº¦å¯¾å¿œåŸºæœ¬è©•ä¾¡: {video_name}")
            result = self.evaluate_comprehensive(video_path, detection_results, video_name)
            
            if result.get("success", False):
                result["data"]["depth_evaluator_type"] = "BasicEvaluator"
                result["data"]["depth_support"] = "limited"
            
            return result

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
VIDEO_PROCESSOR_AVAILABLE = False
VideoProcessor = None

try:
    from processors.video_processor import VideoProcessor
    VIDEO_PROCESSOR_AVAILABLE = True
    print("âœ… VideoProcessor ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ VideoProcessor ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("ğŸ”§ åŸºæœ¬å‹•ç”»å‡¦ç†ã‚’ä½¿ç”¨ã—ã¾ã™")
    VIDEO_PROCESSOR_AVAILABLE = False

# ğŸ”§ BasicVideoProcessorï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
if not VIDEO_PROCESSOR_AVAILABLE:
    class BasicVideoProcessor:
        def __init__(self, config):
            """åŸºæœ¬å‹•ç”»ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
            self.config = config
            self.logger = logging.getLogger(__name__)
            self.processing_stats = {}  # ğŸ”§ çµ±è¨ˆæƒ…å ±è¾æ›¸ã‚’åˆæœŸåŒ–
    
            if hasattr(config, 'get'):
                self.output_dir = Path(config.get('output_dir', 'outputs'))
                self.max_frames = config.get('processing.max_frames', 100)
            else:
                self.output_dir = Path('outputs')
                self.max_frames = 100
    
            self.output_dir.mkdir(exist_ok=True)
            self.logger.info(f"ğŸ¬ åŸºæœ¬å‹•ç”»ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼åˆæœŸåŒ–å®Œäº†")
        
        def load_models(self):
            """ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ï¼ˆãƒ‘ã‚¹é‡è¤‡ä¿®æ­£ç‰ˆï¼‰"""
            try:
                if hasattr(self.config, 'get'):
                    models_config = self.config.get('models', {})
                elif isinstance(self.config, dict):
                    models_config = self.config.get('models', {})
                else:
                    models_config = {}
                
                # âš¡ ãƒ‘ã‚¹é‡è¤‡ã‚’é˜²ãä¿®æ­£
                detection_path = models_config.get('detection', 'models/yolo11x.pt')
                pose_path = models_config.get('pose', 'models/yolo11x-pose.pt')
                
                # ãƒ‘ã‚¹é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if detection_path.startswith('models/models/'):
                    detection_path = detection_path.replace('models/models/', 'models/')
                if pose_path.startswith('models/models/'):
                    pose_path = pose_path.replace('models/models/', 'models/')
                
                self.logger.info(f"ğŸ” ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰é–‹å§‹")
                self.logger.info(f"ğŸ“Š æ¤œå‡ºãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {detection_path}")
                self.logger.info(f"ğŸ“Š ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹: {pose_path}")
                
                # æ¤œå‡ºãƒ¢ãƒ‡ãƒ«
                if Path(detection_path).exists():
                    self.detection_model = YOLO(detection_path)
                    self.logger.info(f"âœ… æ¤œå‡ºãƒ¢ãƒ‡ãƒ«: {detection_path}")
                else:
                    self.logger.warning(f"âš ï¸ æ¤œå‡ºãƒ¢ãƒ‡ãƒ«æœªç™ºè¦‹: {detection_path}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    self.detection_model = YOLO('yolo11x.pt')
                    self.logger.info("âœ… æ¤œå‡ºãƒ¢ãƒ‡ãƒ«: è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")

                # ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«
                if Path(pose_path).exists():
                    self.pose_model = YOLO(pose_path)
                    self.logger.info(f"âœ… ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«: {pose_path}")
                else:
                    self.logger.warning(f"âš ï¸ ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«æœªç™ºè¦‹: {pose_path}")
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                    self.pose_model = YOLO('yolo11x-pose.pt')
                    self.logger.info("âœ… ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«: è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
                
                self.logger.info("âœ… å…¨ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰å®Œäº†")
                
            except Exception as e:
                self.logger.error(f"âŒ ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼: {e}")
                raise
        
        # BasicVideoProcessor ã® extract_frames ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¿®æ­£:

        def extract_frames(video_path, frame_dir, max_frames=1000):
            """
            ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œãƒ»æ—¢å­˜å‡¦ç†ã‚’è¸è¥²ï¼‰
            """
            import cv2
            from pathlib import Path

            logger = logging.getLogger(__name__)
            logger.info(f"ğŸ“¸ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹: {video_path}")
            frame_dir = Path(frame_dir)
            frame_dir.mkdir(parents=True, exist_ok=True)

            # å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
            if not Path(video_path).exists():
                logger.error(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {video_path}")
                return {"success": False, "error": f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {video_path}"}

            cap = cv2.VideoCapture(str(video_path))
            if not cap.isOpened():
                logger.error(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}")
                return {"success": False, "error": f"å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é–‹ã‘ã¾ã›ã‚“: {video_path}"}

            frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            duration = frame_count / fps if fps > 0 else 0

            logger.info(f"ğŸ“¹ å‹•ç”»æƒ…å ±: {width}x{height}, {frame_count}ãƒ•ãƒ¬ãƒ¼ãƒ , {fps:.1f}FPS, {duration:.1f}ç§’")

            # æŠ½å‡ºé–“éš”è¨ˆç®—
            interval = max(1, frame_count // max_frames)
            logger.info(f"ğŸ”¢ æŠ½å‡ºé–“éš”: {interval} (æœ€å¤§{max_frames}ãƒ•ãƒ¬ãƒ¼ãƒ )")

            extracted = 0
            frame_number = 0

            while True:
                ret, frame = cap.read()
                if not ret:
                    break

                if frame_number % interval == 0:
                    frame_path = frame_dir / f"frame_{frame_number:06d}.jpg"
                    success = cv2.imwrite(str(frame_path), frame)
                    if success:
                        extracted += 1
                        if extracted >= max_frames:
                            break
                    else:
                        logger.warning(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ä¿å­˜å¤±æ•—: {frame_path}")

                frame_number += 1

            cap.release()

            saved_frames = len(list(frame_dir.glob("frame_*.jpg")))
            logger.info(f"ğŸ“Š æŠ½å‡º: {extracted}å€‹, å®Ÿéš›ã«ä¿å­˜: {saved_frames}å€‹")

            final_extracted = max(extracted, saved_frames)

            logger.info(f"âœ… ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Œäº†: {final_extracted}ãƒ•ãƒ¬ãƒ¼ãƒ ")

            if final_extracted == 0:
                logger.error("âŒ ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ")
                return {"success": False, "error": "ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºã«å¤±æ•—ã—ã¾ã—ãŸ"}

            return {
                "success": True,
                "extracted_frames": final_extracted,
                "video_info": {
                    "total_frames": frame_count,
                    "fps": fps,
                    "duration": duration,
                    "resolution": [width, height],
                    "extraction_interval": interval
                }
            }
        
        def run_detection_tracking(self, frame_dir, video_name, output_dir=None):
            """åŸºæœ¬æ¤œå‡ºãƒ»è¿½è·¡å‡¦ç†ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œãƒ»æ©Ÿèƒ½ç¶­æŒç‰ˆï¼‰"""
            try:
                self.logger.info(f"ğŸ‘ï¸ åŸºæœ¬æ¤œå‡ºãƒ»è¿½è·¡å‡¦ç†é–‹å§‹: {video_name}")
                frame_files = sorted(list(Path(frame_dir).glob("*.jpg")))

                if not frame_files:
                    raise VideoProcessingError(f"ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {frame_dir}")

                self.logger.info(f"ğŸ“¸ å‡¦ç†å¯¾è±¡ãƒ•ãƒ¬ãƒ¼ãƒ : {len(frame_files)}å€‹")

                # ãƒ¢ãƒ‡ãƒ«ã®äº‹å‰ãƒ­ãƒ¼ãƒ‰ç¢ºèª
                if not hasattr(self, 'detection_model') and not hasattr(self, 'pose_model'):
                    self.load_models()

                detection_count = 0
                frame_stats = []

                # ä¿¡é ¼åº¦ã—ãã„å€¤ï¼ˆã‚ˆã‚Šä½ãè¨­å®šã—ã¦æ¤œå‡ºç‡å‘ä¸Šï¼‰
                conf_threshold = 0.25

                # ğŸ”§ ç°¡ç•¥åŒ–ã•ã‚ŒãŸå‡¦ç†ãƒ«ãƒ¼ãƒ—
                for i, frame_file in enumerate(frame_files):
                    try:
                        # ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿
                        frame = cv2.imread(str(frame_file))
                        if frame is None:
                            self.logger.warning(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ èª­ã¿è¾¼ã¿å¤±æ•—: {frame_file}")
                            continue

                        frame = undistort_with_json(frame, calib_path="configs/camera_params.json")

                        frame_detections = 0

                        # ğŸ”§ ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«å„ªå…ˆï¼ˆã‚ˆã‚Šç¢ºå®Ÿï¼‰
                        if hasattr(self, 'pose_model') and self.pose_model:
                            try:
                                results = self.pose_model(frame, verbose=False, conf=conf_threshold)
                                if results and len(results[0].boxes) > 0:
                                    frame_detections = len(results[0].boxes)
                                    detection_count += frame_detections
                            except Exception as e:
                                self.logger.debug(f"ãƒ•ãƒ¬ãƒ¼ãƒ {i}ãƒãƒ¼ã‚ºæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")

                        # ãƒ•ãƒ¬ãƒ¼ãƒ çµ±è¨ˆè¨˜éŒ²ï¼ˆç°¡ç•¥åŒ–ï¼‰
                        frame_stats.append({
                            "frame_id": i,
                            "frame_file": frame_file.name,
                            "detections": frame_detections,
                            "conf": conf_threshold,  # å›ºå®šå€¤
                            "track_id": i,  # ç°¡æ˜“ID
                            "timestamp": datetime.now().isoformat()
                        })

                    except Exception as e:
                        self.logger.warning(f"ãƒ•ãƒ¬ãƒ¼ãƒ {i}å‡¦ç†ã‚¨ãƒ©ãƒ¼ï¼ˆç¶šè¡Œï¼‰: {e}")
                        continue

                # --- ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œ ---
                if output_dir is None:
                    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                    output_dir = Path("outputs/temp") / f"{video_name}_{timestamp}"
                else:
                    output_dir = Path(output_dir)
                    output_dir.mkdir(parents=True, exist_ok=True)
                csv_path = output_dir / f"{video_name}_results.csv"

                if frame_stats:
                    df = pd.DataFrame(frame_stats)
                    df.to_csv(csv_path, index=False)
                    self.logger.info(f"ğŸ“Š çµæœCSVä¿å­˜: {csv_path}")
                else:
                    # ğŸ”§ ç©ºã®å ´åˆã§ã‚‚CSVã‚’ä½œæˆ
                    empty_df = pd.DataFrame(columns=["frame_id", "frame_file", "detections", "conf", "track_id", "timestamp"])
                    empty_df.to_csv(csv_path, index=False)
                    self.logger.warning("âš ï¸ æ¤œå‡ºçµæœãªã— - ç©ºã®CSVã‚’ä½œæˆ")

                # çµ±è¨ˆæƒ…å ±
                self.processing_stats = {
                    "detection_tracking": {
                        "total_frames": len(frame_files),
                        "processed_frames": len(frame_stats),
                        "total_detections": detection_count,
                        "success_rate": len(frame_stats) / len(frame_files) if frame_files else 0
                    }
                }

                self.logger.info(f"âœ… åŸºæœ¬æ¤œå‡ºãƒ»è¿½è·¡å®Œäº†: {detection_count}å€‹æ¤œå‡º / {len(frame_stats)}ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†")

                return {
                    "success": True,
                    "data": {
                        "csv_path": str(csv_path),
                        "detection_count": detection_count,
                        "frame_count": len(frame_files),
                        "processed_frames": len(frame_stats),
                        "processing_stats": self.processing_stats["detection_tracking"]
                    }
                }

            except Exception as e:
                self.logger.error(f"âŒ åŸºæœ¬æ¤œå‡ºãƒ»è¿½è·¡ã‚¨ãƒ©ãƒ¼: {e}")
                return {"success": False, "error": str(e)}
        
        def run_detection_tracking_with_depth(self, frame_dir, video_name):
            """æ·±åº¦çµ±åˆæ¤œå‡ºãƒ»è¿½è·¡å‡¦ç†ï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç‰ˆï¼‰"""
            self.logger.warning("ğŸ”§ æ·±åº¦çµ±åˆå‡¦ç†ã¯åˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚é€šå¸¸å‡¦ç†ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™")
            result = self.run_detection_tracking(frame_dir, video_name)
            
            if result.get("success", False):
                result["data"]["depth_enabled"] = False
                result["data"]["depth_fallback"] = True
                result["data"]["enhanced_csv_path"] = result["data"]["csv_path"]
                self.logger.info("âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†å®Œäº†ï¼ˆæ·±åº¦æ©Ÿèƒ½ç„¡åŠ¹ï¼‰")
            
            return result

# ğŸ”§ æ¡ä»¶ä»˜ãã‚¤ãƒ³ãƒãƒ¼ãƒˆ - ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
METRICS_ANALYZER_AVAILABLE = False
MetricsAnalyzer = None

try:
    from analyzers.metrics_analyzer import MetricsAnalyzer
    METRICS_ANALYZER_AVAILABLE = True
    print("âœ… MetricsAnalyzer ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ MetricsAnalyzer ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("ğŸ”§ åŸºæœ¬åˆ†ææ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
    METRICS_ANALYZER_AVAILABLE = False

# ğŸ”§ BasicMetricsAnalyzerï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
if not METRICS_ANALYZER_AVAILABLE:
    class BasicMetricsAnalyzer:
        def __init__(self, config):
            self.config = config
            self.logger = logging.getLogger(__name__)
        
        def analyze_improvements(self, comparison_results):
            """åŸºæœ¬æ”¹å–„åˆ†æï¼ˆå®Œå…¨ç‰ˆï¼‰"""
            self.logger.info("ğŸ“Š åŸºæœ¬æ”¹å–„åˆ†æé–‹å§‹")
            
            try:
                baseline = comparison_results.get("baseline", {})
                experiment = comparison_results.get("experiment", {})
                
                analysis = {
                    "analyzer_type": "BasicMetricsAnalyzer",
                    "comparison_available": bool(baseline and experiment),
                    "timestamp": datetime.now().isoformat()
                }
                
                if baseline and experiment:
                    # å‡¦ç†æ™‚é–“æ¯”è¼ƒ
                    baseline_time = baseline.get("processing_time", 0)
                    experiment_time = experiment.get("processing_time", 0)
                    
                    if baseline_time > 0:
                        time_improvement = ((baseline_time - experiment_time) / baseline_time) * 100
                        analysis["time_improvement_percent"] = time_improvement
                        analysis["time_comparison"] = {
                            "baseline_time": baseline_time,
                            "experiment_time": experiment_time,
                            "improvement": time_improvement
                        }
                    
                    # æ¤œå‡ºæ•°æ¯”è¼ƒ
                    baseline_detections = baseline.get("detection_count", 0)
                    experiment_detections = experiment.get("detection_count", 0)
                    
                    analysis["detection_comparison"] = {
                        "baseline": baseline_detections,
                        "experiment": experiment_detections,
                        "difference": experiment_detections - baseline_detections,
                        "improvement_rate": ((experiment_detections - baseline_detections) / baseline_detections * 100) if baseline_detections > 0 else 0
                    }
                    
                    # å“è³ªæ¯”è¼ƒ
                    analysis["quality_comparison"] = {
                        "baseline_success": baseline.get("success", False),
                        "experiment_success": experiment.get("success", False)
                    }
                
                self.logger.info("âœ… åŸºæœ¬æ”¹å–„åˆ†æå®Œäº†")
                return analysis
                
            except Exception as e:
                self.logger.error(f"âŒ æ”¹å–„åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                return {"basic_analysis": f"ã‚¨ãƒ©ãƒ¼: {e}", "error": True}
        
        def create_visualizations(self, detection_results, vis_dir):
            """
            å¯è¦–åŒ–ç”Ÿæˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå¯¾å¿œãƒ»æ—¢å­˜æ©Ÿèƒ½å®Œå…¨ç¶­æŒç‰ˆï¼‰
            detection_results: run_detection_trackingç­‰ã®å‡ºåŠ›(dict)
            vis_dir: å¯è¦–åŒ–ç”»åƒã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãï¼‰
            """
            import json
            import pandas as pd
            import matplotlib
            matplotlib.use('Agg')
            import matplotlib.pyplot as plt
            from pathlib import Path
            from datetime import datetime

            self.logger.info(f"ğŸ“ˆ åŸºæœ¬å¯è¦–åŒ–ç”Ÿæˆ: {vis_dir}")

            # åˆæœŸåŒ–
            result = {
                "success": False,
                "error": "åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼",
                "basic_stats_file": None,
                "graphs_generated": 0,
                "total_files": 0
            }

            try:
                vis_path = Path(str(vis_dir))
                vis_path.mkdir(parents=True, exist_ok=True)
                self.logger.info(f"ğŸ“ å¯è¦–åŒ–ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {vis_path}")

                # detection_results ã®è©³ç´°ãƒ­ã‚°
                self.logger.info(f"ğŸ”§ detection_results type: {type(detection_results)}")
                self.logger.info(f"ğŸ”§ detection_results content: {detection_results}")

                # CSVãƒ‘ã‚¹æŠ½å‡º
                csv_path = None
                data = {}

                if isinstance(detection_results, dict):
                    if detection_results.get("success", False):
                        data = detection_results.get("data", {})
                        csv_path = data.get("csv_path")
                        # ãƒã‚¹ãƒˆæ§‹é€ å¯¾å¿œ
                        if not csv_path and "detection_result" in data:
                            nested_data = data["detection_result"].get("data", {})
                            csv_path = nested_data.get("csv_path")
                self.logger.info(f"ğŸ”§ æ¤œå‡ºã•ã‚ŒãŸCSVãƒ‘ã‚¹: {csv_path}")

                # åŸºæœ¬çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆå¿…ãšä½œæˆï¼‰
                stats_file = vis_path / "basic_stats.json"
                basic_stats = {
                    "visualization_type": "BasicVisualization",
                    "detection_count": data.get("detection_count", 0),
                    "frame_count": data.get("frame_count", 0),
                    "processing_time": data.get("processing_time", 0),
                    "processing_stats": data.get("processing_stats", {}),
                    "timestamp": datetime.now().isoformat(),
                    "csv_path": str(csv_path) if csv_path else None,
                    "success": detection_results.get("success", False) if isinstance(detection_results, dict) else False
                }

                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(basic_stats, f, indent=2, ensure_ascii=False)
                self.logger.info(f"âœ… åŸºæœ¬çµ±è¨ˆä¿å­˜: {stats_file}")

                # æˆ»ã‚Šå€¤æ›´æ–°
                result.update({
                    "success": True,
                    "error": None,
                    "basic_stats_file": str(stats_file),
                    "total_files": 1,
                    "graphs_generated": 0
                })

                graphs_generated = 0
                graph_files = []

                try:
                    # ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
                    try:
                        plt.rcParams['font.family'] = ['Hiragino Sans', 'DejaVu Sans']
                    except Exception:
                        plt.rcParams['font.family'] = 'DejaVu Sans'

                    # CSV ãƒ•ã‚¡ã‚¤ãƒ«ã®å‡¦ç†
                    if csv_path and Path(csv_path).exists():
                        self.logger.info(f"ğŸ“Š CSVãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {csv_path}")
                        df = pd.read_csv(csv_path)
                        self.logger.info(f"ğŸ“Š ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {len(df)}è¡Œ, ã‚«ãƒ©ãƒ : {list(df.columns)}")

                        if not df.empty:
                            # 1. ãƒ•ãƒ¬ãƒ¼ãƒ åˆ¥æ¤œå‡ºæ•°ã‚°ãƒ©ãƒ•
                            if 'frame' in df.columns or 'frame_id' in df.columns:
                                try:
                                    frame_col = 'frame' if 'frame' in df.columns else 'frame_id'
                                    plt.figure(figsize=(12, 6))
                                    frame_counts = df[frame_col].value_counts().sort_index()
                                    plt.plot(frame_counts.index, frame_counts.values, 
                                    marker='o', linewidth=2, markersize=4, color='blue')
                                    plt.title('Detection Count by Frame', fontsize=16, pad=20)
                                    plt.xlabel('Frame Number', fontsize=12)
                                    plt.ylabel('Detection Count', fontsize=12)
                                    plt.grid(True, alpha=0.3)
                                    plt.tight_layout()
                                    timeline_path = vis_path / "detection_timeline.png"
                                    plt.savefig(timeline_path, dpi=300, bbox_inches='tight')
                                    plt.close()
                                    graphs_generated += 1
                                    graph_files.append(str(timeline_path))
                                    self.logger.info(f"âœ… æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ç”Ÿæˆ: {timeline_path}")
                                except Exception as e:
                                    self.logger.error(f"âŒ æ™‚ç³»åˆ—ã‚°ãƒ©ãƒ•ã‚¨ãƒ©ãƒ¼: {e}")

                            # 2. ä¿¡é ¼åº¦åˆ†å¸ƒã‚°ãƒ©ãƒ•
                            if 'conf' in df.columns or 'confidence' in df.columns:
                                try:
                                    conf_col = 'conf' if 'conf' in df.columns else 'confidence'
                                    plt.figure(figsize=(10, 6))
                                    conf_data = df[conf_col].dropna()
                                    plt.hist(conf_data, bins=30, alpha=0.7, color='green', edgecolor='black')
                                    plt.axvline(conf_data.mean(), color='red', linestyle='--', 
                                                label=f'Average: {conf_data.mean():.3f}')
                                    plt.title('Confidence Distribution', fontsize=16, pad=20)
                                    plt.xlabel('Confidence', fontsize=12)
                                    plt.ylabel('Frequency', fontsize=12)
                                    plt.legend()
                                    plt.grid(True, alpha=0.3)
                                    plt.tight_layout()
                                    conf_path = vis_path / "confidence_distribution.png"
                                    plt.savefig(conf_path, dpi=300, bbox_inches='tight')
                                    plt.close()
                                    graphs_generated += 1
                                    graph_files.append(str(conf_path))
                                    self.logger.info(f"âœ… ä¿¡é ¼åº¦åˆ†å¸ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ: {conf_path}")
                                except Exception as e:
                                    self.logger.error(f"âŒ ä¿¡é ¼åº¦åˆ†å¸ƒã‚°ãƒ©ãƒ•ã‚¨ãƒ©ãƒ¼: {e}")

                            # 3. ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚°ãƒ©ãƒ•
                            if 'class_name' in df.columns:
                                try:
                                    plt.figure(figsize=(12, 8))
                                    class_counts = df['class_name'].value_counts()
                                    class_counts.plot(kind='bar', color='skyblue', edgecolor='black')
                                    plt.title('Class Distribution', fontsize=16, pad=20)
                                    plt.xlabel('Class Name', fontsize=12)
                                    plt.ylabel('Detection Count', fontsize=12)
                                    plt.xticks(rotation=45)
                                    plt.tight_layout()
                                    class_path = vis_path / "class_distribution.png"
                                    plt.savefig(class_path, dpi=300, bbox_inches='tight')
                                    plt.close()
                                    graphs_generated += 1
                                    graph_files.append(str(class_path))
                                    self.logger.info(f"âœ… ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚°ãƒ©ãƒ•ç”Ÿæˆ: {class_path}")
                                except Exception as e:
                                    self.logger.error(f"âŒ ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚°ãƒ©ãƒ•ã‚¨ãƒ©ãƒ¼: {e}")

                            # 4. 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–åŒ–ï¼ˆã‚‚ã—4ç‚¹CSVãŒã‚ã‚Œã°ï¼‰
                            if 'filtered_csv_path' in data and data['filtered_csv_path'] and Path(data['filtered_csv_path']).exists():
                                try:
                                    filtered_csv = data['filtered_csv_path']
                                    self.logger.info(f"ğŸ¨ 6ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–åŒ–: {filtered_csv}")
                                    vis_4pt_result = self.create_6point_visualization(filtered_csv, data.get('video_path', ''), vis_path)
                                    if vis_4pt_result.get("success"):
                                        graphs_generated += 1
                                        graph_files.append(vis_4pt_result.get("output_dir"))
                                        self.logger.info(f"âœ… 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–åŒ–ç”Ÿæˆ: {vis_4pt_result.get('output_dir')}")
                                except Exception as e:
                                    self.logger.error(f"âŒ 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")

                        else:
                            self.logger.warning("âš ï¸ CSVãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™")
                    else:
                        self.logger.warning(f"âš ï¸ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„: {csv_path}")
                
                except ImportError as e:
                    self.logger.warning(f"âš ï¸ matplotlib/pandasã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                except Exception as plot_error:
                    self.logger.error(f"âŒ ã‚°ãƒ©ãƒ•ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {plot_error}", exc_info=True)

                # æœ€çµ‚çµæœæ›´æ–°
                total_files = 1 + graphs_generated
                result.update({
                    "success": True,
                    "error": None,
                    "graphs_generated": graphs_generated,
                    "total_files": total_files,
                    "graph_files": graph_files
                })

                self.logger.info(f"ğŸ¨ å¯è¦–åŒ–ç”Ÿæˆå®Œäº†: åŸºæœ¬çµ±è¨ˆ1å€‹ + ã‚°ãƒ©ãƒ•{graphs_generated}å€‹ = åˆè¨ˆ{total_files}å€‹")

                return result

            except Exception as e:
                self.logger.error(f"âŒ å¯è¦–åŒ–ç”Ÿæˆå…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                result.update({
                    "success": False,
                    "error": str(e),
                    "graphs_generated": 0,
                    "total_files": 0
                })
                return result
                
        def _create_detection_charts(self, data, vis_path):
            """æ¤œå‡ºçµæœã®ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ"""
            try:
                import matplotlib.pyplot as plt
                import pandas as pd
                import seaborn as sns
                
                # çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª
                processing_stats = data.get("processing_stats", {})
                detection_count = data.get("detection_count", 0)
                
                # 1. åŸºæœ¬çµ±è¨ˆã‚°ãƒ©ãƒ•
                fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 12))
                
                # æ¤œå‡ºæ•°ã‚°ãƒ©ãƒ•
                ax1.bar(['æ¤œå‡ºæ•°'], [detection_count], color='skyblue')
                ax1.set_title('ç·æ¤œå‡ºæ•°')
                ax1.set_ylabel('ä»¶æ•°')
                
                # å‡¦ç†çµ±è¨ˆ
                if processing_stats:
                    stats_keys = list(processing_stats.keys())[:5]  # æœ€å¤§5é …ç›®
                    stats_values = [processing_stats[k] for k in stats_keys]
                    ax2.barh(stats_keys, stats_values, color='lightcoral')
                    ax2.set_title('å‡¦ç†çµ±è¨ˆ')
                    ax2.set_xlabel('å€¤')
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°
                frame_count = data.get("frame_count", 0)
                ax3.pie([frame_count, max(1, 120 - frame_count)], 
                        labels=['å‡¦ç†æ¸ˆã¿', 'æœªå‡¦ç†'], autopct='%1.1f%%',
                        colors=['lightgreen', 'lightgray'])
                ax3.set_title('ãƒ•ãƒ¬ãƒ¼ãƒ å‡¦ç†çŠ¶æ³')
                
                # å‡¦ç†æ™‚é–“
                processing_time = data.get("processing_time", 0)
                ax4.bar(['å‡¦ç†æ™‚é–“'], [processing_time], color='gold')
                ax4.set_title('å‡¦ç†æ™‚é–“ (ç§’)')
                ax4.set_ylabel('ç§’')
                
                plt.tight_layout()
                plt.savefig(vis_path / "detection_summary.png", dpi=300, bbox_inches='tight')
                plt.close()
                
                self.logger.info(f"âœ… ã‚µãƒãƒªãƒ¼ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆ: detection_summary.png")
                
            except ImportError:
                self.logger.warning("âš ï¸ matplotlibæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - åŸºæœ¬çµ±è¨ˆã®ã¿ä¿å­˜")
            except Exception as e:
                self.logger.error(f"âŒ ãƒãƒ£ãƒ¼ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

# ğŸ”§ BasicConfigï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
if not CONFIG_AVAILABLE:
    class BasicConfig:
        def __init__(self, config_path=None):
            self.config_path = config_path
            self.data = self.load_config()
            self.logger = logging.getLogger(__name__)
        
        def load_config(self):
            """è¨­å®šãƒ­ãƒ¼ãƒ‰ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
            logger = logging.getLogger(__name__)
            logger.info(f"âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {self.config_path}")
            
            if self.config_path and Path(self.config_path).exists():
                try:
                    with open(self.config_path, 'r', encoding='utf-8') as f:
                        if self.config_path.endswith(('.yaml', '.yml')):
                            config_data = yaml.safe_load(f)
                            logger.info("âœ… YAMLè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
                        else:
                            config_data = json.load(f)
                            logger.info("âœ… JSONè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
                    
                    return config_data
                    
                except Exception as e:
                    logger.warning(f"âš ï¸ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
                    logger.info("ğŸ”§ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
            else:
                logger.warning(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {self.config_path}")
                logger.info("ğŸ”§ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™")
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šï¼ˆå®Œå…¨ç‰ˆï¼‰
            default_config = {
                "models": {
                    "detection": "models/yolo/yolo11x.pt",
                    "pose": "models/yolo/yolo11x-pose.pt"
                },
                "processing": {
                    "detection": {
                        "confidence_threshold": 0.3, 
                        "iou_threshold": 0.45,
                        "max_detections": 1000
                    },
                    "depth_estimation": {
                        "enabled": False,
                        "model": "midas_v21_small_256",
                        "model_path": "models/depth/midas_v21_small_256.pt"
                    },
                    "tile_inference": {
                        "enabled": False,
                        "tile_size": [640, 640],
                        "overlap": 0.1
                    }
                },
                "video_dir": "videos",
                "output_dir": "outputs",
                "logging": {
                    "level": "INFO",
                    "file": "logs/analysis.log"
                },
                "experiments": {
                    "comparison": {"type": "comparison", "description": "åŸºæœ¬æ¯”è¼ƒå®Ÿé¨“"},
                    "model_test": {"type": "model_test", "description": "ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ãƒ†ã‚¹ãƒˆ"},
                    "tile_inference": {"type": "tile_inference", "description": "ã‚¿ã‚¤ãƒ«æ¨è«–ãƒ†ã‚¹ãƒˆ"}
                }
            }
            
            logger.info("âœ… ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’é©ç”¨ã—ã¾ã—ãŸ")
            return default_config
        
        def get(self, key, default=None):
            """è¨­å®šå€¤å–å¾—ï¼ˆãƒ‰ãƒƒãƒˆè¨˜æ³•å¯¾å¿œãƒ»å®Œå…¨ç‰ˆï¼‰"""
            keys = key.split('.')
            value = self.data
            for k in keys:
                if isinstance(value, dict) and k in value:
                    value = value[k]
                else:
                    return default
            return value
        
        def get_experiment_config(self, experiment_type):
            """å®Ÿé¨“è¨­å®šå–å¾—ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
            experiment_configs = self.get("experiments", {})
            if experiment_type in experiment_configs:
                return experiment_configs[experiment_type]
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå®Ÿé¨“è¨­å®š
            return {
                "type": experiment_type,
                "basic_mode": True,
                "description": f"åŸºæœ¬å®Ÿé¨“: {experiment_type}",
                "enabled": True
            }
        
        @property
        def video_dir(self):
            return self.get("video_dir", "videos")

LOGGER_AVAILABLE = False
setup_logger = None

try:
    from utils.logger import setup_logger
    LOGGER_AVAILABLE = True
    print("âœ… setup_logger ãŒåˆ©ç”¨å¯èƒ½ã§ã™")
except ImportError as e:
    print(f"âš ï¸ setup_logger ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {e}")
    print("ğŸ”§ åŸºæœ¬ãƒ­ã‚°æ©Ÿèƒ½ã‚’ä½¿ç”¨ã—ã¾ã™")
    LOGGER_AVAILABLE = False

# ğŸ”§ åŸºæœ¬ãƒ­ã‚°è¨­å®šï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰
if not LOGGER_AVAILABLE:
    def setup_logger():
        """åŸºæœ¬ãƒ­ã‚°è¨­å®šï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        # ãƒ­ã‚°ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«å
        log_file = log_dir / f"analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # æ—¢å­˜ã®ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‚’ã‚¯ãƒªã‚¢
        logger = logging.getLogger()
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler(sys.stdout)
            ]
        )
        
        logger = logging.getLogger(__name__)
        logger.info(f"ğŸ”§ åŸºæœ¬ãƒ­ã‚°æ©Ÿèƒ½ã‚’åˆæœŸåŒ–ã—ã¾ã—ãŸ: {log_file}")
        return logger

# âœ… ã“ã“ã§ã‚¯ãƒ©ã‚¹å®šç¾©é–‹å§‹ï¼ˆifæ–‡ã®å¤–ã€ã‚¤ãƒ³ãƒ‡ãƒ³ãƒˆ0ï¼‰
class ImprovedYOLOAnalyzer:
    """
    YOLO11 åºƒè§’ã‚«ãƒ¡ãƒ©åˆ†æã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰
    - æ·±åº¦æ¨å®šçµ±åˆå¯¾å¿œ
    - ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ä¸è¶³å®Œå…¨å¯¾å¿œ
    - Stage 5/6ä¿®æ­£å®Œäº†
    - ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½å®Œå…¨çµ±åˆ
    """

    # Line 834-854ã‚’ä¿®æ­£:

    # Line 1272-1310ã®__init__ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä»¥ä¸‹ã§ç½®ãæ›ãˆ:

    def __init__(self, config_path: str = "configs/default.yaml"):
        """
        åˆæœŸåŒ–ï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰

        Args:
            config_path: è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        """
        # ğŸ”§ ãƒ­ã‚¬ãƒ¼ã‚’æœ€åˆã«åˆæœŸåŒ–
        self.logger = setup_logger()

        # ã‚¨ãƒ©ãƒ¼ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆè¨­å®š
        if ERROR_HANDLER_AVAILABLE:
            context_manager = ErrorContext("ImprovedYOLOAnalyzeråˆæœŸåŒ–", logger=self.logger)
        else:
            context_manager = self._basic_context("ImprovedYOLOAnalyzeråˆæœŸåŒ–")

        with context_manager as ctx:
            # è¨­å®šåˆæœŸåŒ–
            self.config = self._initialize_config(config_path)

            # æ·±åº¦æ¨å®šæœ‰åŠ¹æ€§ã®ç¢ºèª
            self.depth_enabled = self.config.get('processing.depth_estimation.enabled', False)
            self.logger.info(f"ğŸ” æ·±åº¦æ¨å®š: {'æœ‰åŠ¹' if self.depth_enabled else 'ç„¡åŠ¹'}")

            # ğŸ”§ å³å¯†ãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ãƒ•ãƒ©ã‚°è¿½åŠ 
            self.force_exact_model = True
            self.model_verification_results = {}

            # è©•ä¾¡å™¨ã®é¸æŠã¨åˆæœŸåŒ–
            self._initialize_evaluator(ctx)

            # ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã¨ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
            self._initialize_processor_analyzer(ctx)

            # ğŸ”§ analyzer ã®æ˜ç¤ºçš„åˆæœŸåŒ–ã‚’è¿½åŠ 
            self._initialize_analyzer(ctx)

            # ã‚¨ãƒ©ãƒ¼åé›†ç”¨
            self.error_collector = []

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
            self._setup_directories()

            # åˆæœŸåŒ–å®Œäº†å ±å‘Š
            self._report_initialization(ctx)

    def _basic_context(self, name):
        """åŸºæœ¬ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼"""
        class BasicContext:
            def __init__(self, name):
                self.name = name
                self.logger = logging.getLogger(__name__)
            def __enter__(self):
                self.logger.debug(f"ğŸ” å‡¦ç†é–‹å§‹: {self.name}")
                return self
            def __exit__(self, exc_type, exc_val, exc_tb):
                if exc_type:
                    self.logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ in {self.name}: {exc_val}")
                else:
                    self.logger.debug(f"âœ… å‡¦ç†å®Œäº†: {self.name}")
                return False
            def add_info(self, key, value):
                self.logger.debug(f"ğŸ“ {self.name} - {key}: {value}")
        return BasicContext(name)

    # Line 857ã®_initialize_configãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä¿®æ­£:

    def _initialize_config(self, config_path: str):
        """è¨­å®šåˆæœŸåŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        depth_config_path = "configs/depth_config.yaml"
    
        # âš ï¸ ã“ã“ã§ self.logger ã‚’ä½¿ã†å‰ã«ã€ãƒ­ã‚¬ãƒ¼ã‚’åˆæœŸåŒ–ã™ã‚‹å¿…è¦ãŒã‚ã‚‹
        logger = logging.getLogger(__name__)  # ğŸ”§ ä¸€æ™‚çš„ãªãƒ­ã‚¬ãƒ¼ä½¿ç”¨
        logger.info(f"âš™ï¸ è¨­å®šåˆæœŸåŒ–é–‹å§‹: {config_path}")
    
        # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®å„ªå…ˆé †ä½æ±ºå®š
        if Path(config_path).exists():
            primary_config = config_path
            logger.info(f"ğŸ“„ æŒ‡å®šè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½¿ç”¨: {config_path}")
        elif Path(depth_config_path).exists():
            primary_config = depth_config_path
            logger.info(f"ğŸ” æ·±åº¦è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•æ¤œå‡º: {depth_config_path}")
        else:
            primary_config = config_path
            logger.warning(f"âš ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {config_path}")

        # è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆåˆæœŸåŒ–
        if CONFIG_AVAILABLE and Config:
            return Config(primary_config)
        else:
            return BasicConfig(primary_config)

    def _initialize_evaluator(self, ctx):
        """è©•ä¾¡å™¨åˆæœŸåŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        if self.depth_enabled and DEPTH_EVALUATOR_AVAILABLE and DepthEnhancedEvaluator:
            try:
                self.evaluator = DepthEnhancedEvaluator(self.config)
                self.logger.info("ğŸ” æ·±åº¦çµ±åˆè©•ä¾¡å™¨ã‚’åˆæœŸåŒ–")
                if hasattr(ctx, 'add_info'):
                    ctx.add_info("evaluator_type", "DepthEnhancedEvaluator")
            except Exception as e:
                self.logger.warning(f"DepthEnhancedEvaluator åˆæœŸåŒ–å¤±æ•—: {e}")
                self._fallback_to_basic_evaluator(ctx)
        elif COMPREHENSIVE_EVALUATOR_AVAILABLE and ComprehensiveEvaluator:
            try:
                self.evaluator = ComprehensiveEvaluator(self.config)
                self.logger.info("ğŸ“Š æ¨™æº–è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–")
                if hasattr(ctx, 'add_info'):
                    ctx.add_info("evaluator_type", "ComprehensiveEvaluator")
            except Exception as e:
                self.logger.warning(f"ComprehensiveEvaluator åˆæœŸåŒ–å¤±æ•—: {e}")
                self._fallback_to_basic_evaluator(ctx)
        else:
            self._fallback_to_basic_evaluator(ctx)

    def _fallback_to_basic_evaluator(self, ctx):
        """åŸºæœ¬è©•ä¾¡å™¨ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        self.evaluator = BasicEvaluator(self.config)
        self.logger.info("ğŸ”§ åŸºæœ¬è©•ä¾¡å™¨ã‚’åˆæœŸåŒ–")
        if hasattr(ctx, 'add_info'):
            ctx.add_info("evaluator_type", "BasicEvaluator")

    def _initialize_processor_analyzer(self, ctx):
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ãƒ»ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        try:
            # Video Processor åˆæœŸåŒ–
            if VIDEO_PROCESSOR_AVAILABLE:
                self.logger.info("ğŸ¥ é«˜åº¦å‹•ç”»ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ã‚’åˆæœŸåŒ–")
                self.processor = VideoProcessor(self.config)
            else:
                self.logger.info("ğŸ”„ BasicVideoProcessor ã‚’åˆæœŸåŒ–")
                self.processor = BasicVideoProcessor(self.config)

            # ğŸ”§ analyzer ã®åˆæœŸåŒ–ã¯åˆ¥ãƒ¡ã‚½ãƒƒãƒ‰ã§è¡Œã†ï¼ˆé‡è¤‡ã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰
            # self._initialize_analyzer(ctx) ã¯ __init__ ã§å‘¼ã³å‡ºã—æ¸ˆã¿
        
            ctx.add_info("processor_type", type(self.processor).__name__)
        
        except Exception as e:
            self.logger.error(f"âŒ ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ãƒ»ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            self._fallback_processor_analyzer(ctx)

    def _fallback_processor_analyzer(self, ctx):
        """ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ãƒ»ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ç”¨ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
        try:
            self.logger.warning("ğŸ”„ åŸºæœ¬ãƒ—ãƒ­ã‚»ãƒƒã‚µãƒ¼ãƒ»ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            self.processor = BasicVideoProcessor(self.config)
        
            # analyzer ãŒæœªåˆæœŸåŒ–ã®å ´åˆã¯åˆæœŸåŒ–
            if not hasattr(self, 'analyzer') or self.analyzer is None:
                self._create_fallback_analyzer(ctx)
            
            ctx.add_info("fallback_applied", True)
        
        except Exception as e:
            self.logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¤±æ•—: {e}", exc_info=True)
            raise

    def _initialize_analyzer(self, ctx):
        """ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æå™¨åˆæœŸåŒ–ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        try:
            self.logger.info("ğŸ“Š é«˜åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆ†æå™¨ã‚’åˆæœŸåŒ–")
        
            # ğŸ”§ METRICS_ANALYZER_AVAILABLE ã®ç¢ºèª
            if METRICS_ANALYZER_AVAILABLE:
                try:
                    # é«˜åº¦åˆ†æå™¨ã®åˆæœŸåŒ–ã‚’è©¦è¡Œ
                    self.analyzer = MetricsAnalyzer(self.config)
                    self.logger.info("âœ… MetricsAnalyzeråˆæœŸåŒ–æˆåŠŸ")
                    ctx.add_info("analyzer_type", "MetricsAnalyzer")
                    return
                except Exception as e:
                    self.logger.warning(f"MetricsAnalyzeråˆæœŸåŒ–å¤±æ•—: {e}")
        
            # ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: BasicMetricsAnalyzer
            self.logger.info("ğŸ”„ BasicMetricsAnalyzerã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯")
            self.analyzer = BasicMetricsAnalyzer(self.config)
            self.logger.info("âœ… BasicMetricsAnalyzeråˆæœŸåŒ–æˆåŠŸ")
            ctx.add_info("analyzer_type", "BasicMetricsAnalyzer")
        
            # ğŸ”§ create_visualizations ãƒ¡ã‚½ãƒƒãƒ‰ã®å­˜åœ¨ç¢ºèª
            if hasattr(self.analyzer, 'create_visualizations'):
                self.logger.info("âœ… create_visualizations ãƒ¡ã‚½ãƒƒãƒ‰ç¢ºèª")
            else:
                self.logger.error("âŒ create_visualizations ãƒ¡ã‚½ãƒƒãƒ‰ãŒå­˜åœ¨ã—ã¾ã›ã‚“")
                # ğŸ”§ ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«è¿½åŠ 
                self._add_fallback_visualization_method()
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ†æå™¨åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
            # ğŸ”§ æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
            self._create_fallback_analyzer(ctx)

    def _add_fallback_visualization_method(self):
        """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã®å‹•çš„è¿½åŠ """
        def fallback_create_visualizations(detection_results, vis_dir):
            """ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–ç”Ÿæˆ"""
            try:
                self.logger.info(f"ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–ç”Ÿæˆ: {vis_dir}")
            
                from pathlib import Path
                import json
                from datetime import datetime
            
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
                vis_path = Path(str(vis_dir))
                vis_path.mkdir(parents=True, exist_ok=True)
            
                # åŸºæœ¬çµ±è¨ˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
                stats_file = vis_path / "basic_stats.json"
                basic_stats = {
                    "visualization_type": "FallbackVisualization",
                    "timestamp": datetime.now().isoformat(),
                    "detection_results_type": str(type(detection_results)),
                    "success": True
                }
            
                with open(stats_file, 'w', encoding='utf-8') as f:
                    json.dump(basic_stats, f, indent=2, ensure_ascii=False)
            
                self.logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–å®Œäº†: {stats_file}")
            
                return {
                    "success": True,
                    "basic_stats_file": str(stats_file),
                    "total_files": 1,
                    "graphs_generated": 0,
                    "fallback": True
                }
            
            except Exception as e:
                self.logger.error(f"âŒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–ã‚¨ãƒ©ãƒ¼: {e}")
                return {
                    "success": False,
                    "error": str(e),
                    "total_files": 0,
                    "graphs_generated": 0,
                    "fallback": True
                }
    
        # ğŸ”§ ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å‹•çš„ã«ãƒã‚¤ãƒ³ãƒ‰
        import types
        self.analyzer.create_visualizations = types.MethodType(fallback_create_visualizations, self.analyzer)
        self.logger.info("ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–ãƒ¡ã‚½ãƒƒãƒ‰è¿½åŠ å®Œäº†")

    def _create_fallback_analyzer(self, ctx):
        """æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå™¨ã®ä½œæˆ"""
        class FallbackAnalyzer:
            def __init__(self, config):
                self.config = config
                self.logger = logging.getLogger(__name__)
        
            def create_visualizations(self, detection_results, vis_dir):
                """æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯è¦–åŒ–"""
                try:
                    from pathlib import Path
                    import json
                    from datetime import datetime
                
                    vis_path = Path(str(vis_dir))
                    vis_path.mkdir(parents=True, exist_ok=True)
                
                    fallback_file = vis_path / "fallback_analysis.json"
                    fallback_data = {
                        "analyzer_type": "FallbackAnalyzer",
                        "timestamp": datetime.now().isoformat(),
                        "note": "åˆ†æå™¨åˆæœŸåŒ–å¤±æ•—ã®ãŸã‚æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨",
                        "detection_results_available": detection_results is not None
                    }
                
                    with open(fallback_file, 'w', encoding='utf-8') as f:
                        json.dump(fallback_data, f, indent=2, ensure_ascii=False)
                
                    return {
                        "success": True,
                        "total_files": 1,
                        "graphs_generated": 0,
                        "fallback": True,
                        "analyzer_type": "FallbackAnalyzer"
                    }
                
                except Exception as e:
                    return {
                        "success": False,
                        "error": str(e),
                        "total_files": 0,
                        "graphs_generated": 0,
                        "fallback": True
                    }
    
        self.analyzer = FallbackAnalyzer(self.config)
        self.logger.warning("âš ï¸ æœ€çµ‚ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯åˆ†æå™¨ã‚’ä½œæˆ")
        ctx.add_info("analyzer_type", "FallbackAnalyzer")

    def _setup_directories(self):
        """ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        directories = [
            "outputs/baseline",
            "outputs/experiments",
            "outputs/visualizations",
            "outputs/temp",
            "logs",
            "models/yolo",
            "models/depth"
        ]
        
        self.logger.info("ğŸ“ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—é–‹å§‹")
        
        for directory in directories:
            dir_path = Path(directory)
            dir_path.mkdir(parents=True, exist_ok=True)
            self.logger.debug(f"ğŸ“ ä½œæˆ/ç¢ºèª: {directory}")
        
        self.logger.info("âœ… ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†")

    def _report_initialization(self, ctx):
        """åˆæœŸåŒ–å®Œäº†å ±å‘Šï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        features = []
        if self.depth_enabled:
            features.append("æ·±åº¦æ¨å®š")
        if self.config.get('processing.tile_inference.enabled', False):
            features.append("ã‚¿ã‚¤ãƒ«æ¨è«–")
        
        # ä½¿ç”¨ä¸­ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ã®è¡¨ç¤º
        fallbacks = []
        if not COMPREHENSIVE_EVALUATOR_AVAILABLE:
            fallbacks.append("åŸºæœ¬è©•ä¾¡å™¨")
        if not VIDEO_PROCESSOR_AVAILABLE:
            fallbacks.append("åŸºæœ¬å‹•ç”»å‡¦ç†")
        if not METRICS_ANALYZER_AVAILABLE:
            fallbacks.append("åŸºæœ¬åˆ†æ")
        if not CONFIG_AVAILABLE:
            fallbacks.append("åŸºæœ¬è¨­å®š")
        if not LOGGER_AVAILABLE:
            fallbacks.append("åŸºæœ¬ãƒ­ã‚°")

        if features:
            self.logger.info(f"ğŸš€ ImprovedYOLOAnalyzeråˆæœŸåŒ–å®Œäº† (æ©Ÿèƒ½: {', '.join(features)})")
        else:
            self.logger.info("ğŸ“‹ ImprovedYOLOAnalyzeråˆæœŸåŒ–å®Œäº† (æ¨™æº–ãƒ¢ãƒ¼ãƒ‰)")
            
        if fallbacks:
            self.logger.info(f"ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ©Ÿèƒ½ä½¿ç”¨ä¸­: {', '.join(fallbacks)}")

        if hasattr(ctx, 'add_info'):
            ctx.add_info("depth_enabled", self.depth_enabled)
            ctx.add_info("fallback_count", len(fallbacks))

    @handle_errors(error_category=ErrorCategory.VIDEO_PROCESSING)
    def run_baseline_analysis(self, video_path: str, output_dir=None) -> Dict[str, Any]:
        """
        ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æå®Ÿè¡Œï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰

        Args:
            video_path: åˆ†æå¯¾è±¡å‹•ç”»ã®ãƒ‘ã‚¹

        Returns:
            åˆ†æçµæœè¾æ›¸
        """
        if ERROR_HANDLER_AVAILABLE:
            context_manager = ErrorContext(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æ: {Path(video_path).name}",
                                        logger=self.logger, raise_on_error=False)
        else:
            context_manager = self._basic_context(f"ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æ: {Path(video_path).name}")

        with context_manager as ctx:
            video_path = Path(video_path)
            video_name = video_path.stem

            self.logger.info(f"ğŸ¯ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æé–‹å§‹: {video_name}")

            if hasattr(ctx, 'add_info'):
                ctx.add_info("video_path", str(video_path))
                ctx.add_info("video_name", video_name)
                ctx.add_info("depth_enabled", self.depth_enabled)

            # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæº–å‚™
            if output_dir is None:
                output_dir = Path("outputs/baseline") / video_name
            output_dir = Path(output_dir)
            output_dir.mkdir(parents=True, exist_ok=True)
            frame_dir = output_dir / "frames"
            
            output_dir.mkdir(parents=True, exist_ok=True)
            frame_dir.mkdir(parents=True, exist_ok=True)

            self.logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

            try:
                # Step 1: ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º
                self.logger.info("ğŸ“¸ Step 1: ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºé–‹å§‹")
                
                # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå®Ÿè¡Œ
                frame_result = self.processor.extract_frames(video_path, frame_dir)
                
                # ğŸ”§ åŸºæœ¬çš„ãªæˆåŠŸ/å¤±æ•—ãƒã‚§ãƒƒã‚¯
                if not frame_result.get("success", False):
                    error_msg = f"ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºå¤±æ•—: {frame_result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
                    self.error_collector.append(error_msg)
                    self.logger.error(f"âŒ {error_msg}")
                    raise VideoProcessingError(error_msg)

                # ğŸ”§ ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã®å¤šé‡ç¢ºèªã‚·ã‚¹ãƒ†ãƒ 
                
                # æ–¹æ³•1: APIã‹ã‚‰è¿”å´ã•ã‚ŒãŸå€¤
                api_extracted_frames = frame_result.get("extracted_frames", 0)
                self.logger.debug(f"ğŸ“Š APIè¿”å´ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {api_extracted_frames}")
                
                # æ–¹æ³•2: ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç›´æ¥ç¢ºèª
                frame_files_jpg = list(frame_dir.glob("frame_*.jpg"))
                frame_files_jpeg = list(frame_dir.glob("frame_*.jpeg"))
                frame_files_png = list(frame_dir.glob("frame_*.png"))
                
                # ã™ã¹ã¦ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã‚’çµ±åˆ
                all_frame_files = frame_files_jpg + frame_files_jpeg + frame_files_png
                actual_frame_count = len(all_frame_files)
                
                self.logger.debug(f"ğŸ“Š ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ãƒ•ã‚¡ã‚¤ãƒ«æ•°:")
                self.logger.debug(f"  - JPGãƒ•ã‚¡ã‚¤ãƒ«: {len(frame_files_jpg)}å€‹")
                self.logger.debug(f"  - JPEGãƒ•ã‚¡ã‚¤ãƒ«: {len(frame_files_jpeg)}å€‹") 
                self.logger.debug(f"  - PNGãƒ•ã‚¡ã‚¤ãƒ«: {len(frame_files_png)}å€‹")
                self.logger.debug(f"  - åˆè¨ˆ: {actual_frame_count}å€‹")
                
                # æ–¹æ³•3: processing_statsã‹ã‚‰ã®å–å¾—
                stats_frames = 0
                if hasattr(self.processor, 'processing_stats') and self.processor.processing_stats:
                    frame_extraction_stats = self.processor.processing_stats.get("frame_extraction", {})
                    stats_frames = frame_extraction_stats.get("extracted_frames", 0)
                
                self.logger.debug(f"ğŸ“Š çµ±è¨ˆæƒ…å ±ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {stats_frames}")
                
                # ğŸ”§ æœ€ã‚‚ä¿¡é ¼ã§ãã‚‹å€¤ã‚’æ¡ç”¨
                frame_counts = [api_extracted_frames, actual_frame_count, stats_frames]
                valid_counts = [count for count in frame_counts if count > 0]
                
                if valid_counts:
                    # æœ‰åŠ¹ãªå€¤ãŒã‚ã‚‹å ´åˆã¯æœ€å¤§å€¤ã‚’æ¡ç”¨
                    final_frame_count = max(valid_counts)
                    self.logger.info(f"ğŸ“Š ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ç¢ºå®š: {final_frame_count}å€‹ï¼ˆå€™è£œ: {frame_counts}ï¼‰")
                else:
                    # ã™ã¹ã¦0ã®å ´åˆã¯è©³ç´°èª¿æŸ»
                    self.logger.warning("âš ï¸ å…¨ã¦ã®æ–¹æ³•ã§ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ãŒ0ã§ã™ã€‚è©³ç´°èª¿æŸ»ã‚’å®Ÿè¡Œ...")
                    
                    # æ–¹æ³•4: ã‚ˆã‚Šåºƒç¯„å›²ãªãƒ•ã‚¡ã‚¤ãƒ«ç¢ºèª
                    all_files = list(frame_dir.glob("*"))
                    image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp']
                    image_files = []
                    
                    for file_path in all_files:
                        if file_path.suffix.lower() in image_extensions:
                            image_files.append(file_path)
                    
                    final_frame_count = len(image_files)
                    
                    if final_frame_count > 0:
                        self.logger.info(f"ğŸ” åºƒç¯„å›²ç¢ºèªã§ç™ºè¦‹: {final_frame_count}å€‹ã®ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«")
                        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ä¾‹ã‚’è¡¨ç¤º
                        sample_files = [f.name for f in image_files[:3]]
                        self.logger.debug(f"  ã‚µãƒ³ãƒ—ãƒ«: {sample_files}")
                    else:
                        # æ–¹æ³•5: æœ€å¾Œã®æ‰‹æ®µ - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹ã®å®Œå…¨ãƒã‚§ãƒƒã‚¯
                        self.logger.error("ğŸ” æœ€çµ‚ç¢ºèª - ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…å®¹:")
                        self.logger.error(f"  - ãƒ‘ã‚¹: {frame_dir}")
                        self.logger.error(f"  - å­˜åœ¨ç¢ºèª: {frame_dir.exists()}")
                        self.logger.error(f"  - ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™: {frame_dir.is_dir() if frame_dir.exists() else 'N/A'}")
                        
                        if frame_dir.exists():
                            all_content = list(frame_dir.glob("*"))
                            self.logger.error(f"  - å…¨ãƒ•ã‚¡ã‚¤ãƒ«({len(all_content)}å€‹): {[f.name for f in all_content[:10]]}")
                            
                            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºç¢ºèª
                            for file_path in all_content[:5]:
                                if file_path.is_file():
                                    size_mb = file_path.stat().st_size / (1024 * 1024)
                                    self.logger.error(f"    {file_path.name}: {size_mb:.2f}MB")
                
                # ğŸ”§ çµæœã®è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                self.logger.info(f"âœ… Step 1å®Œäº†: {final_frame_count}ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡º")
                
                if final_frame_count > 0:
                    # æˆåŠŸæ™‚ã®çµ±è¨ˆæƒ…å ±
                    if actual_frame_count > 0 and len(all_frame_files) > 0:
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µãƒ³ãƒ—ãƒ«ã®è¡¨ç¤º
                        sample_count = min(3, len(all_frame_files))
                        sample_files = [f.name for f in all_frame_files[:sample_count]]
                        self.logger.info(f"ğŸ“ ä¿å­˜ãƒ•ã‚¡ã‚¤ãƒ«ä¾‹: {sample_files}")
                        
                        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºçµ±è¨ˆ
                        total_size = sum(f.stat().st_size for f in all_frame_files[:10])  # æœ€åˆã®10ãƒ•ã‚¡ã‚¤ãƒ«
                        avg_size_kb = (total_size / min(10, len(all_frame_files))) / 1024 if all_frame_files else 0
                        self.logger.debug(f"ğŸ“Š å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {avg_size_kb:.1f}KB")
                    
                    # ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºç‡ã®è¨ˆç®—
                    if hasattr(self.processor, 'processing_stats') and self.processor.processing_stats:
                        extraction_stats = self.processor.processing_stats.get("frame_extraction", {})
                        total_frames = extraction_stats.get("total_frames", 0)
                        if total_frames > 0:
                            extraction_rate = (final_frame_count / total_frames) * 100
                            self.logger.info(f"ğŸ“Š æŠ½å‡ºç‡: {extraction_rate:.1f}% ({final_frame_count}/{total_frames})")
                
                # ğŸ”§ ã‚¼ãƒ­ãƒ•ãƒ¬ãƒ¼ãƒ ã®å ´åˆã®ã‚¨ãƒ©ãƒ¼å‡¦ç†
                if final_frame_count == 0:
                    error_msg = "ãƒ•ãƒ¬ãƒ¼ãƒ æŠ½å‡ºæ•°ãŒ0ã§ã™ã€‚å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã¨å‡¦ç†ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                    self.error_collector.append(error_msg)
                    self.logger.error(f"âŒ {error_msg}")
                    
                    # ğŸ”§ è©³ç´°ãªãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚’å‡ºåŠ›
                    self.logger.error(f"ğŸ” ãƒ‡ãƒãƒƒã‚°æƒ…å ±è©³ç´°:")
                    self.logger.error(f"  å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«:")
                    self.logger.error(f"    - ãƒ‘ã‚¹: {video_path}")
                    self.logger.error(f"    - å­˜åœ¨: {Path(video_path).exists()}")
                    if Path(video_path).exists():
                        video_size = Path(video_path).stat().st_size / (1024 * 1024)
                        self.logger.error(f"    - ã‚µã‚¤ã‚º: {video_size:.1f}MB")
                    
                    self.logger.error(f"  å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª:")
                    self.logger.error(f"    - ãƒ‘ã‚¹: {frame_dir}")
                    self.logger.error(f"    - å­˜åœ¨: {frame_dir.exists()}")
                    self.logger.error(f"    - æ¨©é™: {oct(frame_dir.stat().st_mode)[-3:] if frame_dir.exists() else 'N/A'}")
                    
                    self.logger.error(f"  ãƒ—ãƒ­ã‚»ãƒƒã‚µæƒ…å ±:")
                    self.logger.error(f"    - ã‚¿ã‚¤ãƒ—: {type(self.processor).__name__}")
                    self.logger.error(f"    - è¨­å®š: {getattr(self.processor, 'config', 'N/A')}")
                    
                    # frame_resultã®è©³ç´°
                    self.logger.error(f"  APIå¿œç­”:")
                    self.logger.error(f"    - frame_result: {frame_result}")
                    
                    raise VideoProcessingError(error_msg)
                
                # âœ… å‡¦ç†ç¶™ç¶šã®ãŸã‚ã®ãƒ•ãƒ¬ãƒ¼ãƒ æ•°è¨˜éŒ²
                # å¾Œç¶šå‡¦ç†ã§ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ç¢ºå®šã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’ä¿å­˜
                if not hasattr(self, 'current_analysis_stats'):
                    self.current_analysis_stats = {}
                self.current_analysis_stats['extracted_frame_count'] = final_frame_count
                self.current_analysis_stats['frame_directory'] = str(frame_dir)
                
                self.logger.debug(f"ğŸ“ ç¾åœ¨ã®è§£æçµ±è¨ˆã‚’æ›´æ–°: {self.current_analysis_stats}")

                # Step 2: æ¤œå‡ºãƒ»è¿½è·¡å‡¦ç†ï¼ˆã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆç¢ºå®Ÿå–å¾—ç‰ˆï¼‰
                self.logger.info("ğŸ¯ Step 2: YOLOãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ç¢ºå®Ÿä½¿ç”¨å‡¦ç†é–‹å§‹")

                # ğŸ”§ ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹ã®ç¢ºå®Ÿãªå–å¾—
                models_config = self.config.get('models', {}) if hasattr(self.config, 'get') else {}
                pose_model_path = models_config.get('pose', 'models/yolo/yolo11x-pose.pt')

                # ğŸ”§ ä¿®æ­£: ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã®ç¢ºå®Ÿãªç¢ºèª
                if not Path(pose_model_path).exists():
                    self.logger.error(f"ğŸš¨ ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {pose_model_path}")
    
                    # ä»£æ›¿ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ã‚’æ¢ç´¢
                    alternative_paths = [
                        "models/yolo/yolo11x-pose.pt",
                        "models/yolo11x-pose.pt", 
                        "yolo11x-pose.pt",
                        "models/yolo/yolo11l-pose.pt",
                        "models/yolo11l-pose.pt",
                        "models/yolo/yolo11m-pose.pt",
                        "models/yolo11m-pose.pt"
                    ]
    
                    found_model = None
                    for alt_path in alternative_paths:
                        if Path(alt_path).exists():
                            found_model = alt_path
                            self.logger.info(f"ğŸ”§ ä»£æ›¿ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ç™ºè¦‹: {alt_path}")
                            break
    
                    if found_model:
                        pose_model_path = found_model
                        self.logger.info(f"âœ… ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãƒ‘ã‚¹æ›´æ–°: {pose_model_path}")
                    else:
                        self.logger.error("ğŸš¨ åˆ©ç”¨å¯èƒ½ãªãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                        self.logger.error("ğŸ”§ ä»¥ä¸‹ã®ãƒ‘ã‚¹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:")
                        for path in [pose_model_path] + alternative_paths:
                            self.logger.error(f"  - {path}")
                        return ResponseBuilder.error(
                            message="ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“",
                            details={
                                "original_path": pose_model_path,
                                "searched_paths": alternative_paths,
                                "suggestion": "setup.py ã‚’å®Ÿè¡Œã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„"
                            }
                        )

                # ğŸ”§ ä¿®æ­£: ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ç¢ºå®Ÿä½¿ç”¨ã®è¨­å®š
                detection_config = {
                    "confidence_threshold": 0.3,
                    "tracking_config": "bytetrack.yaml",  # ğŸ”§ ç¢ºå®Ÿã«è¨­å®š
                    "save_visualizations": True,
                    "save_detection_frames": True,
                    "force_pose_task": True,  # ğŸ”§ ãƒãƒ¼ã‚ºã‚¿ã‚¹ã‚¯å¼·åˆ¶
                    "model_verification_required": True,  # ğŸ”§ ãƒ¢ãƒ‡ãƒ«æ¤œè¨¼å¿…é ˆ
                    "keypoint_processing_enabled": True  # ğŸ”§ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‡¦ç†ç¢ºå®Ÿæœ‰åŠ¹
                }

                self.logger.info(f"ğŸ¯ æ¤œå‡ºè¨­å®š:")
                self.logger.info(f"  ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«: {pose_model_path}")
                self.logger.info(f"  tracker: {detection_config['tracking_config']}")
                self.logger.info(f"  ãƒãƒ¼ã‚ºã‚¿ã‚¹ã‚¯å¼·åˆ¶: {detection_config['force_pose_task']}")

                # ğŸš€ yolopose_analyzer ã§ã®ç¢ºå®Ÿãªã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºå®Ÿè¡Œ
                try:
                    if not YOLOPOSE_ANALYZER_AVAILABLE:
                        raise ImportError("yolopose_analyzer ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    
                    # ğŸ”§ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºã‚’ç¢ºå®Ÿã«ã™ã‚‹è¨­å®š
                    enhanced_config = {
                        "models": {
                            "pose": pose_model_path
                        },
                        "processing": {
                            "confidence_threshold": 0.3,
                            "save_keypoints": True,
                            "keypoint_format": "coco",
                            "force_keypoint_detection": True,
                            "model_policy": {
                                "verify_pose_model": True,
                                "require_keypoints": True,
                                "use_pose_model": True
                            }
                        },
                        "tracking": {
                            "tracker_type": "bytetrack",
                            "track_thresh": 0.6,
                            "track_buffer": 60,
                            "match_thresh": 0.8
                        },
                        "output": {
                            "save_visualizations": True,
                            "save_csv": True,
                            "csv_include_keypoints": True
                        },
                        "inference": {
                            "batch_size": 16,
                            "device": "auto",
                            "task": "pose"  # ğŸ”§ configã®ä¸­ã§ã‚¿ã‚¹ã‚¯æŒ‡å®š
                        }
                    }
    
                    # æ—¢å­˜è¨­å®šã¨ãƒãƒ¼ã‚¸
                    base_config = {}
                    if hasattr(self.config, '__dict__'):
                        base_config = self.config.__dict__
                    elif hasattr(self.config, 'data'):
                        base_config = self.config.data
    
                    for key, value in base_config.items():
                        if key not in enhanced_config:
                            enhanced_config[key] = value
    
                    self.logger.info("ğŸš€ ç¢ºå®Ÿã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºã‚’å®Ÿè¡Œ")
    
                    # yolopose_analyzerå®Ÿè¡Œï¼ˆã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆé‡è¦–è¨­å®šï¼‰
                    detection_result = analyze_frames_with_tracking_enhanced(
                        frame_dir=str(frame_dir),
                        result_dir=str(output_dir),
                        model_path=pose_model_path,
                        config=enhanced_config,
                        force_exact_model=True  # ğŸ”§ ç¢ºå®Ÿãªãƒ¢ãƒ‡ãƒ«ä½¿ç”¨ãƒ•ãƒ©ã‚°
                    )
    
                    processing_type = "ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆçµ±åˆ"
    
                    # ğŸ” ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºçµæœã®è©³ç´°æ¤œè¨¼
                    if detection_result.get("success", False):
                        data = detection_result.get("data", {})
                        csv_path = data.get("csv_path")
        
                        if csv_path and Path(csv_path).exists():
                            # CSVå†…å®¹ã®è©³ç´°ç¢ºèª
                            import pandas as pd
                            df = pd.read_csv(csv_path)
            
                            self.logger.info("ğŸ” ========== ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºçµæœæ¤œè¨¼ ==========")
                            self.logger.info(f"ğŸ“Š æ¤œå‡ºãƒ‡ãƒ¼ã‚¿å½¢çŠ¶: {df.shape}")
                            self.logger.info(f"ğŸ“‹ å…¨åˆ—å: {df.columns.tolist()}")
            
                            # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåˆ—ã®ç¢ºèª
                            keypoint_cols = [col for col in df.columns if 'keypoint' in col.lower() or 'kpt' in col.lower()]
                            self.logger.info(f"ğŸ¦´ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆé–¢é€£åˆ—: {len(keypoint_cols)}å€‹")
            
                            if keypoint_cols:
                                self.logger.info(f"âœ… ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºæˆåŠŸ: {keypoint_cols[:10]}...")
                
                                # 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆCOCOå½¢å¼ï¼‰ã®ç‰¹åˆ¥ç¢ºèª
                                target_keypoints = [3, 4, 5, 6]  # left_ear, right_ear, left_shoulder, right_shoulder
                                found_targets = []
                
                                for kpt_idx in target_keypoints:
                                    x_cols = [col for col in df.columns if f'keypoint_{kpt_idx}_x' in col or f'kpt_{kpt_idx}_x' in col]
                                    y_cols = [col for col in df.columns if f'keypoint_{kpt_idx}_y' in col or f'kpt_{kpt_idx}_y' in col]
                    
                                    if x_cols and y_cols:
                                        found_targets.append(f"COCO#{kpt_idx}")
                                        self.logger.info(f"âœ… COCO#{kpt_idx}ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆç™ºè¦‹: {x_cols[0]}, {y_cols[0]}")
                
                                if len(found_targets) >= 3:
                                    self.logger.info(f"ğŸ¯ 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºçŠ¶æ³: {len(found_targets)}/4ç‚¹ç™ºè¦‹")
                                    self.logger.info(f"  ç™ºè¦‹: {found_targets}")
                                else:
                                    self.logger.warning(f"âš ï¸ 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä¸å®Œå…¨: {len(found_targets)}/4ç‚¹ã®ã¿")
                                    self.logger.warning(f"  ç™ºè¦‹æ¸ˆã¿: {found_targets}")
                            else:
                                self.logger.error("âŒ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåˆ—ãŒä¸€åˆ‡æ¤œå‡ºã•ã‚Œã¦ã„ã¾ã›ã‚“ï¼")
                                self.logger.error("ğŸ”§ åŸå› : ãƒãƒ¼ã‚ºãƒ¢ãƒ‡ãƒ«ãŒæ­£ã—ãå‹•ä½œã—ã¦ã„ãªã„å¯èƒ½æ€§")
                
                                # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¡¨ç¤º
                                if not df.empty:
                                    self.logger.error("ğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒ«:")
                                    for col in df.columns[:10]:
                                        sample_value = df.iloc[0][col] if len(df) > 0 else "N/A"
                                        self.logger.error(f"  {col}: {sample_value}")
                        else:
                            self.logger.error(f"âŒ æ¤œå‡ºçµæœCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
                    else:
                        error_msg = detection_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                        self.logger.error(f"âŒ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºå¤±æ•—: {error_msg}")
                        self.error_collector.append(f"ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºå¤±æ•—: {error_msg}")
                        raise VideoProcessingError(error_msg)

                except ImportError as e:
                    self.logger.error(f"âŒ yolopose_analyzer ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
                    self.logger.warning("ğŸ”„ BasicVideoProcessor ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½åˆ¶é™ï¼‰")
    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†
                    if self.depth_enabled and hasattr(self.processor, 'run_detection_tracking_with_depth'):
                        detection_result = self.processor.run_detection_tracking_with_depth(frame_dir, video_name)
                    else:
                        detection_result = self.processor.run_detection_tracking(frame_dir, video_name)
    
                    processing_type = "åŸºæœ¬æ¤œå‡ºï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"
    
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ™‚ã®è­¦å‘Š
                    self.logger.warning("âš ï¸ yolopose_analyzerãŒåˆ©ç”¨ã§ããªã„ãŸã‚ã€ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ©Ÿèƒ½ãŒåˆ¶é™ã•ã‚Œã¾ã™")
                    self.logger.warning("ğŸ’¡ è§£æ±ºç­–: pip install yolopose-analyzer ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")

                except Exception as e:
                    self.logger.error(f"âŒ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    import traceback
                    self.logger.error(f"ğŸ”§ è©³ç´°ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯: {traceback.format_exc()}")
                    self.error_collector.append(f"ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºã‚¨ãƒ©ãƒ¼: {e}")
                    raise VideoProcessingError(f"ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºã«å¤±æ•—: {e}")

                # ğŸ”§ Step 2çµæœã®æœ€çµ‚ç¢ºèª
                if not detection_result.get("success", False):
                    error_msg = detection_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                    self.logger.error(f"âŒ {processing_type}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
                    self.error_collector.append(f"{processing_type}å‡¦ç†å¤±æ•—: {error_msg}")
                    raise VideoProcessingError(error_msg)

                self.logger.info(f"âœ… Step 2å®Œäº†: {processing_type}å‡¦ç†")

                # ğŸ”§ æ¤œå‡ºçµ±è¨ˆã®è¡¨ç¤º
                if detection_result.get("success", False):
                    data = detection_result.get("data", {})
                    detection_count = data.get("detection_count", 0)
                    frame_count = data.get("frame_count", 0)
    
                    self.logger.info(f"ğŸ“Š æ¤œå‡ºçµ±è¨ˆ:")
                    self.logger.info(f"  - ç·æ¤œå‡ºæ•°: {detection_count}")
                    self.logger.info(f"  - å‡¦ç†ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {frame_count}")
    
                    if frame_count > 0:
                        detection_rate = (detection_count / frame_count)
                        self.logger.info(f"  - ãƒ•ãƒ¬ãƒ¼ãƒ å½“ãŸã‚Šæ¤œå‡ºæ•°: {detection_rate:.2f}")

                # ğŸ¯ Step 2.5: 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå‡¦ç†ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                try:
                    original_csv = detection_result["data"]["csv_path"]
                    filtered_result = self.filter_keypoints_to_6points(original_csv, output_dir)
                    if isinstance(filtered_result, dict) and filtered_result.get("success"):
                        sixpoint_csv = filtered_result.get("sixpoint_csv")
                        metrics_csv = filtered_result.get("metrics_csv")
                        detection_result["data"]["filtered_csv_path"] = sixpoint_csv
                        detection_result["data"]["metrics_csv_path"] = metrics_csv
                        detection_result["data"]["keypoint_mode"] = "6_points"
                        if sixpoint_csv and Path(sixpoint_csv).exists():
                            self.logger.info(f"ğŸ¨ 6ç‚¹å¯è¦–åŒ–ç”Ÿæˆ: {sixpoint_csv}")
                            frame_dir = Path(output_dir) / "frames"
                            vis_result = self.create_6point_visualization(output_dir, pd.read_csv(sixpoint_csv), frame_dir)
                        else:
                            self.logger.error(f"âŒ 6ç‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {sixpoint_csv}")
                            vis_result = {"success": False, "error": "6ç‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"}
                    else:
                        self.logger.error(f"âŒ 6ç‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—: {filtered_result}")
                        vis_result = {"success": False, "error": "6ç‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¤±æ•—"}
                
                except Exception as e:
                    self.logger.error(f"âŒ 4ç‚¹å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                    vis_result = {"success": False, "error": str(e)}

                # Step 3: åŒ…æ‹¬çš„è©•ä¾¡
                self.logger.info("ğŸ“Š Step 3: åŒ…æ‹¬çš„è©•ä¾¡é–‹å§‹")
                
                # è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã®é¸æŠ
                if hasattr(self.evaluator, 'evaluate_with_depth') and self.depth_enabled:
                    evaluation_result = self.evaluator.evaluate_with_depth(
                        video_path, detection_result, video_name
                    )
                else:
                    evaluation_result = self.evaluator.evaluate_comprehensive(
                        video_path, detection_result, video_name
                    )

                if not evaluation_result.get("success", False):
                    error_msg = f"è©•ä¾¡å‡¦ç†å¤±æ•—: {evaluation_result.get('error', {}).get('message', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}"
                    self.error_collector.append(error_msg)
                    self.logger.warning(f"âš ï¸ {error_msg}")
                    # è©•ä¾¡å¤±æ•—ã¯è­¦å‘Šã«ç•™ã‚ã‚‹
                    evaluation_result = ResponseBuilder.success(data={
                        "basic_evaluation": True, 
                        "fallback": True,
                        "evaluator_type": type(self.evaluator).__name__
                    })

                self.logger.info("âœ… Step 3å®Œäº†: åŒ…æ‹¬çš„è©•ä¾¡")

                # Step 4: å¯è¦–åŒ–ç”Ÿæˆ
                self.logger.info("ğŸ“ˆ Step 4: å¯è¦–åŒ–ç”Ÿæˆé–‹å§‹")
                vis_dir = output_dir / "visualizations"
                vis_dir.mkdir(exist_ok=True)

                try:
                    # ğŸ”§ æˆ»ã‚Šå€¤ã‚’å—ã‘å–ã£ã¦è©³ç´°ãƒ­ã‚°å‡ºåŠ›
                    vis_result = self.analyzer.create_visualizations(detection_result, vis_dir)
    
                    # ğŸ”§ None ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ 
                    if vis_result is None:
                        self.logger.warning("âš ï¸ Step 4è­¦å‘Š: å¯è¦–åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ãŒNoneã‚’è¿”ã—ã¾ã—ãŸ")
                        vis_result = {"success": False, "error": "å¯è¦–åŒ–ãƒ¡ã‚½ãƒƒãƒ‰ãŒNoneã‚’è¿”ã—ã¾ã—ãŸ"}
                    elif not isinstance(vis_result, dict):
                        self.logger.warning(f"âš ï¸ Step 4è­¦å‘Š: äºˆæœŸã—ãªã„æˆ»ã‚Šå€¤å‹: {type(vis_result)}")
                        vis_result = {"success": False, "error": f"äºˆæœŸã—ãªã„æˆ»ã‚Šå€¤å‹: {type(vis_result)}"}
    
                    # ğŸ”§ å®‰å…¨ãªæˆåŠŸåˆ¤å®š
                    if vis_result.get("success", False):
                        total_files = vis_result.get("total_files", 0)
                        graphs_count = vis_result.get("graphs_generated", 0)
                        self.logger.info(f"âœ… Step 4å®Œäº†: å¯è¦–åŒ–ç”Ÿæˆ ({total_files}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«, {graphs_count}å€‹ã®ã‚°ãƒ©ãƒ•)")
                    else:
                        error_msg = vis_result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                        self.logger.warning(f"âš ï¸ Step 4è­¦å‘Š: å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ç¶™ç¶šï¼‰: {error_msg}")
                        self.error_collector.append(f"å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {error_msg}")
        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ Step 4è­¦å‘Š: å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ç¶™ç¶šï¼‰: {e}")
                    self.logger.error(f"ğŸ”§ Step 4è©³ç´°ã‚¨ãƒ©ãƒ¼: {e}", exc_info=True)
                    self.error_collector.append(f"å¯è¦–åŒ–ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
                    # ğŸ”§ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã®ãƒ€ãƒŸãƒ¼çµæœ
                    vis_result = {"success": False, "error": str(e)}

                # çµ±åˆçµæœã®æ§‹ç¯‰
                integrated_result = {
                    "success": True,
                    "video_name": video_name,
                    "video_path": str(video_path),
                    "processing_type": processing_type,
                    "depth_enabled": self.depth_enabled,
                    "output_directory": str(output_dir),
                    "frame_extraction": frame_result,
                    "detection_tracking": detection_result,
                    "evaluation": evaluation_result,
                    "visualization_path": str(vis_dir),
                    "processing_timestamp": datetime.now().isoformat(),
                    "errors": self.error_collector.copy() if self.error_collector else [],
                    "system_info": {
                        "evaluator_type": type(self.evaluator).__name__,
                        "processor_type": type(self.processor).__name__,
                        "analyzer_type": type(self.analyzer).__name__,
                        "config_type": type(self.config).__name__,
                        "module_availability": {
                            "error_handler": ERROR_HANDLER_AVAILABLE,
                            "comprehensive_evaluator": COMPREHENSIVE_EVALUATOR_AVAILABLE,
                            "depth_evaluator": DEPTH_EVALUATOR_AVAILABLE,
                            "video_processor": VIDEO_PROCESSOR_AVAILABLE,
                            "metrics_analyzer": METRICS_ANALYZER_AVAILABLE,
                            "config": CONFIG_AVAILABLE,
                            "logger": LOGGER_AVAILABLE
                        }
                    },
                    # --- ã“ã“ã‹ã‚‰ summary ã‚’è¿½åŠ  ---
                    "summary": {
                        "total_frames": detection_result.get("data", {}).get("frame_count", 0),
                        "total_detections": detection_result.get("data", {}).get("detection_count", 0),
                        "unique_ids": 0,  # ä¸‹ã§è£œå®Œ
                        "csv_path": detection_result.get("data", {}).get("csv_path", None),
                        "errors": self.error_collector.copy() if self.error_collector else [],
                    }
                }
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯IDæ•°ã‚’CSVã‹ã‚‰å–å¾—
                csv_path = integrated_result["summary"]["csv_path"]
                if csv_path and Path(csv_path).exists():
                    try:
                        import pandas as pd
                        df = pd.read_csv(csv_path)
                        if 'person_id' in df.columns:
                            integrated_result["summary"]["unique_ids"] = df['person_id'].nunique()
                    except Exception as e:
                        self.logger.warning(f"ã‚µãƒãƒªãƒ¼ç”¨CSVèª­è¾¼ã‚¨ãƒ©ãƒ¼: {e}")

                # çµæœãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
                result_file = output_dir / f"{video_name}_baseline_result.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(integrated_result, f, indent=2, ensure_ascii=False)

                if hasattr(ctx, 'add_info'):
                    ctx.add_info("result_file", str(result_file))
                    ctx.add_info("processing_success", True)

                self.logger.info(f"ğŸ‰ ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æå®Œäº†: {video_name}")
                self.logger.info(f"ğŸ“ çµæœä¿å­˜å…ˆ: {output_dir}")
                self.logger.info(f"ğŸ“„ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")

                return ResponseBuilder.success(data=integrated_result)

            except VideoProcessingError as e:
                self.logger.error(f"âŒ å‹•ç”»å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
                if hasattr(ctx, 'add_info'):
                    ctx.add_info("error_type", "VideoProcessingError")
                    ctx.add_info("error_message", str(e))
                return ResponseBuilder.error(e, suggestions=[
                    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒç ´æã—ã¦ã„ãªã„ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                    f"å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª {output_dir} ã¸ã®æ›¸ãè¾¼ã¿æ¨©é™ã‚’ç¢ºèªã—ã¦ãã ã•ã„"
                ])
            
            except Exception as e:
                self.logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
                if hasattr(ctx, 'add_info'):
                    ctx.add_info("error_type", "UnexpectedError")
                    ctx.add_info("error_message", str(e))
                return ResponseBuilder.error(e, suggestions=[
                    "ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã§è©³ç´°ãªã‚¨ãƒ©ãƒ¼æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„",
                    "å¿…è¦ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„"
                ])
    
    # Line 1100ä»˜è¿‘ï¼ˆrun_baseline_analysisãƒ¡ã‚½ãƒƒãƒ‰ã®ç›´å¾Œï¼‰ã«è¿½åŠ :

    # å®Œå…¨ç½®æ›: Line 2184-2296
    def filter_keypoints_to_6points(self, csv_path, output_dir):
        import pandas as pd
        import os

        self.logger.info("ğŸ¯ 6ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°é–‹å§‹")
        self.logger.info(f"ğŸ“‚ å…¥åŠ›CSV: {csv_path}")

        if not Path(csv_path).exists():
            self.logger.error(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã—ã¾ã›ã‚“: {csv_path}")
            raise FileNotFoundError(f"CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")

        df = pd.read_csv(csv_path)
        self.logger.info(f"ğŸ“‹ æ¤œå‡ºã•ã‚ŒãŸå…¨åˆ—: {list(df.columns)}")

        required = [
            "left_ear_x", "left_ear_y", "right_ear_x", "right_ear_y",
            "left_shoulder_x", "left_shoulder_y", "right_shoulder_x", "right_shoulder_y"
        ]
        filtered = df.dropna(subset=required, how='any').copy()

        confidence_threshold = 0.2
        for kpt in ["left_ear", "right_ear", "left_shoulder", "right_shoulder"]:
            conf_col = f"{kpt}_conf"
            if conf_col in filtered.columns:
                filtered = filtered[filtered[conf_col] >= confidence_threshold]

        # head_center, shoulder_midã‚’è¨ˆç®—ï¼ˆfilteredãŒç©ºã§ã‚‚å¿…ãšã‚«ãƒ©ãƒ ã‚’è¿½åŠ ï¼‰
        filtered["head_center_x"] = (filtered["left_ear_x"] + filtered["right_ear_x"]) / 2
        filtered["head_center_y"] = (filtered["left_ear_y"] + filtered["right_ear_y"]) / 2
        filtered["shoulder_mid_x"] = (filtered["left_shoulder_x"] + filtered["right_shoulder_x"]) / 2
        filtered["shoulder_mid_y"] = (filtered["left_shoulder_y"] + filtered["right_shoulder_y"]) / 2

        os.makedirs(output_dir, exist_ok=True)
        sixpoint_csv_path = os.path.join(output_dir, "6point_keypoints.csv")

        # ç©ºã§ã‚‚å¿…ãšã‚«ãƒ©ãƒ ã ã‘ã®DataFrameã‚’å‡ºåŠ›
        if len(filtered) == 0:
            filtered = pd.DataFrame(columns=[
                "frame", "person_id",
                "left_ear_x", "left_ear_y", "right_ear_x", "right_ear_y",
                "left_shoulder_x", "left_shoulder_y", "right_shoulder_x", "right_shoulder_y",
                "head_center_x", "head_center_y", "shoulder_mid_x", "shoulder_mid_y"
            ])
        filtered.to_csv(sixpoint_csv_path, index=False, encoding="utf-8-sig")
        self.logger.info(f"ğŸ“ 6ç‚¹ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {sixpoint_csv_path}ï¼ˆ{len(filtered)}ä»¶ï¼‰")

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚‚åŒæ§˜ã«
        if len(filtered) > 0:
            metrics_df = self._add_6point_metrics(filtered)
        else:
            metrics_df = filtered.copy()
            metrics_df["shoulder_width"] = []
            metrics_df["pose_angle"] = []
            metrics_df["keypoint_completeness"] = []
            metrics_df["pose_confidence"] = []
        metrics_csv_path = os.path.join(output_dir, "6point_metrics.csv")
        metrics_df.to_csv(metrics_csv_path, index=False, encoding="utf-8-sig")
        self.logger.info(f"ğŸ“ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ä¿å­˜: {metrics_csv_path}")

        return {
            "success": True,
            "sixpoint_csv": sixpoint_csv_path,
            "metrics_csv": metrics_csv_path,
            "valid_detections": len(filtered),
            "total_detections": len(df),
            "filter_rate": len(filtered) / len(df) if len(df) > 0 else 0
        }

    def _add_6point_metrics(self, df):
        """
        6ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå°‚ç”¨ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ï¼ˆshoulder_head_angleè¿½åŠ ãƒ»çœç•¥ãªã—å®Œå…¨ç‰ˆï¼‰
        """
        import numpy as np

        self.logger.info("ğŸ“Š 6ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—é–‹å§‹")

        metrics_df = df.copy()

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹åˆæœŸåŒ–
        metrics_df['shoulder_width'] = 0.0
        metrics_df['head_center_x'] = 0.0
        metrics_df['head_center_y'] = 0.0
        metrics_df['shoulder_mid_x'] = 0.0
        metrics_df['shoulder_mid_y'] = 0.0
        metrics_df['pose_angle'] = 0.0
        metrics_df['keypoint_completeness'] = 0.0
        metrics_df['pose_confidence'] = 0.0
        metrics_df['shoulder_head_angle'] = 0.0  # â˜…ãªã™è§’

        calculated_count = 0
        shoulder_width_count = 0
        head_position_count = 0
        pose_angle_count = 0
        angle_count = 0

        for idx, row in metrics_df.iterrows():
            try:
                # è‚©å¹…
                if ('left_shoulder_x' in row and 'right_shoulder_x' in row and
                    'left_shoulder_y' in row and 'right_shoulder_y' in row):
                    left_x, left_y = float(row['left_shoulder_x']), float(row['left_shoulder_y'])
                    right_x, right_y = float(row['right_shoulder_x']), float(row['right_shoulder_y'])
                    if left_x > 0 and left_y > 0 and right_x > 0 and right_y > 0:
                        shoulder_width = np.sqrt((right_x - left_x) ** 2 + (right_y - left_y) ** 2)
                        metrics_df.at[idx, 'shoulder_width'] = shoulder_width
                        shoulder_width_count += 1

                # head_center
                if ('left_ear_x' in row and 'right_ear_x' in row and
                    'left_ear_y' in row and 'right_ear_y' in row):
                    left_ear_x, left_ear_y = float(row['left_ear_x']), float(row['left_ear_y'])
                    right_ear_x, right_ear_y = float(row['right_ear_x']), float(row['right_ear_y'])
                    if left_ear_x > 0 and left_ear_y > 0 and right_ear_x > 0 and right_ear_y > 0:
                        head_center_x = (left_ear_x + right_ear_x) / 2
                        head_center_y = (left_ear_y + right_ear_y) / 2
                        metrics_df.at[idx, 'head_center_x'] = head_center_x
                        metrics_df.at[idx, 'head_center_y'] = head_center_y
                        head_position_count += 1

                # ä¸¡è‚©ã®ä¸­ç‚¹
                if ('left_shoulder_x' in row and 'right_shoulder_x' in row and
                    'left_shoulder_y' in row and 'right_shoulder_y' in row):
                    left_x, left_y = float(row['left_shoulder_x']), float(row['left_shoulder_y'])
                    right_x, right_y = float(row['right_shoulder_x']), float(row['right_shoulder_y'])
                    if left_x > 0 and left_y > 0 and right_x > 0 and right_y > 0:
                        shoulder_mid_x = (left_x + right_x) / 2
                        shoulder_mid_y = (left_y + right_y) / 2
                        metrics_df.at[idx, 'shoulder_mid_x'] = shoulder_mid_x
                        metrics_df.at[idx, 'shoulder_mid_y'] = shoulder_mid_y

                # å§¿å‹¢è§’åº¦ï¼ˆè‚©ãƒ©ã‚¤ãƒ³ï¼‰
                if (metrics_df.at[idx, 'shoulder_width'] > 0 and
                    'left_shoulder_x' in row and 'right_shoulder_x' in row and
                    'left_shoulder_y' in row and 'right_shoulder_y' in row):
                    left_x, left_y = float(row['left_shoulder_x']), float(row['left_shoulder_y'])
                    right_x, right_y = float(row['right_shoulder_x']), float(row['right_shoulder_y'])
                    if left_x > 0 and right_x > 0:
                        angle_rad = np.arctan2(right_y - left_y, right_x - left_x)
                        angle_deg = np.degrees(angle_rad)
                        metrics_df.at[idx, 'pose_angle'] = angle_deg
                        pose_angle_count += 1

                # â˜…è‚©ã®ä¸­ç‚¹ã¨head_centerã®ãªã™è§’
                sx, sy = metrics_df.at[idx, 'shoulder_mid_x'], metrics_df.at[idx, 'shoulder_mid_y']
                hx, hy = metrics_df.at[idx, 'head_center_x'], metrics_df.at[idx, 'head_center_y']
                if sx > 0 and sy > 0 and hx > 0 and hy > 0:
                    dx = hx - sx
                    dy = sy - hy  # yè»¸åè»¢è€ƒæ…®
                    theta = np.degrees(np.arctan2(dy, dx))  # æ°´å¹³å³å‘ã0åº¦ã€ä¸Šå‘ãæ­£
                    metrics_df.at[idx, 'shoulder_head_angle'] = theta
                    angle_count += 1

                # ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå®Œå…¨æ€§ã‚¹ã‚³ã‚¢
                available_keypoints = [
                    'left_ear', 'right_ear', 'left_shoulder', 'right_shoulder',
                    'head_center', 'shoulder_mid'
                ]
                valid_keypoints = 0
                total_keypoints = len(available_keypoints)
                for kpt in ['left_ear', 'right_ear', 'left_shoulder', 'right_shoulder']:
                    x_col, y_col = f"{kpt}_x", f"{kpt}_y"
                    if (x_col in row and y_col in row):
                        if float(row[x_col]) > 0 and float(row[y_col]) > 0:
                            valid_keypoints += 1
                if ('head_center_x' in row and 'head_center_y' in row):
                    if float(row['head_center_x']) > 0 and float(row['head_center_y']) > 0:
                        valid_keypoints += 1
                if ('shoulder_mid_x' in row and 'shoulder_mid_y' in row):
                    if float(row['shoulder_mid_x']) > 0 and float(row['shoulder_mid_y']) > 0:
                        valid_keypoints += 1

                completeness = valid_keypoints / total_keypoints
                metrics_df.at[idx, 'keypoint_completeness'] = completeness

                # ãƒãƒ¼ã‚ºä¿¡é ¼åº¦
                pose_confidence = float(row['conf']) * completeness if 'conf' in row else completeness
                metrics_df.at[idx, 'pose_confidence'] = pose_confidence

                calculated_count += 1
    
            except Exception as row_error:
                self.logger.debug(f"è¡Œ {idx} ã®6ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—ã‚¨ãƒ©ãƒ¼: {row_error}")
                continue

        # çµ±è¨ˆãƒ­ã‚°
        total_rows = len(metrics_df)
        self.logger.info(f"ğŸ“Š 6ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—å®Œäº†:")
        self.logger.info(f"  å‡¦ç†è¡Œæ•°: {calculated_count}/{total_rows}")
        self.logger.info(f"  è‚©å¹…è¨ˆç®—: {shoulder_width_count}è¡Œ")
        self.logger.info(f"  é ­éƒ¨ä½ç½®: {head_position_count}è¡Œ")
        self.logger.info(f"  å§¿å‹¢è§’åº¦: {pose_angle_count}è¡Œ")
        self.logger.info(f"  ãªã™è§’è¨ˆç®—: {angle_count}è¡Œ")

        if calculated_count > 0:
            avg_shoulder_width = metrics_df[metrics_df['shoulder_width'] > 0]['shoulder_width'].mean()
            avg_completeness = metrics_df['keypoint_completeness'].mean()
            avg_pose_conf = metrics_df['pose_confidence'].mean()
            avg_angle = metrics_df['shoulder_head_angle'].mean()
            self.logger.info(f"ğŸ“Š ãƒ¡ãƒˆãƒªã‚¯ã‚¹çµ±è¨ˆ:")
            self.logger.info(f"  å¹³å‡è‚©å¹…: {avg_shoulder_width:.1f}px")
            self.logger.info(f"  å¹³å‡å®Œå…¨æ€§: {avg_completeness:.2f}")
            self.logger.info(f"  å¹³å‡ãƒãƒ¼ã‚ºä¿¡é ¼åº¦: {avg_pose_conf:.2f}")
            self.logger.info(f"  å¹³å‡ãªã™è§’: {avg_angle:.2f}åº¦")

        return metrics_df

    def create_6point_visualization(self, output_dir, keypoints_df, frame_dir, log_path=None, apply_undistort=True):
        import cv2
        from pathlib import Path
        # from utils.camera_calibration import undistort_with_json

        vis_dir = Path(output_dir) / "visualized_frames_6points"
        vis_dir.mkdir(parents=True, exist_ok=True)

        if keypoints_df.empty:
            self.logger.warning("âš ï¸ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ãŒç©ºã§ã™ã€‚å¯è¦–åŒ–ç”»åƒã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã€‚")
            return {"success": False, "output_dir": str(vis_dir), "saved_count": 0}

        saved_count = 0

        for frame_name in keypoints_df["frame"].unique():
            frame_path = Path(frame_dir) / frame_name
            if not frame_path.exists():
                self.logger.warning(f"âš ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {frame_path}")
                continue

            frame = cv2.imread(str(frame_path))
            # --- ä¿®æ­£: apply_undistortãƒ•ãƒ©ã‚°ã§åˆ¶å¾¡ ---
            if apply_undistort:
                from utils.camera_calibration import undistort_with_json
                frame = undistort_with_json(frame, calib_path="configs/camera_params.json")

            rows = keypoints_df[keypoints_df['frame'] == frame_name]
            for _, row in rows.iterrows():
                keypoints = {
                    "left_ear": (row["left_ear_x"], row["left_ear_y"], row.get("left_ear_conf", 1.0)),
                    "right_ear": (row["right_ear_x"], row["right_ear_y"], row.get("right_ear_conf", 1.0)),
                    "left_shoulder": (row["left_shoulder_x"], row["left_shoulder_y"], row.get("left_shoulder_conf", 1.0)),
                    "right_shoulder": (row["right_shoulder_x"], row["right_shoulder_y"], row.get("right_shoulder_conf", 1.0)),
                }
                frame = self.draw_6point_keypoints(frame, keypoints, row)

            output_filename = f"6pt_{frame_name}"
            output_path = vis_dir / output_filename
            cv2.imwrite(str(output_path), frame)
            saved_count += 1

        self.logger.info(f"âœ… 6ç‚¹å¯è¦–åŒ–ç”»åƒã‚’{saved_count}æšä¿å­˜ã—ã¾ã—ãŸï¼ˆ{vis_dir}ï¼‰")
        return {"success": True, "output_dir": str(vis_dir), "saved_count": saved_count}
    
    def draw_6point_keypoints(self, frame, keypoints, row, log_path=None):
        """
        6ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆï¼ˆä¸¡è‚©ãƒ»ä¸¡è€³ãƒ»head_centerãƒ»ä¸¡è‚©ã®ä¸­ç‚¹ï¼‰ã‚’ã‚·ãƒ³ãƒ—ãƒ«ãªè‰²ã§æç”»ã—ã€
        æ¤œå‡ºæ ã¨IDã‚‚ç”»åƒä¸Šã«è¡¨ç¤ºã™ã‚‹
        """
        import cv2
        import json

        # ã‚·ãƒ³ãƒ—ãƒ«ãªè‰²è¨­å®š
        kpt_color = (255, 0, 0)      # é’ï¼ˆå…¨ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆå…±é€šï¼‰
        bbox_color = (0, 255, 0)     # ç·‘ï¼ˆæ¤œå‡ºæ ï¼‰
        id_color = (0, 0, 255)       # èµ¤ï¼ˆIDï¼‰

        # ä¸¡è€³ãƒ»ä¸¡è‚©ã®åº§æ¨™å–å¾—
        left_ear = keypoints.get('left_ear', None)
        right_ear = keypoints.get('right_ear', None)
        left_shoulder = keypoints.get('left_shoulder', None)
        right_shoulder = keypoints.get('right_shoulder', None)

        # head_center
        head_center_x = row.get('head_center_x')
        head_center_y = row.get('head_center_y')
        head_center = None
        if head_center_x is not None and head_center_y is not None:
            head_center = (int(head_center_x), int(head_center_y))

        # ä¸¡è‚©ã®ä¸­ç‚¹
        shoulder_midpoint = None
        if left_shoulder and right_shoulder:
            shoulder_midpoint = (
                int((left_shoulder[0] + right_shoulder[0]) / 2),
                int((left_shoulder[1] + right_shoulder[1]) / 2)
            )

        # 4ç‚¹ï¼‹head_centerï¼‹ä¸¡è‚©ä¸­ç‚¹ã‚’æç”»ï¼ˆå…¨ã¦åŒã˜è‰²ãƒ»ã‚·ãƒ³ãƒ—ãƒ«ï¼‰
        for kpt_name, (x, y, conf) in keypoints.items():
            if x > 0 and y > 0:
                cv2.circle(frame, (int(x), int(y)), 6, kpt_color, -1)

        if head_center:
            cv2.circle(frame, head_center, 8, kpt_color, -1)
        if shoulder_midpoint:
            cv2.circle(frame, shoulder_midpoint, 8, kpt_color, -1)

        # æ¤œå‡ºæ æç”»
        if all(k in row for k in ["x1", "y1", "x2", "y2"]):
            try:
                x1, y1, x2, y2 = int(row["x1"]), int(row["y1"]), int(row["x2"]), int(row["y2"])
                cv2.rectangle(frame, (x1, y1), (x2, y2), bbox_color, 2)
            except Exception:
                pass

        # IDè¡¨ç¤º
        if "person_id" in row and row["person_id"] is not None:
            pid = str(row["person_id"])
            # æ ã®å·¦ä¸Š or ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆã®è¿‘ãã«è¡¨ç¤º
            if all(k in row for k in ["x1", "y1"]):
                pos = (int(row["x1"]), max(0, int(row["y1"]) - 10))
            elif left_shoulder:
                pos = (int(left_shoulder[0]), int(left_shoulder[1]) - 10)
            else:
                pos = (10, 30)
            cv2.putText(frame, f"ID:{pid}", pos, cv2.FONT_HERSHEY_SIMPLEX, 0.8, id_color, 2)

        # ãƒ­ã‚°ä¿å­˜ï¼ˆå¿…è¦ãªã‚‰ï¼‰
        log_data = {
            "frame": row.get("frame"),
            "person_id": row.get("person_id"),
            "left_ear": left_ear[:2] if left_ear else None,
            "right_ear": right_ear[:2] if right_ear else None,
            "left_shoulder": left_shoulder[:2] if left_shoulder else None,
            "right_shoulder": right_shoulder[:2] if right_shoulder else None,
            "head_center": head_center,
            "shoulder_midpoint": shoulder_midpoint,
        }
        if log_path:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(json.dumps(log_data, ensure_ascii=False) + "\n")

        return frame

    def draw_4point_keypoints_dynamic(self, frame, keypoint_data, row):
        """å‹•çš„4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæç”»"""
        try:
            import cv2
            import numpy as np
        
            # âš¡ ã‚ˆã‚Šç›®ç«‹ã¤è‰²ã¨ã‚µã‚¤ã‚º
            ear_color = (0, 255, 0)       # ç·‘ï¼ˆè€³ï¼‰
            shoulder_color = (0, 100, 255) # ã‚ªãƒ¬ãƒ³ã‚¸ï¼ˆè‚©ï¼‰
            line_color = (0, 255, 255)     # é»„ï¼ˆç·šï¼‰
            text_color = (255, 255, 255)   # ç™½ï¼ˆãƒ†ã‚­ã‚¹ãƒˆï¼‰
        
            # âš¡ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæç”»
            ear_points = []
            shoulder_points = []
        
            for name, (x, y, conf) in keypoint_data.items():
                color = ear_color if 'ear' in name else shoulder_color
            
                # âš¡ å¤§ããªå††ã§æç”»
                cv2.circle(frame, (x, y), 8, color, -1)  
                cv2.circle(frame, (x, y), 12, text_color, 2)
            
                # âš¡ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆåã¨ä¿¡é ¼åº¦
                cv2.putText(frame, f"{name.split('_')[0]}:{conf:.2f}", 
                        (x + 10, y - 10),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
            
                # ç‚¹ã‚’åˆ†é¡
                if 'ear' in name:
                    ear_points.append((x, y))
                elif 'shoulder' in name:
                    shoulder_points.append((x, y))
        
            # âš¡ è‚©ãƒ©ã‚¤ãƒ³
            if len(shoulder_points) == 2:
                cv2.line(frame, shoulder_points[0], shoulder_points[1], line_color, 4)
        
            # âš¡ é ­éƒ¨ä¸­å¿ƒ
            if len(ear_points) == 2:
                head_x = (ear_points[0][0] + ear_points[1][0]) // 2
                head_y = (ear_points[0][1] + ear_points[1][1]) // 2
                cv2.circle(frame, (head_x, head_y), 6, line_color, -1)
        
            # äººç‰©IDè¡¨ç¤º
            person_id = row.get('person_id', -1)
            if person_id != -1 and keypoint_data:
                all_points = list(keypoint_data.values())
                center_x = int(np.mean([p[0] for p in all_points]))
                center_y = int(np.mean([p[1] for p in all_points])) - 30
            
                cv2.putText(frame, f"ID:{person_id}", (center_x, center_y),
                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, text_color, 2)
        
            return frame
        
        except Exception as e:
            self.logger.warning(f"å‹•çš„æç”»ã‚¨ãƒ©ãƒ¼: {e}")
            return frame

    @handle_errors(error_category=ErrorCategory.EXPERIMENT)
    def run_experiment(self, video_path: str, experiment_type: str) -> Dict[str, Any]:
        """
        å®Ÿé¨“åˆ†æå®Ÿè¡Œï¼ˆå®Œå…¨çµ±åˆç‰ˆï¼‰

        Args:
            video_path: åˆ†æå¯¾è±¡å‹•ç”»ã®ãƒ‘ã‚¹
            experiment_type: å®Ÿé¨“ã‚¿ã‚¤ãƒ—

        Returns:
            å®Ÿé¨“çµæœè¾æ›¸
        """
        if ERROR_HANDLER_AVAILABLE:
            context_manager = ErrorContext(f"å®Ÿé¨“åˆ†æ: {experiment_type}", 
                                        logger=self.logger, raise_on_error=False)
        else:
            context_manager = self._basic_context(f"å®Ÿé¨“åˆ†æ: {experiment_type}")

        with context_manager as ctx:
            video_path = Path(video_path)
            video_name = video_path.stem

            self.logger.info(f"ğŸ§ª å®Ÿé¨“åˆ†æé–‹å§‹: {experiment_type} - {video_name}")

            if hasattr(ctx, 'add_info'):
                ctx.add_info("video_path", str(video_path))
                ctx.add_info("experiment_type", experiment_type)
                ctx.add_info("depth_enabled", self.depth_enabled)

            try:
                # å®Ÿé¨“ç”¨å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
                output_dir = Path("outputs/experiments") / experiment_type / video_name
                output_dir.mkdir(parents=True, exist_ok=True)

                self.logger.info(f"ğŸ“ å®Ÿé¨“å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

                # å®Ÿé¨“è¨­å®šã®å–å¾—
                if hasattr(self.config, 'get_experiment_config'):
                    experiment_config = self.config.get_experiment_config(experiment_type)
                else:
                    experiment_config = {"type": experiment_type, "basic_mode": True}

                self.logger.info(f"âš™ï¸ å®Ÿé¨“è¨­å®š: {experiment_config}")

                # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœã¨ã®æ¯”è¼ƒç”¨ã«ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
                self.logger.info("ğŸ“Š ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœå–å¾—ä¸­...")
                baseline_result = self.run_baseline_analysis(video_path)
                
                if not baseline_result.get("success", False):
                    raise VideoProcessingError("ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æã«å¤±æ•—ã—ã¾ã—ãŸ")

                self.logger.info("âœ… ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœå–å¾—å®Œäº†")

                # å®Ÿé¨“ç‰¹æœ‰ã®å‡¦ç†
                experiment_result = {
                    "success": True,
                    "experiment_type": experiment_type,
                    "video_name": video_name,
                    "baseline_comparison": baseline_result.get("data", {}),
                    "experiment_config": experiment_config,
                    "depth_enabled": self.depth_enabled,
                    "output_directory": str(output_dir),
                    "processing_timestamp": datetime.now().isoformat(),
                    "system_info": {
                        "evaluator_type": type(self.evaluator).__name__,
                        "processor_type": type(self.processor).__name__,
                        "analyzer_type": type(self.analyzer).__name__
                    }
                }

                # æ”¹å–„åˆ†æ
                try:
                    self.logger.info("ğŸ“ˆ æ”¹å–„åˆ†æé–‹å§‹...")
                    improvement_analysis = self.analyzer.analyze_improvements({
                        "baseline": baseline_result.get("data", {}),
                        "experiment": experiment_result
                    })
                    experiment_result["improvement_analysis"] = improvement_analysis
                    self.logger.info("âœ… æ”¹å–„åˆ†æå®Œäº†")
                except Exception as e:
                    self.logger.warning(f"âš ï¸ æ”¹å–„åˆ†æã‚¨ãƒ©ãƒ¼ï¼ˆå‡¦ç†ç¶™ç¶šï¼‰: {e}")
                    experiment_result["improvement_analysis"] = {"error": str(e)}

                # çµæœä¿å­˜
                result_file = output_dir / f"{video_name}_{experiment_type}_result.json"
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(experiment_result, f, indent=2, ensure_ascii=False)

                if hasattr(ctx, 'add_info'):
                    ctx.add_info("result_file", str(result_file))

                self.logger.info(f"ğŸ‰ å®Ÿé¨“åˆ†æå®Œäº†: {experiment_type} - {video_name}")
                self.logger.info(f"ğŸ“„ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")
                return ResponseBuilder.success(data=experiment_result)

            except Exception as e:
                self.logger.error(f"âŒ å®Ÿé¨“åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
                if hasattr(ctx, 'add_info'):
                    ctx.add_info("error_type", type(e).__name__)
                    ctx.add_info("error_message", str(e))
                return ResponseBuilder.error(e, suggestions=[
                    f"å®Ÿé¨“ã‚¿ã‚¤ãƒ— '{experiment_type}' ã®è¨­å®šã‚’ç¢ºèªã—ã¦ãã ã•ã„",
                    "ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æãŒæ­£å¸¸ã«å‹•ä½œã™ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„"
                ])

    def generate_error_report(self) -> Dict[str, Any]:
        """ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        try:
            self.logger.info("ğŸ“‹ ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆé–‹å§‹")
            
            error_report = {
                "timestamp": datetime.now().isoformat(),
                "total_errors": len(self.error_collector),
                "errors": self.error_collector.copy(),
                "system_info": {
                    "depth_enabled": self.depth_enabled,
                    "evaluator_type": type(self.evaluator).__name__,
                    "processor_type": type(self.processor).__name__,
                    "analyzer_type": type(self.analyzer).__name__,
                    "config_type": type(self.config).__name__
                },
                "module_availability": {
                    "error_handler": ERROR_HANDLER_AVAILABLE,
                    "evaluator": EVALUATOR_AVAILABLE,
                    "comprehensive_evaluator": COMPREHENSIVE_EVALUATOR_AVAILABLE,
                    "depth_evaluator": DEPTH_EVALUATOR_AVAILABLE,
                    "video_processor": VIDEO_PROCESSOR_AVAILABLE,
                    "metrics_analyzer": METRICS_ANALYZER_AVAILABLE,
                    "config": CONFIG_AVAILABLE,
                    "logger": LOGGER_AVAILABLE
                },
                "performance_info": {
                    "processing_stats": getattr(self.processor, 'processing_stats', {}),
                    "error_count_by_type": self._categorize_errors()
                }
            }
            
            # ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            report_file = Path("logs") / f"error_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            report_file.parent.mkdir(exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(error_report, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"âœ… ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_file}")
            return error_report
            
        except Exception as e:
            self.logger.error(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå¤±æ•—: {e}")
            return {"error": str(e)}

    def _categorize_errors(self) -> Dict[str, int]:
        """ã‚¨ãƒ©ãƒ¼ã®åˆ†é¡ï¼ˆå®Œå…¨ç‰ˆï¼‰"""
        categories = {
            "video_processing": 0,
            "model_loading": 0,
            "evaluation": 0,
            "configuration": 0,
            "depth_processing": 0,
            "io_operations": 0,
            "other": 0
        }
        
        for error in self.error_collector:
            error_lower = error.lower()
            if any(keyword in error_lower for keyword in ["video", "frame", "opencv", "mp4", "avi"]):
                categories["video_processing"] += 1
            elif any(keyword in error_lower for keyword in ["model", "yolo", "loading", "pt", "weights"]):
                categories["model_loading"] += 1
            elif any(keyword in error_lower for keyword in ["evaluation", "csv", "analysis", "metrics"]):
                categories["evaluation"] += 1
            elif any(keyword in error_lower for keyword in ["config", "setting", "yaml", "json"]):
                categories["configuration"] += 1
            elif any(keyword in error_lower for keyword in ["depth", "midas", "disparity"]):
                categories["depth_processing"] += 1
            elif any(keyword in error_lower for keyword in ["file", "directory", "path", "permission"]):
                categories["io_operations"] += 1
            else:
                categories["other"] += 1
        
        return categories

    def get_video_files(self) -> List[Path]:
        """å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ï¼ˆä¿®æ­£ç‰ˆãƒ»åŸºæœ¬æ¨è«–å„ªå…ˆï¼‰"""
        try:
            # ğŸ”§ --video ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ã€ãã®ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å‡¦ç†
            if hasattr(sys, 'argv') and '--video' in ' '.join(sys.argv):
                # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã‹ã‚‰å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç›´æ¥å–å¾—ã™ã‚‹å ´åˆã¯
                # ãƒ¡ã‚¤ãƒ³é–¢æ•°ã§å‡¦ç†ã•ã‚Œã‚‹ã®ã§ã€ã“ã“ã§ã¯ç©ºãƒªã‚¹ãƒˆã‚’è¿”ã™
                return []
        
            # é€šå¸¸ã®å‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢
            if hasattr(self.config, 'video_dir'):
                video_dir = Path(self.config.video_dir)
            else:
                video_dir = Path(self.config.get("video_dir", "videos"))
            
            self.logger.info(f"ğŸ¥ å‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ¤œç´¢: {video_dir}")
            
            if not video_dir.exists():
                self.logger.warning(f"âš ï¸ å‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {video_dir}")
                return []
            
            # ã‚µãƒãƒ¼ãƒˆã™ã‚‹å‹•ç”»å½¢å¼
            video_extensions = ['.mp4', '.avi', '.mov', '.mkv', '.wmv', '.flv', '.webm']
            video_files = []
        
            for ext in video_extensions:
                found_files = list(video_dir.glob(f"*{ext}"))
                found_files.extend(video_dir.glob(f"*{ext.upper()}"))
                video_files.extend(found_files)
            
            video_files = sorted(set(video_files))
        
            if not video_files:
                self.logger.warning(f"âš ï¸ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {video_dir}")
                return []
        
            self.logger.info(f"âœ… å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ç™ºè¦‹: {len(video_files)}å€‹")
            for video_file in video_files:
                file_size_mb = video_file.stat().st_size / 1024 / 1024
                self.logger.debug(f"  ğŸ“¹ {video_file.name} ({file_size_mb:.1f}MB)")
            
            return video_files
        
        except Exception as e:
            self.logger.error(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            return []

def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°ï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§æ¯å›çµæœã‚’ä¿å­˜ã—ã€éå»ã®çµæœã‚’æ®‹ã™ä»•æ§˜ï¼‰
    æ—¢å­˜ã®å‡¦ç†ãƒ»è¨­å®šãƒ»ãƒ­ã‚°ãƒ»ã‚µãƒãƒªãƒ¼å‡ºåŠ›ãªã©ã¯ãã®ã¾ã¾ç¶­æŒã—ã¤ã¤ã€å…¨æˆæœç‰©ã‚’
    outputs/baseline/å‹•ç”»å_ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—/
    ã«ä¿å­˜ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
    """
    print("""
    â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
    â•‘                     ğŸ¯ YOLO11 å§¿å‹¢åˆ†æã‚·ã‚¹ãƒ†ãƒ  v2.1                    â•‘
    â•‘                        ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæ¤œå‡ºãƒ»è¿½è·¡ãƒ»è§£æ                        â•‘
    â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    parser = argparse.ArgumentParser(
        description="ğŸ¯ YOLO11å§¿å‹¢åˆ†æã‚·ã‚¹ãƒ†ãƒ  - å‹•ç”»ã‹ã‚‰äººç‰©ã®å§¿å‹¢ã‚’åˆ†æã—ã¾ã™",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
    python improved_main.py input.mp4
    python improved_main.py input.mp4 --use-4points --keypoint-threshold 0.5
    python improved_main.py input.mp4 --enable-depth --depth-model dpt_hybrid
    python improved_main.py input.mp4 --config custom_config.yaml
    python improved_main.py input.mp4 --resolution 1920x1080 --quality high
    python improved_main.py --csv step6_xxxx/results.csv --frames-dir step6_xxxx/frames
        """
    )

    parser.add_argument('video_path', type=str, nargs='?', help='ğŸ¬ åˆ†æå¯¾è±¡ã®å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹')
    parser.add_argument('--config', type=str, default=None, help='âš™ï¸ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆYAML/JSONå½¢å¼ï¼‰')
    parser.add_argument('--output-dir', type=str, default=None, help='ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª')
    parser.add_argument('--use-4points', action='store_true', help='ğŸ¦´ 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–')
    parser.add_argument('--keypoint-threshold', type=float, default=0.3, help='ğŸ¯ ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆä¿¡é ¼åº¦é–¾å€¤')
    parser.add_argument('--disable-shoulder-metrics', action='store_true', help='ğŸš« è‚©å¹…ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--disable-head-tracking', action='store_true', help='ğŸš« é ­éƒ¨è¿½è·¡æ©Ÿèƒ½ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--enable-depth', action='store_true', help='ğŸŒŠ æ·±åº¦æ¨å®šæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–')
    parser.add_argument('--depth-model', type=str, default='dpt_hybrid', choices=['dpt_hybrid', 'midas', 'dpt_large'], help='ğŸ§  æ·±åº¦æ¨å®šãƒ¢ãƒ‡ãƒ«ã®é¸æŠ')
    parser.add_argument('--resolution', type=str, default=None, help='ğŸ“ å‡¦ç†è§£åƒåº¦ï¼ˆä¾‹: 1920x1080, 1280x720ï¼‰')
    parser.add_argument('--fps', type=int, default=None, help='ğŸ¬ å‡¦ç†FPS')
    parser.add_argument('--quality', type=str, default='medium', choices=['low', 'medium', 'high', 'ultra'], help='ğŸ¨ å‡¦ç†å“è³ªãƒ¬ãƒ™ãƒ«')
    parser.add_argument('--skip-frames', type=int, default=0, help='â­ï¸ ã‚¹ã‚­ãƒƒãƒ—ãƒ•ãƒ¬ãƒ¼ãƒ æ•°')
    parser.add_argument('--debug', action='store_true', help='ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ã‚’æœ‰åŠ¹åŒ–')
    parser.add_argument('--log-level', type=str, default='INFO', choices=['DEBUG', 'INFO', 'WARNING', 'ERROR'], help='ğŸ“ ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«')
    parser.add_argument('--save-intermediate', action='store_true', help='ğŸ’¾ ä¸­é–“ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜')
    parser.add_argument('--model-size', type=str, default='x', choices=['n', 's', 'm', 'l', 'x'], help='ğŸ¯ YOLOãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚º')
    parser.add_argument('--confidence-threshold', type=float, default=0.3, help='ğŸ¯ æ¤œå‡ºä¿¡é ¼åº¦é–¾å€¤')
    parser.add_argument('--iou-threshold', type=float, default=0.45, help='ğŸ“ IoUé–¾å€¤')
    parser.add_argument('--disable-visualization', action='store_true', help='ğŸš« å¯è¦–åŒ–å‡ºåŠ›ã‚’ç„¡åŠ¹åŒ–')
    parser.add_argument('--output-format', type=str, default='csv', choices=['csv', 'json', 'both'], help='ğŸ“Š å‡ºåŠ›ãƒ‡ãƒ¼ã‚¿å½¢å¼')
    parser.add_argument('--csv', type=str, default=None, help='ğŸ“Š æ—¢å­˜æ¤œå‡ºçµæœCSVï¼ˆå‹•ç”»æ¨è«–ã›ãšå¯è¦–åŒ–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿å®Ÿè¡Œï¼‰')
    parser.add_argument('--frames-dir', type=str, default=None, help='ğŸ–¼ï¸ ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆCSVã¨åˆã‚ã›ã¦æŒ‡å®šï¼‰')

    args = parser.parse_args()

    # --- ã“ã“ã‹ã‚‰æ­£è¦åŒ–å‡¦ç†ã®åˆ†å²ã‚’è¿½åŠ  ---
    print("æ­£è¦åŒ–å‡¦ç†ã‚’ä½¿ã„ã¾ã™ã‹ï¼Ÿ(y/n): ", end="")
    use_normalization = input().strip().lower() == "y"
    normalization_params = {}
    normalization_input_csv = None

    if use_normalization:
        print("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿jsonï¼ˆlinear/expï¼‰ãŒã‚ã‚‹ãƒ•ã‚©ãƒ«ãƒ€ã‚’æŒ‡å®šã—ã¦ãã ã•ã„: ", end="")
        json_dir = input().strip()
        linear_json = os.path.join(json_dir, "function_parameters_linear.json")
        exp_json = os.path.join(json_dir, "function_parameters_exp.json")
        if os.path.exists(linear_json):
            normalization_params["linear"] = linear_json
        if os.path.exists(exp_json):
            normalization_params["exp"] = exp_json
        if not normalization_params:
            print("âŒ function_parameters_linear.json/function_parameters_exp.jsonãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚æ­£è¦åŒ–å‡¦ç†ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™ã€‚")
            use_normalization = False

        # â˜… ã“ã“ã§å…ƒCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ã‚‚èã
        print("æ­£è¦åŒ–å¯¾è±¡ã®6ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆä¾‹: 6point_metrics_with_column.csvï¼‰ã®ãƒ‘ã‚¹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„: ", end="")
        normalization_input_csv = input().strip()
        if not os.path.exists(normalization_input_csv):
            print(f"âŒ æŒ‡å®šã•ã‚ŒãŸCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {normalization_input_csv}")
            use_normalization = False

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«è¨­å®š
    log_level = getattr(logging, args.log_level.upper())
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler('yolo_pose_analysis.log', encoding='utf-8')
        ]
    )
    logger = logging.getLogger(__name__)

    if args.debug:
        logger.setLevel(logging.DEBUG)
        logger.info("ğŸ› ãƒ‡ãƒãƒƒã‚°ãƒ¢ãƒ¼ãƒ‰ãŒæœ‰åŠ¹åŒ–ã•ã‚Œã¾ã—ãŸ")

    try:
        # --- æ—¢å­˜CSVã¨ãƒ•ãƒ¬ãƒ¼ãƒ ç”»åƒã‹ã‚‰å¯è¦–åŒ–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ã¿å®Ÿè¡Œã™ã‚‹å ´åˆ ---
        if args.csv and args.frames_dir:
            csv_path = Path(args.csv)
            frame_dir = Path(args.frames_dir)
            output_dir = Path(args.output_dir) if args.output_dir else csv_path.parent

            logger.info(f"ğŸ“Š æ—¢å­˜CSVã‹ã‚‰6ç‚¹æŠ½å‡ºãƒ»å¯è¦–åŒ–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡¦ç†ã‚’é–‹å§‹ã—ã¾ã™")
            analyzer = ImprovedYOLOAnalyzer(config_path=args.config or "configs/default.yaml")

            # 6ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆæŠ½å‡ºï¼†ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—
            filter_result = analyzer.filter_keypoints_to_6points(str(csv_path), str(output_dir))
            sixpoint_csv = filter_result["sixpoint_csv"]

            # å¯è¦–åŒ–ç”»åƒç”Ÿæˆï¼ˆæ­ªã¿è£œæ­£ã‚’ã‹ã‘ãªã„ï¼ï¼‰
            import pandas as pd
            keypoints_df = pd.read_csv(sixpoint_csv)
            analyzer.create_6point_visualization(str(output_dir), keypoints_df, frame_dir, apply_undistort=False)

            logger.info("âœ… æ—¢å­˜CSVã‹ã‚‰ã®6ç‚¹å¯è¦–åŒ–ãƒ»ãƒ¡ãƒˆãƒªã‚¯ã‚¹å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ")
            return 0

        # --- é€šå¸¸ã®å‹•ç”»æ¨è«–å‡¦ç† ---
        if not args.video_path or not Path(args.video_path).exists():
            logger.error(f"âŒ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {args.video_path}")
            return 1

        video_path = Path(args.video_path)
        logger.info(f"ğŸ¬ å‹•ç”»ãƒ•ã‚¡ã‚¤ãƒ«: {video_path}")

        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªåã‚’ç”Ÿæˆï¼ˆoutputs/baseline/å‹•ç”»å_ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ï¼‰
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        video_name = video_path.stem
        if args.output_dir:
            output_dir = Path(args.output_dir)
        else:
            output_dir = Path("outputs/baseline") / f"{video_name}_{timestamp}"

        output_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ğŸ“ å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {output_dir}")

        # ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–
        try:
            analyzer = ImprovedYOLOAnalyzer(config_path=args.config or "configs/default.yaml")
            if args.enable_depth:
                analyzer.depth_enabled = True
                logger.info("ğŸ” æ·±åº¦æ¨å®šæ©Ÿèƒ½ã‚’æœ‰åŠ¹åŒ–")
            logger.info("âœ… ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–å®Œäº†")
        except Exception as e:
            logger.error(f"âŒ ã‚¢ãƒŠãƒ©ã‚¤ã‚¶ãƒ¼åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            import traceback
            logger.error(f"ğŸ”§ è©³ç´°ã‚¨ãƒ©ãƒ¼: {traceback.format_exc()}")
            return 1

        # 4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆè¨­å®š
        if args.use_4points:
            try:
                if hasattr(analyzer, 'config') and hasattr(analyzer.config, 'data') and isinstance(analyzer.config.data, dict):
                    analyzer.config.data.setdefault('processing', {})
                    analyzer.config.data['processing']['use_4point_keypoints'] = True
                    analyzer.config.data['processing']['keypoint_confidence_threshold'] = args.keypoint_threshold
                    analyzer.config.data['processing']['force_pose_model'] = True
                    analyzer.config.data['processing']['verify_keypoint_columns'] = True
                    analyzer.config.data['processing'].setdefault('tracking', {})
                    analyzer.config.data['processing']['tracking']['config'] = 'bytetrack.yaml'
                    analyzer.config.data['processing']['enable_shoulder_metrics'] = not args.disable_shoulder_metrics
                    analyzer.config.data['processing']['enable_head_tracking'] = not args.disable_head_tracking
                    logger.info("ğŸ”§ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’4ç‚¹ã‚­ãƒ¼ãƒã‚¤ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ã«æ›´æ–°")
                else:
                    logger.error("âŒ è¨­å®šã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒä¸æ­£ã§ã™")
            except Exception as config_error:
                logger.error(f"âŒ 4ç‚¹ãƒ¢ãƒ¼ãƒ‰è¨­å®šã‚¨ãƒ©ãƒ¼: {config_error}")
                logger.warning("âš ï¸ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã§å‡¦ç†ã‚’ç¶šè¡Œã—ã¾ã™")

        # å“è³ªè¨­å®š
        quality_configs = {
            'low': {'resolution': '640x480', 'skip_frames': 2},
            'medium': {'resolution': '1280x720', 'skip_frames': 1},
            'high': {'resolution': '1920x1080', 'skip_frames': 0},
            'ultra': {'resolution': '1920x1080', 'skip_frames': 0}
        }
        if args.quality in quality_configs:
            quality_config = quality_configs[args.quality]
            if not args.resolution:
                args.resolution = quality_config['resolution']
            if args.skip_frames == 0:
                args.skip_frames = quality_config['skip_frames']

        # è§£åƒåº¦è¨­å®š
        if args.resolution:
            try:
                width, height = map(int, args.resolution.split('x'))
                if hasattr(analyzer.config, 'data'):
                    analyzer.config.data.setdefault('processing', {})
                    analyzer.config.data['processing']['target_width'] = width
                    analyzer.config.data['processing']['target_height'] = height
                logger.info(f"ğŸ“ è§£åƒåº¦è¨­å®š: {width}x{height}")
            except ValueError:
                logger.warning(f"âš ï¸ ä¸æ­£ãªè§£åƒåº¦å½¢å¼: {args.resolution}")

        # ãã®ä»–ã®å‡¦ç†è¨­å®š
        if hasattr(analyzer.config, 'data') and analyzer.config.data:
            processing_config = analyzer.config.data.setdefault('processing', {})
            processing_config['confidence_threshold'] = args.confidence_threshold
            processing_config['iou_threshold'] = args.iou_threshold
            if args.fps:
                processing_config['target_fps'] = args.fps
            processing_config['skip_frames'] = args.skip_frames
            processing_config['save_intermediate'] = args.save_intermediate
            processing_config['enable_visualization'] = not args.disable_visualization
            model_size_map = {'n': 'nano', 's': 'small', 'm': 'medium', 'l': 'large', 'x': 'xlarge'}
            processing_config['model_size'] = model_size_map.get(args.model_size, 'xlarge')

        logger.info("ğŸš€ ========== å§¿å‹¢åˆ†æå‡¦ç†é–‹å§‹ ==========")
        import time
        start_time = time.time()

        try:
            # ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³åˆ†æå®Ÿè¡Œï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãoutput_dirã‚’æ¸¡ã™ï¼‰
            result = analyzer.run_baseline_analysis(str(video_path), output_dir=output_dir)
            if not result.get("success", False):
                error_msg = result.get("error", "ä¸æ˜ãªã‚¨ãƒ©ãƒ¼")
                logger.error(f"âŒ åˆ†æå‡¦ç†å¤±æ•—: {error_msg}")
                return 1

            processing_time = time.time() - start_time
            logger.info(f"â±ï¸ ç·å‡¦ç†æ™‚é–“: {processing_time:.2f}ç§’")

            # ã‚µãƒãƒªãƒ¼å–å¾—ï¼ˆsummaryå„ªå…ˆï¼‰
            data = result.get("data", {})
            summary = data.get("summary", {})
            total_frames = summary.get("total_frames", 0)
            total_detections = summary.get("total_detections", 0)
            unique_ids = summary.get("unique_ids", 0)
            errors = summary.get("errors", [])

            # ã‚µãƒãƒªãƒ¼ãŒãªã‘ã‚Œã°CSVã‹ã‚‰å†å–å¾—
            if (not total_frames or not total_detections) and summary.get("csv_path"):
                csv_path = summary.get("csv_path")
                if csv_path and Path(csv_path).exists():
                    import pandas as pd
                    df = pd.read_csv(csv_path)
                    total_detections = len(df)
                    total_frames = len(df['frame'].unique()) if 'frame' in df.columns else 0
                    unique_ids = len(df['person_id'].unique()) if 'person_id' in df.columns else 0

            logger.info("ğŸ“Š ========== å‡¦ç†çµæœã‚µãƒãƒªãƒ¼ ==========")
            logger.info(f"ğŸ¬ ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}")
            logger.info(f"ğŸ¯ ç·æ¤œå‡ºæ•°: {total_detections}")
            logger.info(f"ğŸ‘¥ ãƒ¦ãƒ‹ãƒ¼ã‚¯äººç‰©ID: {unique_ids}")

            if total_frames > 0:
                detection_rate = total_detections / total_frames
                logger.info(f"ğŸ“ˆ ãƒ•ãƒ¬ãƒ¼ãƒ å½“ãŸã‚Šæ¤œå‡ºæ•°: {detection_rate:.2f}")

            # ã‚¨ãƒ©ãƒ¼å ±å‘Š
            if errors:
                logger.warning(f"âš ï¸ å‡¦ç†ä¸­ã®ã‚¨ãƒ©ãƒ¼: {len(errors)}ä»¶")
                for i, error in enumerate(errors[:5], 1):
                    logger.warning(f"  {i}. {error}")
                if len(errors) > 5:
                    logger.warning(f"  ... ä»– {len(errors) - 5}ä»¶")

            # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ
            fps = total_frames / processing_time if processing_time > 0 else 0
            logger.info(f"âš¡ å‡¦ç†æ€§èƒ½: {fps:.2f} FPS")

            # ã‚µãƒãƒªãƒ¼JSONã‚‚ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ãoutput_dirã«ä¿å­˜
            summary_file = output_dir / f"{video_name}_{timestamp}_summary.json"
            import json
            with open(summary_file, "w", encoding="utf-8") as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
            logger.info(f"ğŸ“„ ã‚µãƒãƒªãƒ¼ä¿å­˜: {summary_file}")

            # --- æ­£è¦åŒ–ã®ã¿å®Ÿè¡Œã™ã‚‹å ´åˆ ---
            if use_normalization and normalization_input_csv:
                import pandas as pd
                df = pd.read_csv(normalization_input_csv)

                output_dir = os.path.dirname(normalization_input_csv)

                # ç›´ç·šè¿‘ä¼¼
                if "linear" in normalization_params:
                    from analysis.normalization_preparation import load_linear_params, normalize_value_by_linear
                    a_l, b_l, c_l = load_linear_params(os.path.dirname(normalization_params["linear"]))
                    df_linear = df[df["column_position"].notnull()].copy()
                    if "shoulder_width" in df_linear.columns and "column_position" in df_linear.columns:
                        df_linear["shoulder_width_normalized_linear"] = df_linear.apply(
                            lambda row: normalize_value_by_linear(
                                row["shoulder_width"],
                                row["column_position"],
                                a_l, b_l, c_l,
                                reference_distance=1
                            ),
                            axis=1
                        )
                    out_csv_linear = os.path.join(output_dir, "6point_metrics_normalized_linear.csv")
                    df_linear.to_csv(out_csv_linear, index=False, encoding="utf-8-sig")
                    print(f"âœ… ç›´ç·šè¿‘ä¼¼ã§æ­£è¦åŒ–æ¸ˆã¿CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {out_csv_linear}")

                # æŒ‡æ•°è¿‘ä¼¼
                if "exp" in normalization_params:
                    from analysis.normalization_preparation import load_exponential_params, normalize_value_by_decay
                    a_e, b_e, c_e = load_exponential_params(os.path.dirname(normalization_params["exp"]))
                    df_exp = df[df["column_position"].notnull()].copy()
                    if "shoulder_width" in df_exp.columns and "column_position" in df_exp.columns:
                        df_exp["shoulder_width_normalized_exp"] = df_exp.apply(
                            lambda row: normalize_value_by_decay(
                                row["shoulder_width"],
                                row["column_position"],
                                a_e, b_e, c_e,
                                reference_distance=1
                            ),
                            axis=1
                        )
                    out_csv_exp = os.path.join(output_dir, "6point_metrics_normalized_exp.csv")
                    df_exp.to_csv(out_csv_exp, index=False, encoding="utf-8-sig")
                    print(f"âœ… æŒ‡æ•°è¿‘ä¼¼ã§æ­£è¦åŒ–æ¸ˆã¿CSVã‚’ä¿å­˜ã—ã¾ã—ãŸ: {out_csv_exp}")

                return 0

        except Exception as e:
            logger.error(f"âŒ åˆ†æå‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
            import traceback
            logger.error(f"ğŸ”§ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
            return 1

    except KeyboardInterrupt:
        logger.warning("â¸ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹å‡¦ç†ä¸­æ–­")
        return 130

    except Exception as e:
        logger.error(f"âŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        logger.error(f"ğŸ”§ ãƒˆãƒ¬ãƒ¼ã‚¹ãƒãƒƒã‚¯:\n{traceback.format_exc()}")
        return 1



if __name__ == "__main__":
    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±å‡ºåŠ›
    print(f"ğŸ Python: {sys.version}")
    print(f"ğŸ’» Platform: {platform.platform()}")
    print(f"ğŸ§  CPU Count: {os.cpu_count()}")
    
    # GPUæƒ…å ±ç¢ºèª
    try:
        import torch
        if torch.cuda.is_available():
            print(f"ğŸš€ CUDA: {torch.version.cuda}")
            print(f"ğŸ® GPU: {torch.cuda.get_device_name(0)}")
        else:
            print("ğŸ’» GPU: CUDAåˆ©ç”¨ä¸å¯ï¼ˆCPUå‡¦ç†ï¼‰")
    except ImportError:
        print("âš ï¸ PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«")
    
    print()
    
    # ãƒ¡ã‚¤ãƒ³å‡¦ç†å®Ÿè¡Œ
    sys.exit(main())