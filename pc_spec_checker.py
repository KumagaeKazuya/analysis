"""
PC ã‚¹ãƒšãƒƒã‚¯è¨ºæ–­ & YOLO11 ãƒ¢ãƒ‡ãƒ«æ¨å¥¨ãƒ„ãƒ¼ãƒ«
ã“ã®PCã§ä½¿ç”¨å¯èƒ½ãªYOLOãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºã‚’åˆ¤å®šã—ã¾ã™
"""

import sys
import platform
import subprocess

def check_system_specs():
    """ã‚·ã‚¹ãƒ†ãƒ ã‚¹ãƒšãƒƒã‚¯ã®è©³ç´°ç¢ºèª"""
    print("=" * 60)
    print("ğŸ–¥ï¸  PC ã‚¹ãƒšãƒƒã‚¯è¨ºæ–­")
    print("=" * 60)

    # åŸºæœ¬æƒ…å ±
    print("\nã€åŸºæœ¬æƒ…å ±ã€‘")
    print(f"OS: {platform.system()} {platform.release()}")
    print(f"ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£: {platform.machine()}")
    print(f"Python ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {sys.version.split()[0]}")

    # CPUæƒ…å ±
    print("\nã€CPUæƒ…å ±ã€‘")
    try:
        import psutil
        cpu_count_physical = psutil.cpu_count(logical=False)
        cpu_count_logical = psutil.cpu_count(logical=True)
        cpu_freq = psutil.cpu_freq()

        print(f"ç‰©ç†ã‚³ã‚¢æ•°: {cpu_count_physical}")
        print(f"è«–ç†ã‚³ã‚¢æ•°: {cpu_count_logical}")
        if cpu_freq:
            print(f"ã‚¯ãƒ­ãƒƒã‚¯: {cpu_freq.current:.0f} MHz (æœ€å¤§: {cpu_freq.max:.0f} MHz)")

        # CPUä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        print(f"ç¾åœ¨ã®CPUä½¿ç”¨ç‡: {cpu_percent}%")
    except ImportError:
        print("âš ï¸ psutilæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - è©³ç´°æƒ…å ±å–å¾—ä¸å¯")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install psutil")

    # ãƒ¡ãƒ¢ãƒªæƒ…å ±
    print("\nã€ãƒ¡ãƒ¢ãƒªæƒ…å ±ã€‘")
    try:
        import psutil
        memory = psutil.virtual_memory()
        
        print(f"ç·ãƒ¡ãƒ¢ãƒª: {memory.total / (1024**3):.1f} GB")
        print(f"ä½¿ç”¨ä¸­: {memory.used / (1024**3):.1f} GB ({memory.percent}%)")
        print(f"åˆ©ç”¨å¯èƒ½: {memory.available / (1024**3):.1f} GB")
        
        # ã‚¹ãƒ¯ãƒƒãƒ—ãƒ¡ãƒ¢ãƒª
        swap = psutil.swap_memory()
        print(f"ã‚¹ãƒ¯ãƒƒãƒ—: {swap.total / (1024**3):.1f} GB (ä½¿ç”¨: {swap.percent}%)")
    except ImportError:
        print("âš ï¸ ãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ä¸å¯")

    # GPUæƒ…å ±
    print("\nã€GPUæƒ…å ±ã€‘")
    gpu_available = False
    gpu_memory_gb = 0
    gpu_name = "N/A"

    try:
        import torch

        if torch.cuda.is_available():
            gpu_available = True
            gpu_count = torch.cuda.device_count()
            print(f"âœ… CUDA åˆ©ç”¨å¯èƒ½")
            print(f"GPUæ•°: {gpu_count}")

            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                props = torch.cuda.get_device_properties(i)
                gpu_memory_gb = props.total_memory / (1024**3)

                print(f"\nGPU {i}: {gpu_name}")
                print(f"  ãƒ¡ãƒ¢ãƒª: {gpu_memory_gb:.1f} GB")
                print(f"  ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆèƒ½åŠ›: {props.major}.{props.minor}")
                print(f"  ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚µæ•°: {props.multi_processor_count}")

                # ç¾åœ¨ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨çŠ¶æ³
                if torch.cuda.is_available():
                    try:
                        allocated = torch.cuda.memory_allocated(i) / (1024**3)
                        reserved = torch.cuda.memory_reserved(i) / (1024**3)
                        print(f"  ä½¿ç”¨ä¸­ãƒ¡ãƒ¢ãƒª: {allocated:.2f} GB")
                        print(f"  äºˆç´„æ¸ˆã¿ãƒ¡ãƒ¢ãƒª: {reserved:.2f} GB")
                    except:
                        pass
        else:
            print("âŒ CUDA åˆ©ç”¨ä¸å¯")
            print("CPUãƒ¢ãƒ¼ãƒ‰ã§å‹•ä½œã—ã¾ã™")

            # CUDAãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèª
            cuda_version = torch.version.cuda
            if cuda_version:
                print(f"PyTorch CUDA ãƒãƒ¼ã‚¸ãƒ§ãƒ³: {cuda_version}")
                print("âš ï¸ GPU ãƒ‰ãƒ©ã‚¤ãƒãƒ¼ã®ç¢ºèªãŒå¿…è¦ã§ã™")

    except ImportError:
        print("âš ï¸ PyTorchæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - GPUæƒ…å ±å–å¾—ä¸å¯")
        print("ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«: pip install torch")

    # ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±
    print("\nã€ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±ã€‘")
    try:
        import psutil
        disk = psutil.disk_usage('.')

        print(f"ç·å®¹é‡: {disk.total / (1024**3):.1f} GB")
        print(f"ä½¿ç”¨ä¸­: {disk.used / (1024**3):.1f} GB ({disk.percent}%)")
        print(f"ç©ºãå®¹é‡: {disk.free / (1024**3):.1f} GB")
    except:
        print("âš ï¸ ãƒ‡ã‚£ã‚¹ã‚¯æƒ…å ±å–å¾—ä¸å¯")

    return {
        "gpu_available": gpu_available,
        "gpu_memory_gb": gpu_memory_gb,
        "gpu_name": gpu_name,
        "ram_gb": memory.total / (1024**3) if 'memory' in locals() else 0,
        "ram_available_gb": memory.available / (1024**3) if 'memory' in locals() else 0,
        "cpu_cores": cpu_count_physical if 'cpu_count_physical' in locals() else 0
    }

def recommend_yolo_model(specs):
    """ã‚¹ãƒšãƒƒã‚¯ã«åŸºã¥ã„ã¦YOLOãƒ¢ãƒ‡ãƒ«ã‚’æ¨å¥¨"""
    print("\n" + "=" * 60)
    print("ğŸ¯ YOLO11 ãƒ¢ãƒ‡ãƒ«æ¨å¥¨")
    print("=" * 60)

    gpu_available = specs["gpu_available"]
    gpu_memory_gb = specs["gpu_memory_gb"]
    ram_available_gb = specs["ram_available_gb"]

    print(f"\nåˆ¤å®šæ¡ä»¶:")
    print(f"  GPU: {'âœ… ã‚ã‚Š' if gpu_available else 'âŒ ãªã—'}")
    if gpu_available:
        print(f"  GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory_gb:.1f} GB")
    print(f"  åˆ©ç”¨å¯èƒ½RAM: {ram_available_gb:.1f} GB")

    # ãƒ¢ãƒ‡ãƒ«æƒ…å ±
    models = {
        "yolo11n": {"size_mb": 2.6, "gpu_ram": 1, "system_ram": 2, "speed": "æœ€é€Ÿ", "accuracy": "ä½"},
        "yolo11s": {"size_mb": 9.4, "gpu_ram": 2, "system_ram": 3, "speed": "é€Ÿã„", "accuracy": "ä¸­ä½"},
        "yolo11m": {"size_mb": 20.1, "gpu_ram": 4, "system_ram": 6, "speed": "ä¸­", "accuracy": "ä¸­é«˜"},
        "yolo11l": {"size_mb": 25.3, "gpu_ram": 6, "system_ram": 8, "speed": "é…ã„", "accuracy": "é«˜"},
        "yolo11x": {"size_mb": 56.9, "gpu_ram": 8, "system_ram": 12, "speed": "æœ€é…", "accuracy": "æœ€é«˜"},
    }

    print("\nã€ãƒ¢ãƒ‡ãƒ«æ¨å¥¨çµæœã€‘\n")

    recommended = []
    possible = []
    not_recommended = []

    for model_name, model_info in models.items():
        status = ""
        reason = []

        if gpu_available:
            # GPUä½¿ç”¨æ™‚ã®åˆ¤å®š
            if gpu_memory_gb >= model_info["gpu_ram"] * 1.5:  # 1.5å€ã®ä½™è£•
                status = "âœ… æ¨å¥¨"
                recommended.append(model_name)
            elif gpu_memory_gb >= model_info["gpu_ram"]:
                status = "âš ï¸ å¯èƒ½ï¼ˆã‚®ãƒªã‚®ãƒªï¼‰"
                possible.append(model_name)
                reason.append("GPU ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã‚")
            else:
                status = "âŒ éæ¨å¥¨"
                not_recommended.append(model_name)
                reason.append(f"GPU ãƒ¡ãƒ¢ãƒªä¸è¶³ (å¿…è¦: {model_info['gpu_ram']}GB)")
        else:
            # CPUä½¿ç”¨æ™‚ã®åˆ¤å®š
            if ram_available_gb >= model_info["system_ram"] * 1.5:
                if model_name in ["yolo11n", "yolo11s"]:
                    status = "âœ… æ¨å¥¨ï¼ˆCPUï¼‰"
                    recommended.append(model_name)
                elif model_name == "yolo11m":
                    status = "âš ï¸ å¯èƒ½ï¼ˆé…ã„ï¼‰"
                    possible.append(model_name)
                    reason.append("CPUã§å‡¦ç†ãŒé…ã„")
                else:
                    status = "âŒ éæ¨å¥¨"
                    not_recommended.append(model_name)
                    reason.append("CPUã§ã¯éå¸¸ã«é…ã„")
            elif ram_available_gb >= model_info["system_ram"]:
                if model_name in ["yolo11n", "yolo11s"]:
                    status = "âš ï¸ å¯èƒ½ï¼ˆã‚®ãƒªã‚®ãƒªï¼‰"
                    possible.append(model_name)
                    reason.append("ãƒ¡ãƒ¢ãƒªãŒå°‘ãªã‚")
                else:
                    status = "âŒ éæ¨å¥¨"
                    not_recommended.append(model_name)
                    reason.append("ãƒ¡ãƒ¢ãƒªä¸è¶³")
            else:
                status = "âŒ éæ¨å¥¨"
                not_recommended.append(model_name)
                reason.append("ãƒ¡ãƒ¢ãƒªä¸è¶³")

        # çµæœè¡¨ç¤º
        print(f"{status} {model_name}.pt")
        print(f"    ã‚µã‚¤ã‚º: {model_info['size_mb']} MB")
        print(f"    å¿…è¦GPU RAM: {model_info['gpu_ram']} GB / ã‚·ã‚¹ãƒ†ãƒ RAM: {model_info['system_ram']} GB")
        print(f"    é€Ÿåº¦: {model_info['speed']} / ç²¾åº¦: {model_info['accuracy']}")
        if reason:
            print(f"    ç†ç”±: {', '.join(reason)}")
        print()

    # ã‚µãƒãƒªãƒ¼
    print("=" * 60)
    print("ğŸ“‹ æ¨å¥¨ã‚µãƒãƒªãƒ¼")
    print("=" * 60)

    if recommended:
        print(f"\nâœ… æ¨å¥¨ãƒ¢ãƒ‡ãƒ«: {', '.join(recommended)}")
        print(f"   â†’ configs/default.yaml ã§è¨­å®š: {recommended[-1]}.pt")

    if possible:
        print(f"\nâš ï¸ ä½¿ç”¨å¯èƒ½ï¼ˆåˆ¶é™ã‚ã‚Šï¼‰: {', '.join(possible)}")

    if not_recommended:
        print(f"\nâŒ éæ¨å¥¨: {', '.join(not_recommended)}")

    # å…·ä½“çš„ãªæ¨å¥¨
    print("\n" + "=" * 60)
    print("ğŸ’¡ å…·ä½“çš„ãªæ¨å¥¨è¨­å®š")
    print("=" * 60)

    if gpu_available:
        if gpu_memory_gb >= 8:
            best_model = "yolo11x"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆæœ€é«˜ç²¾åº¦ï¼‰")
        elif gpu_memory_gb >= 6:
            best_model = "yolo11l"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆé«˜ç²¾åº¦ï¼‰")
        elif gpu_memory_gb >= 4:
            best_model = "yolo11m"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆãƒãƒ©ãƒ³ã‚¹å‹ï¼‰")
        else:
            best_model = "yolo11s"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆè»½é‡ãƒ»é«˜é€Ÿï¼‰")
    else:
        if ram_available_gb >= 6:
            best_model = "yolo11s"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆCPUä½¿ç”¨ãƒ»ãƒãƒ©ãƒ³ã‚¹ï¼‰")
        else:
            best_model = "yolo11n"
            print(f"\nğŸ¯ æœ€é©: {best_model}.ptï¼ˆCPUä½¿ç”¨ãƒ»æœ€è»½é‡ï¼‰")

    print(f"\nconfigs/default.yaml ã«ä»¥ä¸‹ã‚’è¨­å®š:")
    print("```yaml")
    print("models:")
    print(f'  detection: "{best_model}.pt"')
    print(f'  pose: "{best_model}-pose.pt"')
    print("```")

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ
    print("\n" + "=" * 60)
    print("âš™ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã®ãƒ’ãƒ³ãƒˆ")
    print("=" * 60)

    if gpu_available:
        print("\nâœ… GPUåˆ©ç”¨å¯èƒ½ - ä»¥ä¸‹ã®æœ€é©åŒ–ã‚’æ¨å¥¨:")
        print("  1. åŠç²¾åº¦æ¼”ç®—ï¼ˆFP16ï¼‰ã‚’æœ‰åŠ¹åŒ–")
        print("     processing.gpu_settings.use_half_precision: true")
        print("  2. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´")
        print(f"     processing.gpu_settings.batch_size: {max(4, int(gpu_memory_gb / 2))}")
    else:
        print("\nâš ï¸ CPUä½¿ç”¨ - ä»¥ä¸‹ã§é«˜é€ŸåŒ–:")
        print("  1. ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ã‚’åˆ¶é™")
        print("     processing.frame_sampling.max_frames: 500")
        print("  2. ãƒ•ãƒ¬ãƒ¼ãƒ é–“éš”ã‚’åºƒã’ã‚‹")
        print("     processing.frame_sampling.interval_sec: 5")
        print("  3. ä¿¡é ¼åº¦é–¾å€¤ã‚’ä¸Šã’ã‚‹")
        print("     processing.detection.confidence_threshold: 0.5")

    if ram_available_gb < 8:
        print("\nâš ï¸ ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å¯èƒ½æ€§ - ä»¥ä¸‹ã‚’æ¨å¥¨:")
        print("  1. å¯è¦–åŒ–ã‚’ç„¡åŠ¹åŒ–")
        print("     visualization.save_annotated_frames: false")
        print("  2. ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‡¦ç†ã‚’æœ‰åŠ¹åŒ–")
        print("     processing.streaming_output: true")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        specs = check_system_specs()
        recommend_yolo_model(specs)

        print("\n" + "=" * 60)
        print("âœ… è¨ºæ–­å®Œäº†")
        print("=" * 60)
        print("\næ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
        print("1. æ¨å¥¨ãƒ¢ãƒ‡ãƒ«ã‚’ configs/default.yaml ã«è¨­å®š")
        print("2. python setup.py ã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰")
        print("3. python improved_main.py --mode baseline ã§å®Ÿè¡Œ")

    except Exception as e:
        print(f"\nâŒ ã‚¨ãƒ©ãƒ¼: {e}")
        print("\nå¿…è¦ãªãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«:")
        print("pip install psutil torch")

if __name__ == "__main__":
    main()