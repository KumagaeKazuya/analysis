"""
ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å…ƒã® yolopose_analyzer.py ã‹ã‚‰æŠ½å‡º
"""

import os
import torch
import psutil
import logging
import traceback
from typing import Dict, Any
from ultralytics import YOLO

logger = logging.getLogger(__name__)


class ModelInitializationError(Exception):
    """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼"""
    pass


class ResourceExhaustionError(Exception):
    """ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã‚¨ãƒ©ãƒ¼"""
    pass


def check_system_resources() -> Dict[str, Any]:
    """
    ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª
    
    Returns:
        ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã®è¾æ›¸
    """
    try:
        memory = psutil.virtual_memory()

        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç¢ºèªï¼ˆã‚¯ãƒ­ã‚¹ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ å¯¾å¿œï¼‰
        try:
            if os.name == 'nt':  # Windows
                disk = psutil.disk_usage('C:')
            else:  # Unix/Linux/Mac
                disk = psutil.disk_usage('/')
        except:
            disk = None

        # GPUæ¤œå‡ºï¼ˆCUDA + MPSå¯¾å¿œï¼‰
        gpu_available = False
        gpu_type = "none"
        gpu_count = 0
        gpu_name = "N/A"

        # CUDA (NVIDIA GPU) ãƒã‚§ãƒƒã‚¯
        if torch.cuda.is_available():
            gpu_available = True
            gpu_type = "cuda"
            gpu_count = torch.cuda.device_count()
            if gpu_count > 0:
                gpu_name = torch.cuda.get_device_name(0)
        # MPS (Apple Silicon GPU) ãƒã‚§ãƒƒã‚¯
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            gpu_available = True
            gpu_type = "mps"
            gpu_count = 1
            gpu_name = "Apple Silicon GPU (MPS)"

        result = {
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_percent": memory.percent,
            "cpu_count": psutil.cpu_count(),
            "gpu_available": gpu_available,
            "gpu_type": gpu_type,
            "gpu_count": gpu_count,
            "gpu_name": gpu_name
        }

        if disk:
            result["disk_free_gb"] = disk.free / (1024**3)

        return result
    except Exception as e:
        logger.warning(f"ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return {"error": str(e)}


def safe_model_initialization(model_path: str, config: Dict[str, Any]) -> YOLO:
    """
    å®‰å…¨ãªãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
    
    Args:
        model_path: YOLOãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        config: è¨­å®šè¾æ›¸ï¼ˆdeviceç­‰ã‚’å«ã‚€ï¼‰
        
    Returns:
        åˆæœŸåŒ–æ¸ˆã¿YOLOãƒ¢ãƒ‡ãƒ«
        
    Raises:
        ModelInitializationError: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸå ´åˆ
    """
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
    resources = check_system_resources()
    if "error" not in resources:
        logger.info(f"ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: ãƒ¡ãƒ¢ãƒª {resources['memory_available_gb']:.1f}GB åˆ©ç”¨å¯èƒ½")

        if resources["memory_available_gb"] < 2.0:
            logger.warning("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒ2GBæœªæº€ã§ã™ã€‚å‡¦ç†ãŒé…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        # GPUæƒ…å ±ã®å‡ºåŠ›
        if resources["gpu_available"]:
            gpu_type = resources.get("gpu_type", "unknown")
            gpu_name = resources.get("gpu_name", "N/A")
            gpu_count = resources.get("gpu_count", 0)

            if gpu_type == "mps":
                logger.info(f"ğŸ Apple Silicon GPU (MPS) åˆ©ç”¨å¯èƒ½")
            elif gpu_type == "cuda":
                logger.info(f"ğŸš€ NVIDIA GPU (CUDA) åˆ©ç”¨å¯èƒ½: {gpu_count}å€‹ã®ãƒ‡ãƒã‚¤ã‚¹")
                if gpu_name != "N/A":
                    logger.info(f"   GPUå: {gpu_name}")
            else:
                logger.info(f"GPUåˆ©ç”¨å¯èƒ½: {gpu_type.upper()}")
        else:
            logger.info("ğŸ’» GPUåˆ©ç”¨ä¸å¯ã€‚CPUã§å‡¦ç†ã—ã¾ã™ã€‚")

    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
    from .validation import validate_model_file
    validation = validate_model_file(model_path)
    
    if not validation["valid"]:
        error_msg = "\n".join(validation["errors"])
        suggestions = "\n".join(validation["suggestions"])
        raise ModelInitializationError(f"{error_msg}\n\næ¨å¥¨å¯¾å¿œ:\n{suggestions}")

    if validation["warnings"]:
        for warning in validation["warnings"]:
            logger.info(warning)

    # PyTorch 2.6ä»¥é™ã®weights_onlyå¯¾ç­–
    try:
        import torch.serialization
        torch.serialization.add_safe_globals([
            'ultralytics.nn.tasks.PoseModel',
            'ultralytics.nn.tasks.DetectionModel'
        ])
    except AttributeError:
        pass

    try:
        # PyTorch 2.8å¯¾ç­–: weights_onlyã‚’å¼·åˆ¶çš„ã«Falseã«
        _original_torch_load = torch.load
        torch.load = lambda *args, **kwargs: _original_torch_load(
            *args, **{**kwargs, 'weights_only': False}
        )

        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–
        model = YOLO(model_path)

        # torch.loadã‚’å…ƒã«æˆ»ã™
        torch.load = _original_torch_load

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = config.get("device", "auto")
        
        if device == "auto":
            # è‡ªå‹•æ¤œå‡º
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        try:
            model.to(device)
        except Exception as e:
            logger.warning(f"ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {e}. CPUã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            device = "cpu"
            model.to(device)

        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±ã®å‡ºåŠ›
        if device == "mps":
            logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {model_path} on ğŸ {device.upper()}")
        elif device == "cuda":
            logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {model_path} on ğŸš€ {device.upper()}")
        else:
            logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {model_path} on ğŸ’» {device.upper()}")

        # GPUä½¿ç”¨æ™‚ã®è¿½åŠ è¨­å®š
        if device == "cuda" and torch.cuda.is_available():
            try:
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
                logger.info(f"GPU ãƒ¡ãƒ¢ãƒª: {gpu_memory:.1f}GB")

                # åŠç²¾åº¦æ¼”ç®—è¨­å®šï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
                if config.get("use_half_precision", False):
                    model.half()
                    logger.info("åŠç²¾åº¦æ¼”ç®—ã‚’æœ‰åŠ¹åŒ–")
            except Exception as e:
                logger.warning(f"GPUè¨­å®šè­¦å‘Š: {e}")

        return model

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        logger.error(f"è©³ç´°: {traceback.format_exc()}")
        raise ModelInitializationError(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")