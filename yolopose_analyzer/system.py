"""
ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
"""

import os
import torch
import psutil
import logging
from typing import Dict, Any
from ultralytics import YOLO

logger = logging.getLogger(__name__)


def check_system_resources() -> Dict[str, Any]:
    """ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª"""
    try:
        memory = psutil.virtual_memory()

        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç¢ºèª
        try:
            if os.name == 'nt':  # Windows
                disk = psutil.disk_usage('C:')
            else:  # Unix/Linux/Mac
                disk = psutil.disk_usage('/')
        except:
            disk = None

        # GPUæ¤œå‡ºï¼ˆCUDA + MPSå¯¾å¿œï¼‰
        gpu_available = False
        gpu_type = "none"
        gpu_count = 0
        gpu_name = "N/A"

        if torch.cuda.is_available():
            gpu_available = True
            gpu_type = "cuda"
            gpu_count = torch.cuda.device_count()
            if gpu_count > 0:
                gpu_name = torch.cuda.get_device_name(0)
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            gpu_available = True
            gpu_type = "mps"
            gpu_count = 1
            gpu_name = "Apple Silicon GPU (MPS)"

        result = {
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_percent": memory.percent,
            "cpu_count": psutil.cpu_count(),
            "gpu_available": gpu_available,
            "gpu_type": gpu_type,
            "gpu_count": gpu_count,
            "gpu_name": gpu_name
        }

        if disk:
            result["disk_free_gb"] = disk.free / (1024**3)

        return result
    except Exception as e:
        logger.warning(f"ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        return {"error": str(e)}


def safe_model_initialization(model_path: str, config: Dict[str, Any]) -> YOLO:
    """å®‰å…¨ãªãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–"""
    from .validation import validate_model_file
    
    # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
    resources = check_system_resources()
    if "error" not in resources:
        logger.info(f"ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹: ãƒ¡ãƒ¢ãƒª {resources['memory_available_gb']:.1f}GB åˆ©ç”¨å¯èƒ½")

        if resources["memory_available_gb"] < 2.0:
            logger.warning("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒ2GBæœªæº€ã§ã™ã€‚å‡¦ç†ãŒé…ããªã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

        if resources["gpu_available"]:
            gpu_type = resources.get("gpu_type", "unknown")
            gpu_name = resources.get("gpu_name", "N/A")
            
            if gpu_type == "mps":
                logger.info(f"ğŸ Apple Silicon GPU (MPS) åˆ©ç”¨å¯èƒ½")
            elif gpu_type == "cuda":
                logger.info(f"ğŸš€ NVIDIA GPU (CUDA) åˆ©ç”¨å¯èƒ½: {resources['gpu_count']}å€‹")
        else:
            logger.info("ğŸ’» GPUåˆ©ç”¨ä¸å¯ã€‚CPUã§å‡¦ç†ã—ã¾ã™ã€‚")

    # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
    validation = validate_model_file(model_path)
    if not validation["valid"]:
        error_msg = "\n".join(validation["errors"])
        suggestions = "\n".join(validation["suggestions"])
        raise ModelInitializationError(f"{error_msg}\n\næ¨å¥¨å¯¾å¿œ:\n{suggestions}")

    try:
        # PyTorch 2.8å¯¾ç­–
        _original_torch_load = torch.load
        torch.load = lambda *args, **kwargs: _original_torch_load(*args, **{**kwargs, 'weights_only': False})

        model = YOLO(model_path)
        torch.load = _original_torch_load

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = config.get("device", "auto")
        
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        try:
            model.to(device)
        except Exception as e:
            logger.warning(f"ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {e}. CPUã‚’ä½¿ç”¨ã—ã¾ã™ã€‚")
            device = "cpu"
            model.to(device)

        logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {model_path} on {device.upper()}")

        return model

    except Exception as e:
        logger.error(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
        raise ModelInitializationError(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")


class ModelInitializationError(Exception):
    """ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼"""
    pass


class ResourceExhaustionError(Exception):
    """ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã‚¨ãƒ©ãƒ¼"""
    pass