"""
ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç®¡ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
å…ƒã® yolopose_analyzer.py ã‹ã‚‰æŠ½å‡º
"""

import os
import torch
import psutil
import logging
import traceback
from typing import Dict, Any
from ultralytics import YOLO

logger = logging.getLogger(__name__)

# ğŸ”§ çµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒ©ãƒ¼ã‹ã‚‰ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from utils.error_handler import (
    ModelInitializationError,
    ResourceExhaustionError,
    ResponseBuilder,
    handle_errors,
    ErrorContext,
    ErrorCategory
)


@handle_errors(logger=logger, error_category=ErrorCategory.SYSTEM)
def check_system_resources() -> Dict[str, Any]:
    """
    ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèª

    Returns:
        ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹æƒ…å ±ã®è¾æ›¸ï¼ˆResponseBuilderå½¢å¼ï¼‰
    """
    with ErrorContext("ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª", logger=logger) as ctx:
        memory = psutil.virtual_memory()

        # ãƒ‡ã‚£ã‚¹ã‚¯ä½¿ç”¨é‡ç¢ºèª
        try:
            disk = psutil.disk_usage('C:' if os.name == 'nt' else '/')
        except:
            disk = None

        # GPUæ¤œå‡º
        gpu_available = False
        gpu_type = "none"
        gpu_count = 0
        gpu_name = "N/A"

        if torch.cuda.is_available():
            gpu_available = True
            gpu_type = "cuda"
            gpu_count = torch.cuda.device_count()
            if gpu_count > 0:
                gpu_name = torch.cuda.get_device_name(0)
        elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
            gpu_available = True
            gpu_type = "mps"
            gpu_count = 1
            gpu_name = "Apple Silicon GPU (MPS)"

        data = {
            "memory_total_gb": memory.total / (1024**3),
            "memory_available_gb": memory.available / (1024**3),
            "memory_percent": memory.percent,
            "cpu_count": psutil.cpu_count(),
            "gpu_available": gpu_available,
            "gpu_type": gpu_type,
            "gpu_count": gpu_count,
            "gpu_name": gpu_name
        }

        if disk:
            data["disk_free_gb"] = disk.free / (1024**3)

        ctx.add_info("gpu_type", gpu_type)
        ctx.add_info("memory_available", data["memory_available_gb"])

        return ResponseBuilder.success(data=data, message="ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèªå®Œäº†")


@handle_errors(logger=logger, error_category=ErrorCategory.INITIALIZATION)
def safe_model_initialization(model_path: str, config: Dict[str, Any]) -> YOLO:
    """
    å®‰å…¨ãªãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ï¼ˆçµ±ä¸€ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å¯¾å¿œç‰ˆï¼‰
    
    Args:
        model_path: YOLOãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        config: è¨­å®šè¾æ›¸
        
    Returns:
        åˆæœŸåŒ–æ¸ˆã¿YOLOãƒ¢ãƒ‡ãƒ«
        
    Raises:
        ModelInitializationError: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ã«å¤±æ•—ã—ãŸå ´åˆ
        ResourceExhaustionError: ãƒªã‚½ãƒ¼ã‚¹ä¸è¶³ã®å ´åˆ
    """
    with ErrorContext("ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–", logger=logger) as ctx:
        # ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ç¢ºèª
        resources_response = check_system_resources()
        if not resources_response.get("success", False):
            raise ResourceExhaustionError(
                "ã‚·ã‚¹ãƒ†ãƒ ãƒªã‚½ãƒ¼ã‚¹ã®ç¢ºèªã«å¤±æ•—ã—ã¾ã—ãŸ",
                details=resources_response.get("error", {})
            )
        
        resources = resources_response["data"]
        ctx.add_info("available_memory_gb", resources["memory_available_gb"])
        
        if resources["memory_available_gb"] < 2.0:
            logger.warning("åˆ©ç”¨å¯èƒ½ãƒ¡ãƒ¢ãƒªãŒ2GBæœªæº€ã§ã™")
            
        # GPUæƒ…å ±ãƒ­ã‚°å‡ºåŠ›
        if resources["gpu_available"]:
            gpu_type = resources.get("gpu_type", "unknown")
            gpu_name = resources.get("gpu_name", "N/A")
            
            if gpu_type == "mps":
                logger.info("ğŸ Apple Silicon GPU (MPS) åˆ©ç”¨å¯èƒ½")
            elif gpu_type == "cuda":
                logger.info(f"ğŸš€ NVIDIA GPU (CUDA) åˆ©ç”¨å¯èƒ½: {gpu_name}")
            else:
                logger.info(f"GPUåˆ©ç”¨å¯èƒ½: {gpu_type.upper()}")
        else:
            logger.info("ğŸ’» GPUåˆ©ç”¨ä¸å¯ã€‚CPUã§å‡¦ç†ã—ã¾ã™")
            
        # ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼
        from yolopose_analyzer.validation import validate_model_file
        validation = validate_model_file(model_path)
        
        if not validation.get("success", False):
            error_details = validation.get("error", {})
            raise ModelInitializationError(
                f"ãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«æ¤œè¨¼å¤±æ•—: {error_details.get('message', 'unknown')}",
                details=error_details
            )

        # PyTorch weights_onlyå¯¾ç­–
        try:
            import torch.serialization
            torch.serialization.add_safe_globals([
                'ultralytics.nn.tasks.PoseModel',
                'ultralytics.nn.tasks.DetectionModel'
            ])
        except AttributeError:
            pass

        # ãƒ¢ãƒ‡ãƒ«èª­ã¿è¾¼ã¿
        _original_torch_load = torch.load
        torch.load = lambda *args, **kwargs: _original_torch_load(
            *args, **{**kwargs, 'weights_only': False}
        )

        try:
            model = YOLO(model_path)
        finally:
            torch.load = _original_torch_load

        # ãƒ‡ãƒã‚¤ã‚¹è¨­å®š
        device = config.get("device", "auto")
        if device == "auto":
            if torch.cuda.is_available():
                device = "cuda"
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                device = "mps"
            else:
                device = "cpu"

        try:
            model.to(device)
        except Exception as e:
            logger.warning(f"ãƒ‡ãƒã‚¤ã‚¹è¨­å®šã‚¨ãƒ©ãƒ¼: {e}. CPUã‚’ä½¿ç”¨ã—ã¾ã™")
            device = "cpu"
            model.to(device)

        ctx.add_info("model_path", model_path)
        ctx.add_info("device", device)
        
        # ãƒ‡ãƒã‚¤ã‚¹åˆ¥ã®ãƒ­ã‚°å‡ºåŠ›
        device_emoji = {"mps": "ğŸ", "cuda": "ğŸš€", "cpu": "ğŸ’»"}
        logger.info(f"ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–å®Œäº†: {model_path} on {device_emoji.get(device, '')} {device.upper()}")
        
        return model