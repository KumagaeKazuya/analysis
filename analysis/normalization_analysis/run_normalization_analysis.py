#!/usr/bin/env python3
"""
ğŸ”§ è·é›¢æ­£è¦åŒ–é–¢æ•°åˆ†æ å®Ÿè¡Œã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ã‚«ãƒ¡ãƒ©è·é›¢ã«ã‚ˆã‚‹è‚©å¹…å¤‰åŒ–ã‚’åˆ†æã—ã€æ­£è¦åŒ–é–¢æ•°ã‚’ç”Ÿæˆã™ã‚‹ãƒ„ãƒ¼ãƒ«

ä½¿ç”¨ä¾‹:
    # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    python run_normalization_analysis.py check "outputs/baseline/11æœˆ12æ—¥ 1/4point_metrics.csv"
    
    # 1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
    python run_normalization_analysis.py analyze_one "outputs/baseline/11æœˆ12æ—¥ 1/4point_metrics.csv" "11æœˆ12æ—¥ 1.mp4_frame0.jpg"
    
    # å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
    python run_normalization_analysis.py analyze_all "outputs/baseline/11æœˆ12æ—¥ 1/4point_metrics.csv"
    
    # ã‚µãƒ³ãƒ—ãƒ«åˆ†æ
    python run_normalization_analysis.py
"""

import sys
import argparse
from pathlib import Path
from typing import Dict, List, Optional
import traceback
import os
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime
from scipy.optimize import curve_fit
import json

# --- æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆMacç”¨ï¼‰ ---
plt.rcParams['font.family'] = 'AppleGothic'
# Windowsã®å ´åˆã¯ 'MS Gothic' ã‚„ 'Meiryo' ãªã©ã«å¤‰æ›´ã—ã¦ãã ã•ã„

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(str(Path(__file__).parent.parent))

try:
    from normalization_analysis.distance_normalization import DistanceNormalizationAnalyzer, check_available_data
except ImportError as e:
    print(f"âŒ ã‚¤ãƒ³ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
    print("ğŸ“ ç¾åœ¨ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ç¢ºèªã—ã¦ãã ã•ã„")
    sys.exit(1)

def print_header():
    """ğŸ¨ ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º"""
    print("ğŸ”§" + "=" * 60 + "ğŸ”§")
    print("ğŸ“Š      è·é›¢æ­£è¦åŒ–é–¢æ•°åˆ†æãƒ„ãƒ¼ãƒ« v1.0")
    print("ğŸ¯      ã‚«ãƒ¡ãƒ©è·é›¢ã«ã‚ˆã‚‹è‚©å¹…å¤‰åŒ–ã®å®šé‡åŒ–")
    print("ğŸ”§" + "=" * 60 + "ğŸ”§")

def get_file_emoji(file_type: str) -> str:
    """ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«ã‚¿ã‚¤ãƒ—åˆ¥çµµæ–‡å­—"""
    emoji_map = {
        'visualization': 'ğŸ“Š',
        'report': 'ğŸ“', 
        'function_data': 'ğŸ“‹',
        'normalization_code': 'ğŸ”§'
    }
    return emoji_map.get(file_type, 'ğŸ“„')

def extract_column_assignments_from_csv(csv_path):
    """
    CSVã®column_positionåˆ—ã‹ã‚‰è‡ªå‹•ã§ {åˆ—ç•ªå·: [person_id, ...]} ã‚’æŠ½å‡º
    -1ã‚„Noneã¯é™¤å¤–
    """
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    col_dict = {}
    for _, row in df.iterrows():
        try:
            col = int(float(row['column_position']))
            pid = int(row['person_id'])
            if col > 0:
                col_dict.setdefault(col, []).append(pid)
        except Exception:
            continue
    # é‡è¤‡é™¤å»
    for k in col_dict:
        col_dict[k] = sorted(list(set(col_dict[k])))
    return col_dict

def create_analysis_output_dir(base_dir):
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_dir = os.path.join(base_dir, f"analysis_{timestamp}")
    os.makedirs(output_dir, exist_ok=True)
    return output_dir

def plot_shoulder_width_vs_column_with_fit(csv_path, output_dir):
    """
    è·é›¢-è‚©å¹…é–¢ä¿‚ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ
    - æ¨ªè»¸: åˆ—ä½ç½® (column_position)
    - ç¸¦è»¸: è‚©å¹… (shoulder_width)
    - å€‹äººãƒ‡ãƒ¼ã‚¿: ç‚¹
    - åˆ—å¹³å‡: èµ¤ã„è±å½¢
    - æœ€é©æŒ‡æ•°æ¸›è¡°é–¢æ•°: æ›²ç·š
    - é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿jsonä¿å­˜
    - æ­£è¦åŒ–é–¢æ•°ã‚³ãƒ¼ãƒ‰ã‚‚è‡ªå‹•ç”Ÿæˆ
    """
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    # -1ã‚„Noneã¯é™¤å¤–ï¼ˆfloatå‹ã«ã‚‚å¯¾å¿œï¼‰
    df = df[df['column_position'].apply(lambda x: pd.notnull(x) and float(x) > 0)]
    if 'column_position' not in df.columns or 'shoulder_width' not in df.columns:
        print("âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    plt.figure(figsize=(10, 7))
    plt.scatter(df['column_position'], df['shoulder_width'], alpha=0.5, label='å€‹äººãƒ‡ãƒ¼ã‚¿')

    mean_df = df.groupby('column_position')['shoulder_width'].mean().reset_index()
    plt.scatter(mean_df['column_position'], mean_df['shoulder_width'], 
                color='red', marker='D', s=80, label='åˆ—å¹³å‡')

    def exp_decay(x, a, b, c):
        return a * np.exp(-b * x) + c

    xdata = mean_df['column_position']
    ydata = mean_df['shoulder_width']
    fit_params = None
    try:
        popt, pcov = curve_fit(exp_decay, xdata, ydata, p0=(ydata.max(), 0.1, ydata.min()))
        fit_params = popt
        x_fit = np.linspace(df['column_position'].min(), df['column_position'].max(), 100)
        y_fit = exp_decay(x_fit, *popt)
        plt.plot(x_fit, y_fit, color='blue', linewidth=2, label='æŒ‡æ•°æ¸›è¡°ãƒ•ã‚£ãƒƒãƒˆ')
    except Exception as e:
        print(f"æŒ‡æ•°æ¸›è¡°ãƒ•ã‚£ãƒƒãƒˆå¤±æ•—: {e}")

    plt.xlabel('åˆ—ä½ç½® (column_position)', fontsize=13)
    plt.ylabel('è‚©å¹… (px)', fontsize=13)
    plt.title('è·é›¢-è‚©å¹…é–¢ä¿‚ã¨æ­£è¦åŒ–é–¢æ•°', fontsize=15)
    plt.legend()
    plt.grid(True, alpha=0.3)

    output_path = os.path.join(output_dir, "shoulder_width_vs_column_fit.png")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"âœ… è·é›¢-è‚©å¹…é–¢ä¿‚ã‚°ãƒ©ãƒ•ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

    # é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä¿å­˜
    if fit_params is not None:
        params_dict = {
            "function": "exponential_decay",
            "parameters": {
                "a": fit_params[0],
                "b": fit_params[1],
                "c": fit_params[2]
            },
            "formula": "f(x) = a * exp(-b * x) + c"
        }
        with open(os.path.join(output_dir, "function_parameters.json"), "w", encoding="utf-8-sig") as f:
            json.dump(params_dict, f, ensure_ascii=False, indent=2)
        print(f"âœ… é–¢æ•°ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ function_parameters.json ã«ä¿å­˜ã—ã¾ã—ãŸ")

        # æ­£è¦åŒ–é–¢æ•°ã‚³ãƒ¼ãƒ‰ç”Ÿæˆ
        normalization_code = f"""# è·é›¢æ­£è¦åŒ–é–¢æ•°ï¼ˆè‡ªå‹•ç”Ÿæˆï¼‰
import numpy as np

def distance_normalization_function(column_position):
    \"\"\"
    æŒ‡æ•°æ¸›è¡°é–¢æ•°ã«ã‚ˆã‚‹è‚©å¹…äºˆæ¸¬
    f(x) = {fit_params[0]:.6f} * exp(-{fit_params[1]:.6f} * x) + {fit_params[2]:.6f}
    \"\"\"
    return {fit_params[0]:.6f} * np.exp(-{fit_params[1]:.6f} * column_position) + {fit_params[2]:.6f}

def normalize_shoulder_width(measured_width, column_position, reference_column=1):
    \"\"\"
    å®Ÿæ¸¬è‚©å¹…ã‚’åŸºæº–åˆ—ã§æ­£è¦åŒ–
    \"\"\"
    predicted_width = distance_normalization_function(column_position)
    reference_width = distance_normalization_function(reference_column)
    normalization_factor = reference_width / predicted_width
    return measured_width * normalization_factor
"""
        with open(os.path.join(output_dir, "normalization_function.py"), "w", encoding="utf-8-sig") as f:
            f.write(normalization_code)
        print(f"âœ… æ­£è¦åŒ–é–¢æ•°ã‚³ãƒ¼ãƒ‰ã‚’ normalization_function.py ã«ä¿å­˜ã—ã¾ã—ãŸ")

def plot_angle_boxplot_by_column(csv_path, output_dir):
    """
    åˆ—ä½ç½®ã”ã¨ã®ãªã™è§’åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰ã‚’å‡ºåŠ›
    """
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    # -1ã‚„Noneã¯é™¤å¤–ï¼ˆfloatå‹ã«ã‚‚å¯¾å¿œï¼‰
    df = df[df['column_position'].apply(lambda x: pd.notnull(x) and float(x) > 0)]
    if 'column_position' not in df.columns or 'shoulder_head_angle' not in df.columns:
        print("âŒ å¿…è¦ãªã‚«ãƒ©ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    grouped = df.groupby('column_position')['shoulder_head_angle']
    data = []
    labels = []
    for col, group in grouped:
        values = group.dropna().values
        if len(values) > 0:
            data.append(values)
            labels.append(str(col))

    if len(data) == 0:
        print("âš ï¸ ç®±ã²ã’å›³ã‚’æç”»ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
        return

    plt.figure(figsize=(10, 6))
    plt.boxplot(data, labels=labels, patch_artist=True,
                boxprops=dict(facecolor='skyblue', color='navy'),
                medianprops=dict(color='red'))
    plt.title('åˆ—ä½ç½®ã”ã¨ã®ãªã™è§’åˆ†å¸ƒï¼ˆç®±ã²ã’å›³ï¼‰', fontsize=15)
    plt.xlabel('åˆ—ä½ç½® (column_position)', fontsize=13)
    plt.ylabel('ãªã™è§’ (åº¦)', fontsize=13)
    plt.grid(True, alpha=0.3)

    output_path = os.path.join(output_dir, "angle_boxplot_by_column.png")
    plt.tight_layout()
    plt.savefig(output_path, dpi=300)
    plt.close()
    print(f"âœ… åˆ—ä½ç½®ã”ã¨ã®ãªã™è§’ç®±ã²ã’å›³ã‚’ {output_path} ã«ä¿å­˜ã—ã¾ã—ãŸ")

def check_command(args) -> int:
    """ğŸ“Š ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚³ãƒãƒ³ãƒ‰"""
    print("ğŸ” åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèªä¸­...")
    try:
        result = check_available_data(args.csv_path)
        if 'error' in result:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: {result['error']}")
            return 1
        print(f"\nâœ… ãƒ‡ãƒ¼ã‚¿ç¢ºèªçµæœ:")
        print(f"ğŸ“ CSVãƒ•ã‚¡ã‚¤ãƒ«: {args.csv_path}")
        print(f"ğŸ“Š ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {result['total_frames']}")
        print(f"ğŸ“ è‚©å¹…ãƒ‡ãƒ¼ã‚¿åˆ—: {result['shoulder_width_column']}")
        print(f"\nğŸ“‹ åˆ†æå¿…è¦æ¡ä»¶:")
        req = result['minimum_requirements']
        print(f"   æœ€ä½å¿…è¦äººæ•°: {req['min_people_per_analysis']}äºº")
        print(f"   æœ€ä½å¿…è¦åˆ—æ•°: {req['min_columns']}åˆ—")
        print(f"   æ¨å¥¨äººæ•°/åˆ—: {req['recommended_people_per_column']}äºº")
        print(f"\nğŸ¯ æ¨å¥¨ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆ{req['min_people_per_analysis']}äººä»¥ä¸Šæ¤œå‡ºï¼‰:")
        if not result['recommended_frames']:
            print("âš ï¸  æ¡ä»¶ã‚’æº€ãŸã™ãƒ•ãƒ¬ãƒ¼ãƒ ãŒã‚ã‚Šã¾ã›ã‚“")
            print("ğŸ’¡ ã‚ˆã‚Šç·©ã„æ¡ä»¶ã®ãƒ•ãƒ¬ãƒ¼ãƒ ï¼ˆä¸Šä½5ã¤ï¼‰:")
            all_frames = sorted(
                result['frame_details'].items(), 
                key=lambda x: x[1]['valid_shoulder_data'], 
                reverse=True
            )[:5]
            for frame, info in all_frames:
                print(f"   ğŸ¬ {frame}:")
                print(f"      æ¤œå‡º: {info['valid_shoulder_data']}äºº")
                print(f"      ID: {info['available_ids']}")
                if info['shoulder_width_range']:
                    print(f"      è‚©å¹…ç¯„å›²: {info['shoulder_width_range'][0]:.1f}-{info['shoulder_width_range'][1]:.1f}px")
                print()
        else:
            for frame in result['recommended_frames']:
                info = result['frame_details'][frame]
                print(f"   ğŸ¬ {frame}:")
                print(f"      æ¤œå‡º: {info['valid_shoulder_data']}äºº")
                print(f"      åˆ©ç”¨å¯èƒ½ID: {info['available_ids']}")
                print(f"      è‚©å¹…ç¯„å›²: {info['shoulder_width_range'][0]:.1f}-{info['shoulder_width_range'][1]:.1f}px")
                print()
        if result['recommended_frames']:
            frame = result['recommended_frames'][0]
            ids = result['frame_details'][frame]['available_ids']
            if len(ids) >= 6:
                ids_per_col = len(ids) // 3
                col1 = ids[:ids_per_col]
                col2 = ids[ids_per_col:ids_per_col*2] 
                col3 = ids[ids_per_col*2:]
                print(f"ğŸ’¡ æ¨å¥¨å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰ä¾‹:")
                print(f"python {Path(__file__).name} analyze_one \\")
                print(f'    "{args.csv_path}" \\')
                print(f'    "{frame}"')
        return 0
    except Exception as e:
        print(f"âŒ ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return 1

def analyze_one_command(args) -> int:
    """ğŸ¯ 1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æã‚³ãƒãƒ³ãƒ‰"""
    try:
        output_dir = create_analysis_output_dir(args.output_dir)
        column_assignments = extract_column_assignments_from_csv(args.csv_path)
        if not column_assignments:
            print("âŒ ã‚¨ãƒ©ãƒ¼: column_positionåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å‰²ã‚Šå½“ã¦ãŒã‚ã‚Šã¾ã›ã‚“")
            return 1
        if len(column_assignments) < 2:
            print("âŒ ã‚¨ãƒ©ãƒ¼: æ­£è¦åŒ–é–¢æ•°ä½œæˆã«ã¯æœ€ä½2åˆ—å¿…è¦ã§ã™")
            return 1
        print(f"ğŸ¯ 1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æé–‹å§‹:")
        print(f"   ğŸ“ CSV: {args.csv_path}")
        print(f"   ğŸ¬ ãƒ•ãƒ¬ãƒ¼ãƒ : {args.frame_id}")
        print(f"   ğŸ“‹ åˆ—æ§‹æˆ: {column_assignments}")
        print(f"   ğŸ“‚ å‡ºåŠ›å…ˆ: {output_dir}")
        analyzer = DistanceNormalizationAnalyzer(
            csv_path=args.csv_path,
            output_base_dir=output_dir
        )
        result = analyzer.analyze_distance_function(
            frame_id=args.frame_id,
            column_assignments=column_assignments
        )
        if result['success']:
            print(f"\nğŸ‰ åˆ†ææˆåŠŸ!")
            print(f"ğŸ“ çµæœãƒ•ã‚©ãƒ«ãƒ€: {result['analysis_info']['output_dir']}")
            plot_shoulder_width_vs_column_with_fit(args.csv_path, output_dir)
            plot_angle_boxplot_by_column(args.csv_path, output_dir)
            return 0
        else:
            print(f"âŒ åˆ†æå¤±æ•—: {result.get('error', 'ä¸æ˜ãªã‚¨ãƒ©ãƒ¼')}")
            return 1
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return 1

def analyze_all_command(args) -> int:
    """ğŸ¯ å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æã‚³ãƒãƒ³ãƒ‰"""
    try:
        output_dir = create_analysis_output_dir(args.output_dir)
        column_assignments = extract_column_assignments_from_csv(args.csv_path)
        if not column_assignments:
            print("âŒ ã‚¨ãƒ©ãƒ¼: column_positionåˆ—ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€å‰²ã‚Šå½“ã¦ãŒã‚ã‚Šã¾ã›ã‚“")
            return 1
        print(f"ğŸ¯ å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æé–‹å§‹:")
        print(f"   ğŸ“ CSV: {args.csv_path}")
        print(f"   ğŸ“‹ åˆ—æ§‹æˆ: {column_assignments}")
        print(f"   ğŸ“‚ å‡ºåŠ›å…ˆ: {output_dir}")
        plot_shoulder_width_vs_column_with_fit(args.csv_path, output_dir)
        plot_angle_boxplot_by_column(args.csv_path, output_dir)
        return 0
    except Exception as e:
        print(f"âŒ å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return 1

def sample_command() -> int:
    """ğŸ”¬ ã‚µãƒ³ãƒ—ãƒ«åˆ†æã®å®Ÿè¡Œ"""
    print("ğŸ”¬ ã‚µãƒ³ãƒ—ãƒ«æ­£è¦åŒ–åˆ†æã‚’å®Ÿè¡Œ...")
    try:
        sample_csv_paths = [
            'outputs/baseline/11æœˆ12æ—¥ 1/4point_metrics.csv',
            'outputs/baseline/*/4point_metrics.csv',
            'data/4point_metrics.csv',
            '4point_metrics.csv'
        ]
        sample_csv = None
        for path_pattern in sample_csv_paths:
            if '*' in path_pattern:
                from glob import glob
                matches = glob(path_pattern)
                if matches:
                    sample_csv = matches[0]
                    break
            else:
                if Path(path_pattern).exists():
                    sample_csv = path_pattern
                    break
        if not sample_csv:
            print("âŒ ã‚µãƒ³ãƒ—ãƒ«ç”¨CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print("ğŸ’¡ ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã‚’é…ç½®ã—ã¦ãã ã•ã„:")
            for path in sample_csv_paths[:3]:
                print(f"   - {path}")
            return 1
        print(f"ğŸ“ ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {sample_csv}")
        output_dir = create_analysis_output_dir("outputs/normalization_analysis")
        plot_shoulder_width_vs_column_with_fit(sample_csv, output_dir)
        plot_angle_boxplot_by_column(sample_csv, output_dir)
        print(f"\nâœ… ã‚µãƒ³ãƒ—ãƒ«ã‚°ãƒ©ãƒ•ã‚’ {output_dir} ã«å‡ºåŠ›ã—ã¾ã—ãŸã€‚")
        return 0
    except Exception as e:
        print(f"âŒ ã‚µãƒ³ãƒ—ãƒ«åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return 1

def main() -> int:
    """ğŸ¯ ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    print_header()
    parser = argparse.ArgumentParser(
        description='ğŸ”§ è·é›¢æ­£è¦åŒ–é–¢æ•°åˆ†æãƒ„ãƒ¼ãƒ«',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ä¾‹:
  # ãƒ‡ãƒ¼ã‚¿ç¢ºèª
  %(prog)s check "outputs/baseline/11æœˆ12æ—¥ 1/4point_metrics.csv"
  
  # 1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
  %(prog)s analyze_one "data.csv" "frame.jpg"
  
  # å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ
  %(prog)s analyze_all "data.csv"
  
  # ã‚µãƒ³ãƒ—ãƒ«åˆ†æ
  %(prog)s
        """
    )
    subparsers = parser.add_subparsers(dest='command', help='å®Ÿè¡Œã‚³ãƒãƒ³ãƒ‰')
    check_parser = subparsers.add_parser(
        'check', 
        help='ğŸ“Š åˆ©ç”¨å¯èƒ½ãƒ‡ãƒ¼ã‚¿ã®ç¢ºèª',
        description='CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰åˆ†æå¯èƒ½ãªãƒ•ãƒ¬ãƒ¼ãƒ ãƒ»IDã‚’ç¢ºèª'
    )
    check_parser.add_argument('csv_path', help='4ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    analyze_one_parser = subparsers.add_parser(
        'analyze_one', 
        help='ğŸ¯ 1ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ',
        description='æŒ‡å®šã—ãŸãƒ•ãƒ¬ãƒ¼ãƒ ã§æ­£è¦åŒ–é–¢æ•°ã‚’ç”Ÿæˆ'
    )
    analyze_one_parser.add_argument('csv_path', help='4ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    analyze_one_parser.add_argument('frame_id', help='åˆ†æå¯¾è±¡ãƒ•ãƒ¬ãƒ¼ãƒ ID')
    analyze_one_parser.add_argument('--output-dir', default='outputs/normalization_analysis',
                               help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputs/normalization_analysis)')
    analyze_all_parser = subparsers.add_parser(
        'analyze_all', 
        help='ğŸ¯ å…¨ãƒ•ãƒ¬ãƒ¼ãƒ åˆ†æ',
        description='å…¨ãƒ•ãƒ¬ãƒ¼ãƒ ã§åˆ†å¸ƒãƒ»å¹³å‡ã‚°ãƒ©ãƒ•ã‚’ç”Ÿæˆ'
    )
    analyze_all_parser.add_argument('csv_path', help='4ç‚¹ãƒ¡ãƒˆãƒªã‚¯ã‚¹CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹')
    analyze_all_parser.add_argument('--output-dir', default='outputs/normalization_analysis',
                               help='å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: outputs/normalization_analysis)')
    args = parser.parse_args()
    try:
        if args.command == 'check':
            return check_command(args)
        elif args.command == 'analyze_one':
            return analyze_one_command(args)
        elif args.command == 'analyze_all':
            return analyze_all_command(args)
        else:
            print("ğŸ“ å¼•æ•°ãªã—ã§ã‚µãƒ³ãƒ—ãƒ«åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™...")
            return sample_command()
    except KeyboardInterrupt:
        print("\nâ¹ï¸  ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã‚‹ä¸­æ–­")
        return 130
    except Exception as e:
        print(f"\nâŒ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        traceback.print_exc()
        return 1

if __name__ == "__main__":
    sys.exit(main())

# --- ç®±ã²ã’å›³ã®è¦‹æ–¹ ---
# æ¨ªè»¸ï¼šåˆ—ä½ç½®ï¼ˆcolumn_positionï¼‰
# ç¸¦è»¸ï¼šãªã™è§’ï¼ˆåº¦ï¼‰
# ç®±ï¼šãƒ‡ãƒ¼ã‚¿ã®ä¸­å¤®50%ï¼ˆç¬¬1å››åˆ†ä½ï½ç¬¬3å››åˆ†ä½ï¼‰
# èµ¤ç·šï¼šä¸­å¤®å€¤
# ã²ã’ï¼šå¤–ã‚Œå€¤ã‚’é™¤ã„ãŸç¯„å›²
# ç‚¹ï¼šå¤–ã‚Œå€¤
# â†’ åˆ—ã”ã¨ã«å§¿å‹¢ï¼ˆãªã™è§’ï¼‰ã®ã°ã‚‰ã¤ãã‚„å‚¾å‘ãŒåˆ†ã‹ã‚Šã¾ã™