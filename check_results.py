"""
å‡¦ç†çµæœç¢ºèªã‚¹ã‚¯ãƒªãƒ—ãƒˆï¼ˆæ·±åº¦æ¨å®šçµ±åˆå¯¾å¿œç‰ˆï¼‰
outputs/baseline/ ã®çµæœã‚’ç°¡æ˜“åˆ†æ

ğŸ” æ–°æ©Ÿèƒ½:
- æ·±åº¦æ¨å®šçµæœã®åˆ†æ
- æ·±åº¦çµ±åˆCSVç¢ºèª
- ã‚¾ãƒ¼ãƒ³åˆ¥åˆ†æ
- æ·±åº¦å¯è¦–åŒ–ç¢ºèª
- æ¯”è¼ƒå®Ÿé¨“çµæœè¡¨ç¤º
"""

import pandas as pd
import os
import json
import numpy as np
from pathlib import Path
import glob
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, Any, Optional, List
import logging

# ãƒ­ã‚°è¨­å®š
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def find_latest_baseline():
    """æœ€æ–°ã®ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™"""
    baseline_dirs = sorted(glob.glob("outputs/baseline/baseline_*"))
    if not baseline_dirs:
        return None
    return baseline_dirs[-1]

def find_experiment_results():
    """å®Ÿé¨“çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™"""
    experiment_dirs = sorted(glob.glob("outputs/experiments/*"))
    return experiment_dirs

def check_depth_integration(df: pd.DataFrame) -> Dict[str, Any]:
    """æ·±åº¦æ¨å®šçµ±åˆçŠ¶æ³ã®ç¢ºèª"""
    depth_info = {
        "has_depth": False,
        "depth_success_rate": 0.0,
        "zone_distribution": {},
        "depth_stats": {},
        "depth_confidence_stats": {}
    }
    
    try:
        # æ·±åº¦é–¢é€£ã‚«ãƒ©ãƒ ã®ç¢ºèª
        depth_columns = [col for col in df.columns if 'depth' in col.lower()]
        
        if depth_columns:
            depth_info["has_depth"] = True
            depth_info["depth_columns"] = depth_columns
            
            # æ·±åº¦è·é›¢ã®æˆåŠŸç‡
            if 'depth_distance' in df.columns:
                valid_depth = df[df['depth_distance'] >= 0]
                depth_info["depth_success_rate"] = len(valid_depth) / len(df) if len(df) > 0 else 0
                
                if len(valid_depth) > 0:
                    depth_info["depth_stats"] = {
                        "mean": float(valid_depth['depth_distance'].mean()),
                        "std": float(valid_depth['depth_distance'].std()),
                        "min": float(valid_depth['depth_distance'].min()),
                        "max": float(valid_depth['depth_distance'].max()),
                        "median": float(valid_depth['depth_distance'].median())
                    }
            
            # ã‚¾ãƒ¼ãƒ³åˆ†å¸ƒ
            if 'depth_zone' in df.columns:
                zone_counts = df['depth_zone'].value_counts()
                total = len(df)
                depth_info["zone_distribution"] = {
                    zone: {"count": int(count), "percentage": float(count/total*100)}
                    for zone, count in zone_counts.items()
                }
            
            # æ·±åº¦ä¿¡é ¼åº¦
            if 'depth_confidence' in df.columns:
                depth_conf = df['depth_confidence'].dropna()
                if len(depth_conf) > 0:
                    depth_info["depth_confidence_stats"] = {
                        "mean": float(depth_conf.mean()),
                        "std": float(depth_conf.std()),
                        "min": float(depth_conf.min()),
                        "max": float(depth_conf.max())
                    }
        
    except Exception as e:
        logger.warning(f"æ·±åº¦çµ±åˆç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    return depth_info

def display_depth_analysis(depth_info: Dict[str, Any]):
    """æ·±åº¦åˆ†æçµæœã®è¡¨ç¤º"""
    if not depth_info["has_depth"]:
        print("  ğŸ” æ·±åº¦æ¨å®š: ç„¡åŠ¹")
        return
    
    print(f"  ğŸ” æ·±åº¦æ¨å®š: æœ‰åŠ¹")
    print(f"  æ·±åº¦æˆåŠŸç‡: {depth_info['depth_success_rate']:.1%}")
    
    # æ·±åº¦çµ±è¨ˆ
    if depth_info.get("depth_stats"):
        stats = depth_info["depth_stats"]
        print(f"  æ·±åº¦è·é›¢çµ±è¨ˆ:")
        print(f"    å¹³å‡: {stats['mean']:.2f}")
        print(f"    æ¨™æº–åå·®: {stats['std']:.2f}")
        print(f"    ç¯„å›²: {stats['min']:.2f} - {stats['max']:.2f}")
        print(f"    ä¸­å¤®å€¤: {stats['median']:.2f}")
    
    # ã‚¾ãƒ¼ãƒ³åˆ†å¸ƒ
    if depth_info.get("zone_distribution"):
        print(f"  ã‚¾ãƒ¼ãƒ³åˆ¥åˆ†å¸ƒ:")
        for zone, data in depth_info["zone_distribution"].items():
            print(f"    {zone}: {data['count']}ä»¶ ({data['percentage']:.1f}%)")
    
    # æ·±åº¦ä¿¡é ¼åº¦
    if depth_info.get("depth_confidence_stats"):
        conf_stats = depth_info["depth_confidence_stats"]
        print(f"  æ·±åº¦ä¿¡é ¼åº¦:")
        print(f"    å¹³å‡: {conf_stats['mean']:.3f}")
        print(f"    ç¯„å›²: {conf_stats['min']:.3f} - {conf_stats['max']:.3f}")

def check_depth_visualizations(video_dir: Path) -> Dict[str, Any]:
    """æ·±åº¦å¯è¦–åŒ–ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
    viz_info = {
        "depth_analysis_graphs": [],
        "depth_heatmaps": [],
        "depth_summaries": [],
        "depth_comparisons": []
    }
    
    try:
        # æ·±åº¦åˆ†æã‚°ãƒ©ãƒ•
        depth_graphs = list(video_dir.glob("**/depth_analysis_*.png"))
        viz_info["depth_analysis_graphs"] = [str(p) for p in depth_graphs]
        
        # æ·±åº¦ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—
        heatmaps = list(video_dir.glob("**/depth_heatmap_*.png"))
        viz_info["depth_heatmaps"] = [str(p) for p in heatmaps]
        
        # æ·±åº¦ã‚µãƒãƒªãƒ¼
        summaries = list(video_dir.glob("**/depth_summary_*.txt"))
        viz_info["depth_summaries"] = [str(p) for p in summaries]
        
        # æ·±åº¦æ¯”è¼ƒã‚°ãƒ©ãƒ•
        comparisons = list(video_dir.glob("**/depth_comparison_*.png"))
        viz_info["depth_comparisons"] = [str(p) for p in comparisons]
        
    except Exception as e:
        logger.warning(f"æ·±åº¦å¯è¦–åŒ–ç¢ºèªã‚¨ãƒ©ãƒ¼: {e}")
    
    return viz_info

def analyze_model_performance(df: pd.DataFrame) -> Dict[str, Any]:
    """ãƒ¢ãƒ‡ãƒ«æ€§èƒ½ã®è©³ç´°åˆ†æ"""
    perf_info = {}
    
    try:
        # æ¤œå‡ºå“è³ªåˆ†æ
        if 'conf' in df.columns:
            # ä¿¡é ¼åº¦åˆ¥åˆ†å¸ƒ
            conf_ranges = {
                "excellent": len(df[df['conf'] > 0.9]),
                "good": len(df[(df['conf'] > 0.7) & (df['conf'] <= 0.9)]),
                "fair": len(df[(df['conf'] > 0.5) & (df['conf'] <= 0.7)]),
                "poor": len(df[df['conf'] <= 0.5])
            }
            
            total = len(df)
            perf_info["confidence_distribution"] = {
                level: {"count": count, "percentage": count/total*100}
                for level, count in conf_ranges.items()
            }
        
        # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹å“è³ª
        if all(col in df.columns for col in ['x1', 'y1', 'x2', 'y2']):
            df['width'] = df['x2'] - df['x1']
            df['height'] = df['y2'] - df['y1']
            df['area'] = df['width'] * df['height']
            df['aspect_ratio'] = df['width'] / df['height']
            
            perf_info["bbox_stats"] = {
                "avg_area": float(df['area'].mean()),
                "avg_width": float(df['width'].mean()),
                "avg_height": float(df['height'].mean()),
                "avg_aspect_ratio": float(df['aspect_ratio'].mean()),
                "area_std": float(df['area'].std())
            }
        
        # è¿½è·¡å“è³ªï¼ˆIDãŒã‚ã‚‹å ´åˆï¼‰
        if 'person_id' in df.columns and 'frame' in df.columns:
            id_stats = df.groupby('person_id').agg({
                'frame': ['count', 'min', 'max'],
                'conf': ['mean', 'std']
            }).round(3)
            
            track_lengths = df.groupby('person_id')['frame'].count()
            
            perf_info["tracking_stats"] = {
                "total_tracks": int(df['person_id'].nunique()),
                "avg_track_length": float(track_lengths.mean()),
                "max_track_length": int(track_lengths.max()),
                "min_track_length": int(track_lengths.min()),
                "long_tracks": int(len(track_lengths[track_lengths > 30])),  # 30ãƒ•ãƒ¬ãƒ¼ãƒ ä»¥ä¸Š
                "short_tracks": int(len(track_lengths[track_lengths < 10]))   # 10ãƒ•ãƒ¬ãƒ¼ãƒ æœªæº€
            }
    
    except Exception as e:
        logger.warning(f"æ€§èƒ½åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
    
    return perf_info

def display_performance_analysis(perf_info: Dict[str, Any]):
    """æ€§èƒ½åˆ†æçµæœã®è¡¨ç¤º"""
    
    # ä¿¡é ¼åº¦åˆ†å¸ƒ
    if "confidence_distribution" in perf_info:
        print(f"\n  ğŸ“Š æ¤œå‡ºå“è³ªåˆ†æ:")
        conf_dist = perf_info["confidence_distribution"]
        print(f"    å„ªç§€ (>0.9): {conf_dist['excellent']['count']}ä»¶ ({conf_dist['excellent']['percentage']:.1f}%)")
        print(f"    è‰¯å¥½ (0.7-0.9): {conf_dist['good']['count']}ä»¶ ({conf_dist['good']['percentage']:.1f}%)")
        print(f"    æ™®é€š (0.5-0.7): {conf_dist['fair']['count']}ä»¶ ({conf_dist['fair']['percentage']:.1f}%)")
        print(f"    ä½å“è³ª (â‰¤0.5): {conf_dist['poor']['count']}ä»¶ ({conf_dist['poor']['percentage']:.1f}%)")
    
    # ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹çµ±è¨ˆ
    if "bbox_stats" in perf_info:
        bbox = perf_info["bbox_stats"]
        print(f"\n  ğŸ“¦ ãƒã‚¦ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ãƒœãƒƒã‚¯ã‚¹çµ±è¨ˆ:")
        print(f"    å¹³å‡é¢ç©: {bbox['avg_area']:.0f} pxÂ²")
        print(f"    å¹³å‡ã‚µã‚¤ã‚º: {bbox['avg_width']:.0f} Ã— {bbox['avg_height']:.0f} px")
        print(f"    å¹³å‡ã‚¢ã‚¹ãƒšã‚¯ãƒˆæ¯”: {bbox['avg_aspect_ratio']:.2f}")
        print(f"    é¢ç©æ¨™æº–åå·®: {bbox['area_std']:.0f}")
    
    # è¿½è·¡çµ±è¨ˆ
    if "tracking_stats" in perf_info:
        track = perf_info["tracking_stats"]
        print(f"\n  ğŸ¯ è¿½è·¡å“è³ªåˆ†æ:")
        print(f"    ç·è¿½è·¡æ•°: {track['total_tracks']}äºº")
        print(f"    å¹³å‡è¿½è·¡é•·: {track['avg_track_length']:.1f}ãƒ•ãƒ¬ãƒ¼ãƒ ")
        print(f"    æœ€é•·è¿½è·¡: {track['max_track_length']}ãƒ•ãƒ¬ãƒ¼ãƒ ")
        print(f"    é•·æœŸè¿½è·¡ (>30f): {track['long_tracks']}äºº")
        print(f"    çŸ­æœŸè¿½è·¡ (<10f): {track['short_tracks']}äºº")

def check_experiment_results():
    """å®Ÿé¨“çµæœã®ç¢ºèª"""
    experiment_dirs = find_experiment_results()
    
    if not experiment_dirs:
        print("\nğŸ§ª å®Ÿé¨“çµæœ: ãªã—")
        return
    
    print(f"\nğŸ§ª å®Ÿé¨“çµæœ: {len(experiment_dirs)}ä»¶")
    
    for exp_dir in experiment_dirs[-3:]:  # æœ€æ–°3ä»¶ã®ã¿è¡¨ç¤º
        exp_path = Path(exp_dir)
        exp_name = exp_path.name
        
        print(f"\n  ğŸ“‹ å®Ÿé¨“: {exp_name}")
        
        # å®Ÿé¨“çµæœJSONã®ç¢ºèª
        result_json = exp_path / "experiment_results.json"
        if result_json.exists():
            try:
                with open(result_json, 'r', encoding='utf-8') as f:
                    results = json.load(f)
                
                print(f"    å®Ÿé¨“ã‚¿ã‚¤ãƒ—: {results.get('experiment_type', 'N/A')}")
                print(f"    å‡¦ç†å‹•ç”»æ•°: {len(results.get('videos', []))}")
                print(f"    å®Ÿè¡Œæ™‚é–“: {results.get('total_processing_time', 'N/A')}")
                
                # æˆåŠŸç‡
                if 'videos' in results:
                    successful = len([v for v in results['videos'] if v.get('success', False)])
                    success_rate = successful / len(results['videos']) * 100
                    print(f"    æˆåŠŸç‡: {success_rate:.1f}%")
                
                # æ·±åº¦æ¨å®šå®Ÿé¨“ã®ç‰¹åˆ¥ãªè¡¨ç¤º
                if results.get('experiment_type') == 'depth_analysis_comparison':
                    print(f"    ğŸ” æ·±åº¦æ¨å®šæ¯”è¼ƒå®Ÿé¨“")
                    if 'comparison_metrics' in results:
                        metrics = results['comparison_metrics']
                        print(f"    æ”¹å–„åŠ¹æœ: {metrics.get('improvement_summary', 'N/A')}")
            
            except Exception as e:
                print(f"    âš ï¸ çµæœèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        
        # æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ
        comparison_files = list(exp_path.glob("*comparison*.html"))
        if comparison_files:
            print(f"    ğŸ“Š æ¯”è¼ƒãƒ¬ãƒãƒ¼ãƒˆ: {len(comparison_files)}ä»¶")

def check_model_files():
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª"""
    print("\nğŸ”§ åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«:")
    
    # YOLOãƒ¢ãƒ‡ãƒ«
    yolo_dir = Path("models/yolo")
    if yolo_dir.exists():
        yolo_models = list(yolo_dir.glob("*.pt"))
        print(f"  YOLO: {len(yolo_models)}ä»¶")
        
        # ã‚µã‚¤ã‚ºåˆ¥è¡¨ç¤º
        sizes = ['n', 's', 'm', 'l', 'x']
        for size in sizes:
            size_models = [m for m in yolo_models if f'yolo11{size}' in m.name]
            if size_models:
                model_types = []
                for model in size_models:
                    if '-pose' in model.name:
                        model_types.append('ãƒãƒ¼ã‚º')
                    elif '-seg' in model.name:
                        model_types.append('ã‚»ã‚°ãƒ¡')
                    else:
                        model_types.append('æ¤œå‡º')
                print(f"    {size.upper()}ã‚µã‚¤ã‚º: {', '.join(model_types)}")
    
    # æ·±åº¦æ¨å®šãƒ¢ãƒ‡ãƒ«
    depth_dir = Path("models/depth")
    if depth_dir.exists():
        depth_models = list(depth_dir.glob("*.pt"))
        print(f"  ğŸ” æ·±åº¦æ¨å®š: {len(depth_models)}ä»¶")
        for model in depth_models:
            size_mb = model.stat().st_size / (1024*1024)
            print(f"    {model.name} ({size_mb:.1f}MB)")

def generate_quick_summary(baseline_dir: str):
    """ã‚¯ã‚¤ãƒƒã‚¯ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""
    try:
        # å…¨å‹•ç”»ã®çµæœã‚’çµ±åˆ
        all_detections = []
        video_count = 0
        total_frames = 0
        
        for video_dir in Path(baseline_dir).iterdir():
            if not video_dir.is_dir() or video_dir.name in ['reports', 'visualizations']:
                continue
            
            video_count += 1
            
            # CSVæ¤œç´¢
            csv_files = list(video_dir.glob("**/*detection*.csv"))
            if csv_files:
                try:
                    df = pd.read_csv(csv_files[0])
                    all_detections.append(df)
                    if 'frame' in df.columns:
                        total_frames += df['frame'].nunique()
                except Exception as e:
                    logger.warning(f"CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼ {video_dir.name}: {e}")
        
        if all_detections:
            combined_df = pd.concat(all_detections, ignore_index=True)
            
            print(f"\nğŸ“ˆ å…¨ä½“ã‚µãƒãƒªãƒ¼:")
            print(f"  å‡¦ç†å‹•ç”»æ•°: {video_count}")
            print(f"  ç·ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {total_frames}")
            print(f"  ç·æ¤œå‡ºæ•°: {len(combined_df)}")
            print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯äººæ•°: {combined_df['person_id'].nunique()}")
            print(f"  å¹³å‡ä¿¡é ¼åº¦: {combined_df['conf'].mean():.3f}")
            
            # æ·±åº¦çµ±åˆã‚µãƒãƒªãƒ¼
            depth_info = check_depth_integration(combined_df)
            if depth_info["has_depth"]:
                print(f"  ğŸ” æ·±åº¦æ¨å®šæˆåŠŸç‡: {depth_info['depth_success_rate']:.1%}")
                if depth_info.get("zone_distribution"):
                    main_zone = max(depth_info["zone_distribution"].items(), key=lambda x: x[1]['count'])
                    print(f"  ä¸»è¦ã‚¾ãƒ¼ãƒ³: {main_zone[0]} ({main_zone[1]['percentage']:.1f}%)")
    
    except Exception as e:
        logger.warning(f"ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")

def check_results():
    """çµæœç¢ºèªãƒ¡ã‚¤ãƒ³å‡¦ç†ï¼ˆæ·±åº¦æ¨å®šçµ±åˆå¯¾å¿œç‰ˆï¼‰"""
    print("=" * 80)
    print("ğŸ“Š YOLO11 åºƒè§’ã‚«ãƒ¡ãƒ©åˆ†æã‚·ã‚¹ãƒ†ãƒ  - å‡¦ç†çµæœç¢ºèªï¼ˆæ·±åº¦æ¨å®šçµ±åˆå¯¾å¿œç‰ˆï¼‰")
    print("=" * 80)

    # æœ€æ–°ã®çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
    baseline_dir = find_latest_baseline()

    if not baseline_dir:
        print("âŒ çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        print("å…ˆã«ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  python improved_main.py --mode baseline")
        print("  python improved_main.py --mode baseline --config configs/depth_config.yaml  # æ·±åº¦æ¨å®šç‰ˆ")
        return

    print(f"\nçµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª: {baseline_dir}\n")

    # åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã®ç¢ºèª
    check_model_files()

    # å‹•ç”»ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’æ¢ã™
    video_dirs = [d for d in Path(baseline_dir).iterdir()
                if d.is_dir() and d.name not in ['reports', 'visualizations']]

    if not video_dirs:
        print("âš ï¸ å‹•ç”»å‡¦ç†çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return

    print(f"\nå‡¦ç†æ¸ˆã¿å‹•ç”»æ•°: {len(video_dirs)}")

    # å„å‹•ç”»ã®çµæœã‚’ç¢ºèª
    for i, video_dir in enumerate(video_dirs, 1):
        print("\n" + "â”" * 80)
        print(f"ğŸ“¹ å‹•ç”» {i}/{len(video_dirs)}: {video_dir.name}")
        print("â”" * 80)

        # CSVç¢ºèªï¼ˆè¤‡æ•°ã®å¯èƒ½æ€§ã®ã‚ã‚‹å ´æ‰€ã‚’æ¢ç´¢ï¼‰
        csv_path = None
        possible_csv_paths = [
            video_dir / "results" / "detections.csv",
            video_dir / "results" / "detections_streaming.csv",
            video_dir / "results" / "detections_enhanced.csv",  # ğŸ” æ·±åº¦çµ±åˆç‰ˆ
        ]

        for path in possible_csv_paths:
            if path.exists():
                csv_path = path
                break

        # ãã‚Œã§ã‚‚è¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å†å¸°çš„ã«æ¢ã™
        if not csv_path:
            csv_files = list(video_dir.glob("**/*detection*.csv"))
            if csv_files:
                csv_path = csv_files[0]

        if csv_path and csv_path.exists():
            try:
                df = pd.read_csv(csv_path)

                print(f"\nâœ… æ¤œå‡ºçµæœCSV: {csv_path.name}")
                print(f"  ç·æ¤œå‡ºæ•°: {len(df)}")
                print(f"  ãƒ¦ãƒ‹ãƒ¼ã‚¯ID: {df['person_id'].nunique()}")
                print(f"  ãƒ•ãƒ¬ãƒ¼ãƒ æ•°: {df['frame'].nunique()}")
                print(f"  å¹³å‡ä¿¡é ¼åº¦: {df['conf'].mean():.3f}")
                print(f"  ä¿¡é ¼åº¦ç¯„å›²: {df['conf'].min():.3f} - {df['conf'].max():.3f}")

                # ğŸ” æ·±åº¦æ¨å®šçµ±åˆåˆ†æ
                depth_info = check_depth_integration(df)
                display_depth_analysis(depth_info)

                # è©³ç´°æ€§èƒ½åˆ†æ
                perf_info = analyze_model_performance(df)
                display_performance_analysis(perf_info)

                # IDåˆ¥æ¤œå‡ºæ•°ï¼ˆä¸Šä½10ã«æ‹¡å¼µï¼‰
                print(f"\n  IDåˆ¥æ¤œå‡ºæ•°ï¼ˆä¸Šä½10ï¼‰:")
                id_counts = df['person_id'].value_counts().head(10)
                for pid, count in id_counts.items():
                    avg_conf = df[df['person_id'] == pid]['conf'].mean()
                    print(f"    ID {pid}: {count}å› (å¹³å‡ä¿¡é ¼åº¦: {avg_conf:.3f})")

            except Exception as e:
                print(f"âš ï¸ CSVèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        else:
            print(f"âŒ æ¤œå‡ºçµæœCSVãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            print(f"   æ¢ç´¢ã—ãŸãƒ‘ã‚¹:")
            for path in possible_csv_paths:
                print(f"     - {path}")

        # ãƒ•ãƒ¬ãƒ¼ãƒ æ•°ç¢ºèª
        frames_dir = video_dir / "frames"
        if frames_dir.exists():
            frame_files = list(frames_dir.glob("*.jpg")) + list(frames_dir.glob("*.png"))
            print(f"\nâœ… æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ : {len(frame_files)}æš")
        else:
            # åˆ¥ã®å ´æ‰€ã«ã‚ã‚‹å¯èƒ½æ€§
            frame_dirs = list(video_dir.glob("**/frames"))
            if frame_dirs:
                frame_files = list(frame_dirs[0].glob("*.jpg")) + list(frame_dirs[0].glob("*.png"))
                print(f"\nâœ… æŠ½å‡ºãƒ•ãƒ¬ãƒ¼ãƒ : {len(frame_files)}æš")

        # å¯è¦–åŒ–ç”»åƒç¢ºèª
        vis_files = list(video_dir.glob("**/vis_*.jpg"))
        if vis_files:
            print(f"âœ… å¯è¦–åŒ–ç”»åƒ: {len(vis_files)}æš")

        # ğŸ” æ·±åº¦å¯è¦–åŒ–ç¢ºèª
        depth_viz = check_depth_visualizations(video_dir)
        if any(depth_viz.values()):
            print(f"\nğŸ” æ·±åº¦å¯è¦–åŒ–:")
            if depth_viz["depth_analysis_graphs"]:
                print(f"  åˆ†æã‚°ãƒ©ãƒ•: {len(depth_viz['depth_analysis_graphs'])}ä»¶")
            if depth_viz["depth_heatmaps"]:
                print(f"  ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—: {len(depth_viz['depth_heatmaps'])}ä»¶")
            if depth_viz["depth_summaries"]:
                print(f"  ã‚µãƒãƒªãƒ¼: {len(depth_viz['depth_summaries'])}ä»¶")
            if depth_viz["depth_comparisons"]:
                print(f"  æ¯”è¼ƒã‚°ãƒ©ãƒ•: {len(depth_viz['depth_comparisons'])}ä»¶")

        # çµ±è¨ˆã‚°ãƒ©ãƒ•ç¢ºèª
        viz_dir = video_dir / "visualizations"
        if viz_dir.exists():
            graph_files = list(viz_dir.glob("*.png"))
            if graph_files:
                print(f"\nâœ… çµ±è¨ˆã‚°ãƒ©ãƒ•: {len(graph_files)}æš")
                for graph in graph_files[:5]:  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
                    print(f"    - {graph.name}")
                if len(graph_files) > 5:
                    print(f"    ... ä»– {len(graph_files)-5} ä»¶")

    # å…¨ä½“ã‚µãƒãƒªãƒ¼
    generate_quick_summary(baseline_dir)

    # ãƒ¬ãƒãƒ¼ãƒˆç¢ºèª
    print("\n" + "=" * 80)
    print("ğŸ“„ ãƒ¬ãƒãƒ¼ãƒˆ")
    print("=" * 80)

    reports_dir = Path(baseline_dir) / "reports"
    if reports_dir.exists():
        report_files = list(reports_dir.glob("*"))
        if report_files:
            print(f"\nâœ… ç”Ÿæˆã•ã‚ŒãŸãƒ¬ãƒãƒ¼ãƒˆ:")
            for report in report_files:
                size_kb = report.stat().st_size / 1024
                print(f"  - {report.name} ({size_kb:.1f} KB)")

            # HTMLãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‘ã‚¹è¡¨ç¤º
            html_reports = list(reports_dir.glob("*.html"))
            if html_reports:
                print(f"\nğŸ’¡ HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’é–‹ã:")
                for html in html_reports:
                    print(f"  {html.absolute()}")
                    print(f"\n  ã‚³ãƒãƒ³ãƒ‰:")
                    print(f"  open {html.absolute()}")  # Mac
                    print(f"  # ã¾ãŸã¯")
                    print(f"  xdg-open {html.absolute()}")  # Linux
    else:
        print("\nâš ï¸ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

    # ğŸ§ª å®Ÿé¨“çµæœç¢ºèª
    check_experiment_results()

    # å®Ÿé¨“çµæœJSONç¢ºèª
    json_path = Path(baseline_dir) / "experiment_results.json"
    if json_path.exists():
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                results = json.load(f)

            print(f"\nâœ… å®Ÿé¨“çµæœJSON:")
            print(f"  å®Ÿé¨“å: {results.get('experiment_name', 'N/A')}")
            print(f"  ãƒ•ã‚§ãƒ¼ã‚º: {results.get('phase', 'N/A')}")
            print(f"  å‡¦ç†å‹•ç”»æ•°: {len(results.get('videos', []))}")
            
            # æ·±åº¦æ¨å®šé–¢é€£ã®çµæœ
            if 'depth_estimation' in results:
                depth_results = results['depth_estimation']
                print(f"  ğŸ” æ·±åº¦æ¨å®šæˆåŠŸ: {depth_results.get('enabled', False)}")
                if depth_results.get('success_rate'):
                    print(f"  æ·±åº¦æˆåŠŸç‡: {depth_results['success_rate']:.1%}")
                    
        except Exception as e:
            print(f"âš ï¸ JSONèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # ç·æ‹¬ã¨æ¨å¥¨äº‹é …
    print("\n" + "=" * 80)
    print("âœ… çµæœç¢ºèªå®Œäº†")
    print("=" * 80)
    print("\nğŸ“‹ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
    print("1. HTMLãƒ¬ãƒãƒ¼ãƒˆã‚’ãƒ–ãƒ©ã‚¦ã‚¶ã§é–‹ã")
    print("2. å¯è¦–åŒ–ç”»åƒã‚’ç¢ºèª:")
    print(f"   ls {baseline_dir}/*/results/vis_*.jpg")
    print("3. ğŸ” æ·±åº¦åˆ†æã‚°ãƒ©ãƒ•ã‚’ç¢ºèª:")
    print(f"   ls {baseline_dir}/*/visualizations/depth_*.png")
    print("4. ã‚ˆã‚Šå¤§ãã„ãƒ¢ãƒ‡ãƒ«ã§è©¦ã™:")
    print("   - configs/default.yaml ã§ãƒ¢ãƒ‡ãƒ«ã‚’ yolo11m-pose.pt ã«å¤‰æ›´")
    print("   - å†åº¦ python improved_main.py --mode baseline")
    print("5. æ·±åº¦æ¨å®šã‚’æœ‰åŠ¹åŒ–:")
    print("   python improved_main.py --mode baseline --config configs/depth_config.yaml")
    print("6. ã‚¿ã‚¤ãƒ«æ¨è«–ã‚’è©¦ã™:")
    print("   - configs/default.yaml ã§ tile_inference.enabled: true")
    print("7. ğŸ§ª æ·±åº¦æ¨å®šæ¯”è¼ƒå®Ÿé¨“:")
    print("   python improved_main.py --mode experiment --experiment-type depth_analysis_comparison")
    
    print("\nğŸ¯ æ€§èƒ½æ”¹å–„ã®ãƒ’ãƒ³ãƒˆ:")
    print("- ä¿¡é ¼åº¦ãŒä½ã„å ´åˆ: ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«(m/l/x)ã‚’è©¦ã™")
    print("- è¿½è·¡ãŒä¸å®‰å®šãªå ´åˆ: ByteTrackè¨­å®šã‚’èª¿æ•´")
    print("- ğŸ” æ·±åº¦æ¨å®šç²¾åº¦å‘ä¸Š: ã‚ˆã‚Šå¤§ããªæ·±åº¦ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨")
    print("- ãƒ¡ãƒ¢ãƒªä¸è¶³ã®å ´åˆ: ã‚¿ã‚¤ãƒ«æ¨è«–ã‚’æœ‰åŠ¹åŒ–")

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    try:
        check_results()
    except KeyboardInterrupt:
        print("\nâŒ ç¢ºèªå‡¦ç†ãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
    except Exception as e:
        print(f"âŒ ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        
        print(f"\nğŸ’¡ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°:")
        print(f"1. çµæœãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ç¢ºèª:")
        print(f"   ls -la outputs/baseline/")
        print(f"2. ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã®ç¢ºèª:")
        print(f"   cat logs/latest.log")
        print(f"3. æ¨©é™ã®ç¢ºèª:")
        print(f"   ls -la outputs/")

if __name__ == "__main__":
    main()